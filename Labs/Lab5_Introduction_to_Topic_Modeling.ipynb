{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dulw-iD5JsV"
      },
      "source": [
        "## Introduction\n",
        "##### How to get started with topic modeling using LDA in Python\n",
        "** **\n",
        "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n",
        "\n",
        "- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
        "- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
        "- **Tagging**, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
        "\n",
        "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "In this tutorial, we’ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n",
        "\n",
        "### Theoretical Overview\n",
        "LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
        "\n",
        "![LDA_Model](https://github.com/chdoig/pytexas2015-topic-modeling/blob/master/images/lda-4.png?raw=true)\n",
        "\n",
        "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
        "\n",
        "- `psi`, the distribution of words for each topic K\n",
        "- `phi`, the distribution of topics for each document i\n",
        "\n",
        "#### Parameters of LDA\n",
        "\n",
        "- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
        "- `Beta parameter` is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.\n",
        "\n",
        "**To read more: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRUKGTVx5JsX"
      },
      "source": [
        "** **\n",
        "### LDA Implementation\n",
        "\n",
        "1. [Loading data](#load_data)\n",
        "2. [Data cleaning](#clean_data)\n",
        "3. [Exploratory analysis](#eda)\n",
        "4. [Prepare data for LDA analysis](#data_preparation)\n",
        "5. [LDA model training](#train_model)\n",
        "6. [Analyzing LDA model results](#results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OohtUYD5JsX"
      },
      "source": [
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQZUYhn25JsY"
      },
      "source": [
        "** **\n",
        "#### Step 1: Loading Data <a class=\"anchor\\\" id=\"load_data\"></a>\n",
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "hTgAeLQ35JsY",
        "outputId": "bd5a015a-f9fc-43cf-bd28-c752ddb3499d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f334e52a-aa7a-4470-9ce0-e0939d71cf75\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f334e52a-aa7a-4470-9ce0-e0939d71cf75')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f334e52a-aa7a-4470-9ce0-e0939d71cf75 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f334e52a-aa7a-4470-9ce0-e0939d71cf75');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d5992bd9-d2f2-4bd4-862a-674dc5108bf2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d5992bd9-d2f2-4bd4-862a-674dc5108bf2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d5992bd9-d2f2-4bd4-862a-674dc5108bf2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1loox8k5JsZ"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning <a class=\"anchor\\\" id=\"clean_data\"></a>\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, let's focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we'll only look at 100 papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "zKiKqpq45JsZ",
        "outputId": "1dc5c06a-a68a-4699-c8d5-a8fc57c43c28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      year                                              title  \\\n",
              "5010  2014  Transportability from Multiple Environments wi...   \n",
              "1184  1989  Generalization and Scaling in Reinforcement Le...   \n",
              "3337  2010  Generalized roof duality and bisubmodular func...   \n",
              "2031  2005  Response Analysis of Neuronal Population with ...   \n",
              "6452  1994  SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MI...   \n",
              "\n",
              "                                               abstract  \\\n",
              "5010  This paper addresses the problem of $mz$-trans...   \n",
              "1184                                   Abstract Missing   \n",
              "3337  Consider a convex relaxation $\\hat f$ of a pse...   \n",
              "2031                                   Abstract Missing   \n",
              "6452                                   Abstract Missing   \n",
              "\n",
              "                                             paper_text  \n",
              "5010  Transportability from Multiple Environments\\nw...  \n",
              "1184  550\\n\\nAckley and Littman\\n\\nGeneralization an...  \n",
              "3337  Generalized roof duality and bisubmodular func...  \n",
              "2031  Response Analysis of Neuronal Population with\\...  \n",
              "6452  SIMPLIFYING NEURAL NETS BY\\nDISCOVERING FLAT M...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b86b9fbb-732d-40ce-a684-ca2f6af976e9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5010</th>\n",
              "      <td>2014</td>\n",
              "      <td>Transportability from Multiple Environments wi...</td>\n",
              "      <td>This paper addresses the problem of $mz$-trans...</td>\n",
              "      <td>Transportability from Multiple Environments\\nw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1184</th>\n",
              "      <td>1989</td>\n",
              "      <td>Generalization and Scaling in Reinforcement Le...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>550\\n\\nAckley and Littman\\n\\nGeneralization an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3337</th>\n",
              "      <td>2010</td>\n",
              "      <td>Generalized roof duality and bisubmodular func...</td>\n",
              "      <td>Consider a convex relaxation $\\hat f$ of a pse...</td>\n",
              "      <td>Generalized roof duality and bisubmodular func...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2031</th>\n",
              "      <td>2005</td>\n",
              "      <td>Response Analysis of Neuronal Population with ...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Response Analysis of Neuronal Population with\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6452</th>\n",
              "      <td>1994</td>\n",
              "      <td>SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MI...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>SIMPLIFYING NEURAL NETS BY\\nDISCOVERING FLAT M...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b86b9fbb-732d-40ce-a684-ca2f6af976e9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b86b9fbb-732d-40ce-a684-ca2f6af976e9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b86b9fbb-732d-40ce-a684-ca2f6af976e9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fcd88cae-23b6-4dbc-a65b-1912eb363784\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fcd88cae-23b6-4dbc-a65b-1912eb363784')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fcd88cae-23b6-4dbc-a65b-1912eb363784 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1988,\n        \"max\": 2016,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          1997,\n          2001,\n          2003\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Discriminatively Trained Sparse Code Gradients for Contour Detection\",\n          \"A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation\",\n          \"Extended and Unscented Gaussian Processes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"Accurately measuring the similarity between text documents lies at the core of many real world applications of machine learning. These include web-search ranking, document recommendation, multi-lingual document matching, and article categorization. Recently, a new document metric, the word mover's distance (WMD), has been proposed with unprecedented results on kNN-based document classification. The WMD elevates high quality word embeddings to document metrics by formulating the distance between two documents as an optimal transport problem between the embedded words. However, the document distances are entirely unsupervised and lack a mechanism to incorporate supervision when available. In this paper we propose an efficient technique to learn a supervised metric, which we call the Supervised WMD (S-WMD) metric. Our algorithm learns document distances that measure the underlying semantic differences between documents by leveraging semantic differences between individual words discovered during supervised training. This is achieved with an linear transformation of the underlying word embedding space and tailored word-specific weights, learned to minimize the stochastic leave-one-out nearest neighbor classification error on a per-document level. We evaluate our metric on eight real-world text classification tasks on which S-WMD consistently  outperforms almost all of our 26 competitive baselines.\",\n          \"We address the scalability of symbolic planning under uncertainty with factored states and actions. Prior work has focused almost exclusively on factored states but not factored actions, and on value iteration (VI) compared to policy iteration (PI). Our ?rst contribution is a novel method for symbolic policy backups via the application of constraints, which is used to yield a new ef?cient symbolic imple- mentation of modi?ed PI (MPI) for factored action spaces. While this approach improves scalability in some cases, naive handling of policy constraints comes with its own scalability issues. This leads to our second and main contribution, symbolic Opportunistic Policy Iteration (OPI), which is a novel convergent al- gorithm lying between VI and MPI. The core idea is a symbolic procedure that applies policy constraints only when they reduce the space and time complexity of the update, and otherwise performs full Bellman backups, thus automatically adjusting the backup per state. We also give a memory bounded version of this algorithm allowing a space-time tradeoff. Empirical results show signi?cantly improved scalability over the state-of-the-art.\",\n          \"Owing to several applications in large scale learning and vision problems, fast submodular function minimization (SFM) has become a critical problem. Theoretically, unconstrained SFM can be performed in polynomial time (Iwata and Orlin 2009), however these algorithms are not practical. In 1976, Wolfe proposed an algorithm to find the minimum Euclidean norm point in a polytope, and in 1980, Fujishige showed how Wolfe's algorithm can be used for SFM. For general submodular functions, the Fujishige-Wolfe minimum norm algorithm seems to have the best empirical performance. Despite its good practical performance, theoretically very little is known about Wolfe's minimum norm algorithm -- to our knowledge the only result is an exponential time analysis due to Wolfe himself. In this paper we give a maiden convergence analysis of Wolfe's algorithm. We prove that in t iterations, Wolfe's algorithm returns a O(1/t)-approximate solution to the min-norm point. We also prove a robust version of Fujishige's theorem which shows that an O(1/n^2)-approximate solution to the min-norm point problem implies exact submodular minimization. As a corollary, we get the first pseudo-polynomial time guarantee for the Fujishige-Wolfe minimum norm algorithm for submodular function minimization. In particular, we show that the min-norm point algorithm solves SFM in O(n^7F^2)-time, where $F$ is an upper bound on the maximum change a single element can cause in the function value.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Discriminatively Trained Sparse Code Gradients\\nfor Contour Detection\\nXiaofeng Ren and Liefeng Bo\\nIntel Science and Technology Center for Pervasive Computing, Intel Labs\\nSeattle, WA 98195, USA\\n{xiaofeng.ren,liefeng.bo}@intel.com\\n\\nAbstract\\nFinding contours in natural images is a fundamental problem that serves as the\\nbasis of many tasks such as image segmentation and object recognition. At the\\ncore of contour detection technologies are a set of hand-designed gradient features, used by most approaches including the state-of-the-art Global Pb (gPb)\\noperator. In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure\\ncontrast using patch representations automatically learned through sparse coding.\\nWe use K-SVD for dictionary learning and Orthogonal Matching Pursuit for computing sparse codes on oriented local neighborhoods, and apply multi-scale pooling and power transforms before classifying them with linear SVMs. By extracting rich representations from pixels and avoiding collapsing them prematurely,\\nSparse Code Gradients effectively learn how to measure local contrasts and find\\ncontours. We improve the F-measure metric on the BSDS500 benchmark to 0.74\\n(up from 0.71 of gPb contours). Moreover, our learning approach can easily adapt\\nto novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth maps and surface normals lead to promising contour detection using\\ndepth and depth+color, as verified on the NYU Depth Dataset.\\n\\n1\\n\\nIntroduction\\n\\nContour detection is a fundamental problem in vision. Accurately finding both object boundaries and\\ninterior contours has far reaching implications for many vision tasks including segmentation, recognition and scene understanding. High-quality image segmentation has increasingly been relying on\\ncontour analysis, such as in the widely used system of Global Pb [2]. Contours and segmentations\\nhave also seen extensive uses in shape matching and object recognition [8, 9].\\nAccurately finding contours in natural images is a challenging problem and has been extensively\\nstudied. With the availability of datasets with human-marked groundtruth contours, a variety of\\napproaches have been proposed and evaluated (see a summary in [2]), such as learning to classify [17, 20, 16], contour grouping [23, 31, 12], multi-scale features [21, 2], and hierarchical region\\nanalysis [2]. Most of these approaches have one thing in common [17, 23, 31, 21, 12, 2]: they are\\nbuilt on top of a set of gradient features [17] measuring local contrast of oriented discs, using chisquare distances of histograms of color and textons. Despite various efforts to use generic image\\nfeatures [5] or learn them [16], these hand-designed gradients are still widely used after a decade\\nand support top-ranking algorithms on the Berkeley benchmarks [2].\\nIn this work, we demonstrate that contour detection can be vastly improved by replacing the handdesigned Pb gradients of [17] with rich representations that are automatically learned from data.\\nWe use sparse coding, in particularly Orthogonal Matching Pursuit [18] and K-SVD [1], to learn\\nsuch representations on patches. Instead of a direct classification of patches [16], the sparse codes\\non the pixels are pooled over multi-scale half-discs for each orientation, in the spirit of the Pb\\n1\\n\\n\\f+-\\n\\nSVM\\n\\nSVM\\nimage patch: gray, ab\\n\\n?\\n\\nSVM\\n\\n?\\n\\n?\\n\\nRGB-(D) contours\\nSVM\\ndepth patch (optional):\\ndepth, surface normal\\nlocal sparse coding\\n\\nmulti-scale pooling\\noriented gradients\\npower transforms\\n? linear SVM\\n\\nper-pixel\\nsparse codes\\n\\nFigure 1: We combine sparse coding and oriented gradients for contour analysis on color as well as\\ndepth images. Sparse coding automatically learns a rich representation of patches from data. With\\nmulti-scale pooling, oriented gradients efficiently capture local contrast and lead to much more\\naccurate contour detection than those using hand-designed features including Global Pb (gPb) [2].\\ngradients, before being classified with a linear SVM. The SVM outputs are then smoothed and nonmax suppressed over orientations, as commonly done, to produce the final contours (see Fig. 1).\\nOur sparse code gradients (SCG) are much more effective in capturing local contour contrast than\\nexisting features. By only changing local features and keeping the smoothing and globalization parts\\nfixed, we improve the F-measure on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb), a substantial step toward human-level accuracy (see the precision-recall curves in Fig. 4). Large improvements in accuracy are also observed on other datasets including MSRC2 and PASCAL2008. Moreover, our approach is built on unsupervised feature learning and can directly apply to novel sensor\\ndata such as RGB-D images from Kinect-style depth cameras. Using the NYU Depth dataset [27],\\nwe verify that our SCG approach combines the strengths of color and depth contour detection and\\noutperforms an adaptation of gPb to RGB-D by a large margin.\\n\\n2\\n\\nRelated Work\\n\\nContour detection has a long history in computer vision as a fundamental building block. Modern\\napproaches to contour detection are evaluated on datasets of natural images against human-marked\\ngroundtruth. The Pb work of Martin et. al. [17] combined a set of gradient features, using brightness, color and textons, to outperform the Canny edge detector on the Berkeley Benchmark (BSDS).\\nMulti-scale versions of Pb were developed and found beneficial [21, 2]. Building on top of the Pb\\ngradients, many approaches studied the globalization aspects, i.e. moving beyond local classification and enforcing consistency and continuity of contours. Ren et. al. developed CRF models on\\nsuperpixels to learn junction types [23]. Zhu et. al. used circular embedding to enforce orderings\\nof edgels [31]. The gPb work of Arbelaez et. al. computed gradients on eigenvectors of the affinity\\ngraph and combined them with local cues [2]. In addition to Pb gradients, Dollar et. al. [5] learned\\nboosted trees on generic features such as gradients and Haar wavelets, Kokkinos used SIFT features\\non edgels [12], and Prasad et. al. [20] used raw pixels in class-specific settings. One closely related\\nwork was the discriminative sparse models of Mairal et al [16], which used K-SVD to represent\\nmulti-scale patches and had moderate success on the BSDS. A major difference of our work is the\\nuse of oriented gradients: comparing to directly classifying a patch, measuring contrast between\\noriented half-discs is a much easier problem and can be effectively learned.\\nSparse coding represents a signal by reconstructing it using a small set of basis functions. It has\\nseen wide uses in vision, for example for faces [28] and recognition [29]. Similar to deep network\\napproaches [11, 14], recent works tried to avoid feature engineering and employed sparse coding of\\nimage patches to learn features from ?scratch?, for texture analysis [15] and object recognition [30,\\n3]. In particular, Orthogonal Matching Pursuit [18] is a greedy algorithm that incrementally finds\\nsparse codes, and K-SVD is also efficient and popular for dictionary learning. Closely related to our\\nwork but on the different problem of recognition, Bo et. al. used matching pursuit and K-SVD to\\nlearn features in a coding hierarchy [3] and are extending their approach to RGB-D data [4].\\n2\\n\\n\\fThanks to the mass production of Kinect, active RGB-D cameras became affordable and were\\nquickly adopted in vision research and applications. The Kinect pose estimation of Shotton et.\\nal. used random forests to learn from a huge amount of data [25]. Henry et. al. used RGB-D cameras to scan large environments into 3D models [10]. RGB-D data were also studied in the context\\nof object recognition [13] and scene labeling [27, 22]. In-depth studies of contour and segmentation\\nproblems for depth data are much in need given the fast growing interests in RGB-D perception.\\n\\n3\\n\\nContour Detection using Sparse Code Gradients\\n\\nWe start by examining the processing pipeline of Global Pb (gPb) [2], a highly influential and\\nwidely used system for contour detection. The gPb contour detection has two stages: local contrast\\nestimation at multiple scales, and globalization of the local cues using spectral grouping. The core\\nof the approach lies within its use of local cues in oriented gradients. Originally developed in\\n[17], this set of features use relatively simple pixel representations (histograms of brightness, color\\nand textons) and similarity functions (chi-square distance, manually chosen), comparing to recent\\nadvances in using rich representations for high-level recognition (e.g. [11, 29, 30, 3]).\\nWe set out to show that both the pixel representation and the aggregation of pixel information in local\\nneighborhoods can be much improved and, to a large extent, learned from and adapted to input data.\\nFor pixel representation, in Section 3.1 we show how to use Orthogonal Matching Pursuit [18] and\\nK-SVD [1], efficient sparse coding and dictionary learning algorithms that readily apply to low-level\\nvision, to extract sparse codes at every pixel. This sparse coding approach can be viewed similar\\nin spirit to the use of filterbanks but avoids manual choices and thus directly applies to the RGBD data from Kinect. We show learned dictionaries for a number of channels that exhibit different\\ncharacteristics: grayscale/luminance, chromaticity (ab), depth, and surface normal.\\nIn Section 3.2 we show how the pixel-level sparse codes can be integrated through multi-scale pooling into a rich representation of oriented local neighborhoods. By computing oriented gradients\\non this high dimensional representation and using a double power transform to code the features\\nfor linear classification, we show a linear SVM can be efficiently and effectively trained for each\\norientation to classify contour vs non-contour, yielding local contrast estimates that are much more\\naccurate than the hand-designed features in gPb.\\n3.1\\n\\nLocal Sparse Representation of RGB-(D) Patches\\n\\nK-SVD and Orthogonal Matching Pursuit. K-SVD [1] is a popular dictionary learning algorithm\\nthat generalizes K-Means and learns dictionaries of codewords from unsupervised data. Given a set\\nof image patches Y = [y1 , ? ? ? , yn ], K-SVD jointly finds a dictionary D = [d1 , ? ? ? , dm ] and an\\nassociated sparse code matrix X = [x1 , ? ? ? , xn ] by minimizing the reconstruction error\\nmin kY ? DXk2F\\nD,X\\n\\ns.t. ?i, kxi k0 ? K; ?j, kdj k2 = 1\\n\\n(1)\\n\\nwhere k ? kF denotes the Frobenius norm, xi are the columns of X, the zero-norm k ? k0 counts the\\nnon-zero entries in the sparse code xi , and K is a predefined sparsity level (number of non-zero entries). This optimization can be solved in an alternating manner. Given the dictionary D, optimizing\\nthe sparse code matrix X can be decoupled to sub-problems, each solved with Orthogonal Matching\\nPursuit (OMP) [18], a greedy algorithm for finding sparse codes. Given the codes X, the dictionary\\nD and its associated sparse coefficients are updated sequentially by singular value decomposition.\\nFor our purpose of representing local patches, the dictionary D has a small size (we use 75 for 5x5\\npatches) and does not require a lot of sample patches, and it can be learned in a matter of minutes.\\nOnce the dictionary D is learned, we again use the Orthogonal Matching Pursuit (OMP) algorithm\\nto compute sparse codes at every pixel. This can be efficiently done with convolution and a batch\\nversion of the OMP algorithm [24]. For a typical BSDS image of resolution 321x481, the sparse\\ncode extraction is efficient and takes 1?2 seconds.\\nSparse Representation of RGB-D Data. One advantage of unsupervised dictionary learning is\\nthat it readily applies to novel sensor data, such as the color and depth frames from a Kinect-style\\nRGB-D camera. We learn K-SVD dictionaries up to four channels of color and depth: grayscale\\nfor luminance, chromaticity ab for color in the Lab space, depth (distance to camera) and surface\\nnormal (3-dim). The learned dictionaries are visualized in Fig. 2. These dictionaries are interesting\\n3\\n\\n\\f(a) Grayscale\\n\\n(b) Chromaticity (ab)\\n\\n(c) Depth\\n\\n(d) Surface normal\\n\\nFigure 2: K-SVD dictionaries learned for four different channels: grayscale and chromaticity (in\\nab) for an RGB image (a,b), and depth and surface normal for a depth image (c,d). We use a fixed\\ndictionary size of 75 on 5x5 patches. The ab channel is visualized using a constant luminance of 50.\\nThe 3-dimensional surface normal (xyz) is visualized in RGB (i.e. blue for frontal-parallel surfaces).\\nto look at and qualitatively distinctive: for example, the surface normal codewords tend to be more\\nsmooth due to flat surfaces, the depth codewords are also more smooth but with speckles, and the\\nchromaticity codewords respect the opponent color pairs. The channels are coded separately.\\n3.2\\n\\nCoding Multi-Scale Neighborhoods for Measuring Contrast\\n\\nMulti-Scale Pooling over Oriented Half-Discs. Over decades of research on contour detection and\\nrelated topics, a number of fundamental observations have been made, repeatedly: (1) contrast is\\nthe key to differentiate contour vs non-contour; (2) orientation is important for respecting contour\\ncontinuity; and (3) multi-scale is useful. We do not wish to throw out these principles. Instead, we\\nseek to adopt these principles for our case of high dimensional representations with sparse codes.\\nEach pixel is presented with sparse codes extracted from a small patch (5-by-5) around it. To aggregate pixel information, we use oriented half-discs as used in gPb (see an illustration in Fig. 1). Each\\norientation is processed separately. For each orientation, at each pixel p and scale s, we define two\\nhalf-discs (rectangles) N a and N b of size s-by-(2s+1), on both sides of p, rotated to that orientation. For each half-disc N , we use average pooling on non-zero entries (i.e. a hybrid of average and\\nmax pooling) to generate its representation\\n\\\"\\n,\\n,\\n#\\nX\\nX\\nX\\nX\\nF (N ) =\\n|xi1 |\\nI|xi1 |>0 , ? ? ? ,\\n|xim |\\nI|xim |>0\\n(2)\\ni?N\\n\\ni?N\\n\\ni?N\\n\\ni?N\\n\\nwhere xij is the j-th entry of the sparse code xi , and I is the indicator function whether xij is nonzero. We rotate the image (after sparse coding) and use integral images for fast computations (on\\nboth |xij | and |xij | > 0, whose costs are independent of the size of N .\\nFor two oriented half-dics N a and N b at a scale s, we compute a difference (gradient) vector D\\n\\f\\n\\f\\nD(Nsa , Nsb ) = \\fF (Nsa ) ? F (Nsb )\\f\\n(3)\\nwhere | ? | is an element-wise absolute value operation. We divide D(Nsa , Nsb ) by their norms\\nkF (Nsa )k + kF (Nsb )k + \\u000f, where \\u000f is a positive number. Since the magnitude of sparse codes varies\\nover a wide range due to local variations in illumination as well as occlusion, this step makes the\\nappearance features robust to such variations and increases their discriminative power, as commonly\\ndone in both contour detection and object recognition. This value is not hard to set, and we find a\\nvalue of \\u000f = 0.5 is better than, for instance, \\u000f = 0.\\nAt this stage, one could train a classifier on D for each scale to convert it to a scalar value of\\ncontrast, which would resemble the chi-square distance function in gPb. Instead, we find that it is\\nmuch better to avoid doing so separately at each scale, but combining multi-scale features in a joint\\nrepresentation, so as to allow interactions both between codewords and between scales. That is, our\\nfinal representation of the contrast at a pixel p is the concatenation of sparse codes pooled at all the\\n4\\n\\n\\fscales s ? {1, ? ? ? , S} (we use S = 4):\\n\\u0002\\n\\u0003\\nDp = D(N1a , N1b ), ? ? ? , D(NSa , NSb ); F (N1a ? N1b ), ? ? ? , F (NSa ? NSb )\\n\\n(4)\\n\\nIn addition to difference D, we also include a union term F (Nsa ? Nsb ), which captures the appearance of the whole disc (union of the two half discs) and is normalized by kF (Nsa )k + kF (Nsb )k + \\u000f.\\nDouble Power Transform and Linear Classifiers. The concatenated feature Dp (non-negative)\\nprovides multi-scale contrast information for classifying whether p is a contour location for a particular orientation. As Dp is high dimensional (1200 and above in our experiments) and we need to do\\nit at every pixel and every orientation, we prefer using linear SVMs for both efficient testing as well\\nas training. Directly learning a linear function on Dp , however, does not work very well. Instead,\\nwe apply a double power transformation to make the features more suitable for linear SVMs\\n\\u0002\\n\\u0003\\nDp = Dp?1 , Dp?2\\n(5)\\nwhere 0<?1 <?2 <1. Empirically, we find that the double power transform works much better\\nthan either no transform or a single power transform ?, as sometimes done in other classification\\ncontexts. Perronnin et. al. [19] provided an intuition why a power transform helps classification,\\nwhich ?re-normalizes? the distribution of the features into a more Gaussian form. One plausible\\nintuition for a double power transform is that the optimal exponent ? may be different across feature\\ndimensions. By putting two power transforms of Dp together, we allow the classifier to pick its\\nlinear combination, different for each dimension, during the stage of supervised training.\\nFrom Local Contrast to Global Contours. We intentionally only change the local contrast estimation in gPb and keep the other steps fixed. These steps include: (1) the Savitzky-Goley filter\\nto smooth responses and find peak locations; (2) non-max suppression over orientations; and (3)\\noptionally, we apply the globalization step in gPb that computes a spectral gradient from the local\\ngradients and then linearly combines the spectral gradient with the local ones. A sigmoid transform\\nstep is needed to convert the SVM outputs on Dp before computing spectral gradients.\\n\\n4\\n\\nExperiments\\n\\nWe use the evaluation framework of, and extensively compare to, the publicly available Global\\nPb (gPb) system [2], widely used as the state of the art for contour detection1 . All the results\\nreported on gPb are from running the gPb contour detection and evaluation codes (with default\\nparameters), and accuracies are verified against the published results in [2]. The gPb evaluation\\nincludes a number of criteria, including precision-recall (P/R) curves from contour matching (Fig. 4),\\nF-measures computed from P/R (Table 1,2,3) with a fixed contour threshold (ODS) or per-image\\nthresholds (OIS), as well as average precisions (AP) from the P/R curves.\\nBenchmark Datasets. The main dataset we use is the BSDS500 benchmark [2], an extension of the\\noriginal BSDS300 benchmark and commonly used for contour evaluation. It includes 500 natural\\nimages of roughly resolution 321x481, including 200 for training, 100 for validation, and 200 for\\ntesting. We conduct both color and grayscale experiments (where we convert the BSDS500 images\\nto grayscale and retain the groundtruth). In addition, we also use the MSRC2 and PASCAL2008\\nsegmentation datasets [26, 6], as done in the gPb work [2]. The MSRC2 dataset has 591 images of\\nresolution 200x300; we randomly choose half for training and half for testing. The PASCAL2008\\ndataset includes 1023 images in its training and validation sets, roughly of resolution 350x500. We\\nrandomly choose half for training and half for testing.\\nFor RGB-D contour detection, we use the NYU Depth dataset (v2) [27], which includes 1449 pairs\\nof color and depth frames of resolution 480x640, with groundtruth semantic regions. We choose\\n60% images for training and 40% for testing, as in its scene labeling setup. The Kinect images are\\nof lower quality than BSDS, and we resize the frames to 240x320 in our experiments.\\nTraining Sparse Code Gradients. Given sparse codes from K-SVD and Orthogonal Matching Pursuit, we train the Sparse Code Gradients classifiers, one linear SVM per orientation, from sampled\\nlocations. For positive data, we sample groundtruth contour locations and estimate the orientations\\nat these locations using groundtruth. For negative data, locations and orientations are random. We\\nsubtract the mean from the patches in each data channel. For BSDS500, we typically have 1.5 to 2\\n1\\n\\nIn this work we focus on contour detection and do not address how to derive segmentations from contours.\\n\\n5\\n\\n\\f0.94\\n0.92\\n\\n0.88\\n0.86\\n0.84\\n\\n0.8\\n\\n0.9\\n0.88\\n0.86\\n\\nhorizontal edge\\n45?deg edge\\nvertical edge\\n135?deg edge\\n\\n0.84\\n\\n0.82\\n2\\n\\n3\\n\\n4\\n5\\n7\\n10\\npooling disc size (pixel)\\n\\n14\\n\\naverage precision\\n\\n0.92\\n\\n0.9\\n\\naverage precision\\n\\naverage precision\\n\\nsingle scale\\naccum. scale\\n\\n0.82\\n\\n25\\n\\n50\\n\\n(a)\\n\\n75\\n100\\ndictionary size\\n\\n0.9\\n\\n0.88\\ngray\\ncolor (ab)\\ngray+color\\n\\n0.86\\n\\n125\\n\\n0.84\\n\\n150\\n\\n1\\n\\n2\\n\\n3\\n\\n(b)\\n\\n4\\n5\\n6\\nsparsity level\\n\\n7\\n\\n8\\n\\n(c)\\n\\nFigure 3: Analysis of our sparse code gradients, using average precision of classification on sampled\\nboundaries. (a) The effect of single-scale vs multi-scale pooling (accumulated from the smallest).\\n(b) Accuracy increasing with dictionary size, for four orientation channels. (c) The effect of the\\nsparsity level K, which exhibits different behavior for grayscale and chromaticity.\\n\\nglobal\\n\\ngPb (gray)\\nSCG (gray)\\ngPb (color)\\nSCG (color)\\ngPb (gray)\\nSCG (gray)\\ngPb (color)\\nSCG (color)\\n\\nBSDS500\\nODS OIS\\n.67\\n.69\\n.69\\n.71\\n.70\\n.72\\n.72\\n.74\\n.69\\n.71\\n.71\\n.73\\n.71\\n.74\\n.74\\n.76\\n\\n0.9\\n\\nAP\\n.68\\n.71\\n.71\\n.75\\n.67\\n.74\\n.72\\n.77\\n\\n0.8\\n\\n0.7\\nPrecision\\n\\nlocal\\n\\n1\\n\\n0.6\\n\\ngPb (gray) F=0.69\\ngPb (color) F=0.71\\nSCG (gray) F=0.71\\nSCG (color) F=0.74\\n\\n0.5\\n\\n0.4\\n\\n0.3\\n\\n0.2\\n0\\n\\nTable 1: F-measure evaluation on the BSDS500\\nbenchmark [2], comparing to gPb on grayscale\\nand color images, both for local contour detection as well as for global detection (i.e. combined with the spectral gradient analysis in [2]).\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\nRecall\\n\\nFigure 4: Precision-recall curves of SCG vs\\ngPb on BSDS500, for grayscale and color\\nimages. We make a substantial step beyond\\nthe current state of the art toward reaching\\nhuman-level accuracy (green dot).\\n\\nmillion data points. We use 4 spatial scales, at half-disc sizes 2, 4, 7, 25. For a dictionary size of 75\\nand 4 scales, the feature length for one data channel is 1200. For full RGB-D data, the dimension is\\n4800. For BSDS500, we train only using the 200 training images. We modify liblinear [7] to take\\ndense matrices (features are dense after pooling) and single-precision floats.\\nLooking under the Hood. We empirically analyze a number of settings in our Sparse Code Gradients. In particular, we want to understand how the choices in the local sparse coding affect contour\\nclassification. Fig. 3 shows the effects of multi-scale pooling, dictionary size, and sparsity level\\n(K). The numbers reported are intermediate results, namely the mean of average precision of four\\noriented gradient classifier (0, 45, 90, 135 degrees) on sampled locations (grayscale unless otherwise\\nnoted, on validation). As a reference, the average precision of gPb on this task is 0.878.\\nFor multi-scale pooling, the single best scale for the half-disc filter is about 4x8, consistent with\\nthe settings in gPb. For accumulated scales (using all the scales from the smallest up to the current\\nlevel), the accuracy continues to increase and does not seem to be saturated, suggesting the use of\\nlarger scales. The dictionary size has a minor impact, and there is a small (yet observable) benefit to\\nuse dictionaries larger than 75, particularly for diagonal orientations (45- and 135-deg). The sparsity\\nlevel K is a more intriguing issue. In Fig. 3(c), we see that for grayscale only, K = 1 (normalized\\nnearest neighbor) does quite well; on the other hand, color needs a larger K, possibly because ab is\\na nonlinear space. When combining grayscale and color, it seems that we want K to be at least 3. It\\nalso varies with orientation: horizontal and vertical edges require a smaller K than diagonal edges.\\n(If using K = 1, our final F-measure on BSDS500 is 0.730.)\\nWe also empirically evaluate the double power transform vs single power transform vs no transform.\\nWith no transform, the average precision is 0.865. With a single power transform, the best choice of\\nthe exponent is around 0.4, with average precision 0.884. A double power transform (with exponents\\n6\\n\\n\\fgPb\\nSCG\\n\\ngPb\\nSCG\\n\\nMSRC2\\nODS OIS AP\\n.37\\n.39 .22\\n.43\\n.43 .33\\nPASCAL2008\\nODS OIS AP\\n.34\\n.38 .20\\n.37\\n.41 .27\\n\\ngPb (color)\\nSCG (color)\\ngPb (depth)\\nSCG (depth)\\ngPb (RGB-D)\\nSCG (RGB-D)\\n\\nTable 2: F-measure evaluation comparing\\nour SCG approach to gPb on two additional image datasets with contour groundtruth:\\nMSRC2 [26] and PASCAL2008 [6].\\n\\nRGB-D (NYU v2)\\nODS OIS AP\\n.51\\n.52 .37\\n.55\\n.57 .46\\n.44\\n.46 .28\\n.53\\n.54 .45\\n.53\\n.54 .40\\n.62\\n.63 .54\\n\\nTable 3: F-measure evaluation on RGB-D contour detection using the NYU dataset (v2) [27].\\nWe compare to gPb on using color image only,\\ndepth only, as well as color+depth.\\n\\nFigure 5: Examples from the BSDS500 dataset [2]. (Top) Image; (Middle) gPb output; (Bottom)\\nSCG output (this work). Our SCG operator learns to preserve fine details (e.g. windmills, faces, fish\\nfins) while at the same time achieving higher precision on large-scale contours (e.g. back of zebras).\\n(Contours are shown in double width for the sake of visualization.)\\n0.25 and 0.75, which can be computed through sqrt) improves the average precision to 0.900, which\\ntranslates to a large improvement in contour detection accuracy.\\nImage Benchmarking Results. In Table 1 and Fig. 4 we show the precision-recall of our Sparse\\nCode Gradients vs gPb on the BSDS500 benchmark. We conduct four sets of experiments, using\\ncolor or grayscale images, with or without the globalization component (for which we use exactly\\nthe same setup as in gPb). Using Sparse Code Gradients leads to a significant improvement in\\naccuracy in all four cases. The local version of our SCG operator, i.e. only using local contrast, is\\nalready better (F = 0.72) than gPb with globalization (F = 0.71). The full version, local SCG plus\\nspectral gradient (computed from local SCG), reaches an F-measure of 0.739, a large step forward\\nfrom gPb, as seen in the precision-recall curves in Fig. 4. On BSDS300, our F-measure is 0.715.\\nWe observe that SCG seems to pick up fine-scale details much better than gPb, hence the much\\nhigher recall rate, while maintaining higher precision over the entire range. This can be seen in the\\nexamples shown in Fig. 5. While our scale range is similar to that of gPb, the multi-scale pooling\\nscheme allows the flexibility of learning the balance of scales separately for each code word, which\\nmay help detecting the details. The supplemental material contains more comparison examples.\\nIn Table 2 we show the benchmarking results for two additional datasets, MSRC2 and PASCAL2008. Again we observe large improvements in accuracy, in spite of the somewhat different\\nnatures of the scenes in these datasets. The improvement on MSRC2 is much larger, partly because\\nthe images are smaller, hence the contours are smaller in scale and may be over-smoothed in gPb.\\nAs for computational cost, using integral images, local SCG takes ?100 seconds to compute on a\\nsingle-thread Intel Core i5-2500 CPU on a BSDS image. It is slower than but comparable to the\\nhighly optimized multi-thread C++ implementation of gPb (?60 seconds).\\n7\\n\\n\\fFigure 6: Examples of RGB-D contour detection on the NYU dataset (v2) [27]. The five panels\\nare: input image, input depth, image-only contours, depth-only contours, and color+depth contours.\\nColor is good picking up details such as photos on the wall, and depth is useful where color is\\nuniform (e.g. corner of a room, row 1) or illumination is poor (e.g. chair, row 2).\\nRGB-D Contour Detection. We use the second version of the NYU Depth Dataset [27], which\\nhas higher quality groundtruth than the first version. A median filtering is applied to remove double\\ncontours (boundaries from two adjacent regions) within 3 pixels. For RGB-D baseline, we use a\\nsimple adaptation of gPb: the depth values are in meters and used directly as a grayscale image\\nin gPb gradient computation. We use a linear combination to put (soft) color and depth gradients\\ntogether in gPb before non-max suppression, with the weight set from validation.\\nTable 3 lists the precision-recall evaluations of SCG vs gPb for RGB-D contour detection. All\\nthe SCG settings (such as scales and dictionary sizes) are kept the same as for BSDS. SCG again\\noutperforms gPb in all the cases. In particular, we are much better for depth-only contours, for\\nwhich gPb is not designed. Our approach learns the low-level representations of depth data fully\\nautomatically and does not require any manual tweaking. We also achieve a much larger boost by\\ncombining color and depth, demonstrating that color and depth channels contain complementary\\ninformation and are both critical for RGB-D contour detection. Qualitatively, it is easy to see that\\nRGB-D combines the strengths of color and depth and is a promising direction for contour and\\nsegmentation tasks and indoor scene analysis in general [22]. Fig. 6 shows a few examples of RGBD contours from our SCG operator. There are plenty of such cases where color alone or depth alone\\nwould fail to extract contours for meaningful parts of the scenes, and color+depth would succeed.\\n\\n5\\n\\nDiscussions\\n\\nIn this work we successfully showed how to learn and code local representations to extract contours\\nin natural images. Our approach combined the proven concept of oriented gradients with powerful\\nrepresentations that are automatically learned through sparse coding. Sparse Code Gradients (SCG)\\nperformed significantly better than hand-designed features that were in use for a decade, and pushed\\ncontour detection much closer to human-level accuracy as illustrated on the BSDS500 benchmark.\\nComparing to hand-designed features (e.g. Global Pb [2]), we maintain the high dimensional representation from pooling oriented neighborhoods and do not collapse them prematurely (such as\\ncomputing chi-square distance at each scale). This passes a richer set of information into learning contour classification, where a double power transform effectively codes the features for linear\\nSVMs. Comparing to previous learning approaches (e.g. discriminative dictionaries in [16]), our\\nuses of multi-scale pooling and oriented gradients lead to much higher classification accuracies.\\nOur work opens up future possibilities for learning contour detection and segmentation. As we illustrated, there is a lot of information locally that is waiting to be extracted, and a learning approach\\nsuch as sparse coding provides a principled way to do so, where rich representations can be automatically constructed and adapted. This is particularly important for novel sensor data such as RGB-D,\\nfor which we have less understanding but increasingly more need.\\n8\\n\\n\\fReferences\\n[1] M. Aharon, M. Elad, and A. Bruckstein. K-SVD: An algorithm for designing overcomplete dictionaries\\nfor sparse representation. IEEE Transactions on Signal Processing, 54(11):4311?4322, 2006.\\n[2] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation.\\nIEEE Trans. PAMI, 33(5):898?916, 2011.\\n[3] L. Bo, X. Ren, and D. Fox. Hierarchical Matching Pursuit for Image Classification: Architecture and Fast\\nAlgorithms. In Advances in Neural Information Processing Systems 24, 2011.\\n[4] L. Bo, X. Ren, and D. Fox. Unsupervised Feature Learning for RGB-D Based Object Recognition. In\\nInternational Symposium on Experimental Robotics (ISER), 2012.\\n[5] P. Dollar, Z. Tu, and S. Belongie. Supervised learning of edges and object boundaries. In CVPR, volume 2,\\npages 1964?71, 2006.\\n[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object\\nClasses Challenge 2008 (VOC2008). http://www.pascal-network.org/challenges/VOC/voc2008/.\\n[7] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. Liblinear: A library for large linear classification. The\\nJournal of Machine Learning Research, 9:1871?1874, 2008.\\n[8] V. Ferrari, T. Tuytelaars, and L. V. Gool. Object detection by contour segment networks. In ECCV, pages\\n14?28, 2006.\\n[9] C. Gu, J. Lim, P. Arbel?aez, and J. Malik. Recognition using regions. In CVPR, pages 1030?1037, 2009.\\n[10] P. Henry, M. Krainin, E. Herbst, X. Ren, and D. Fox. Rgb-d mapping: Using depth cameras for dense 3d\\nmodeling of indoor environments. In International Symposium on Experimental Robotics (ISER), 2010.\\n[11] G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural computation,\\n18(7):1527?1554, 2006.\\n[12] I. Kokkinos. Highly accurate boundary detection and grouping. In CVPR, pages 2520?2527, 2010.\\n[13] K. Lai, L. Bo, X. Ren, and D. Fox. A large-scale hierarchical multi-view RGB-D object dataset. In ICRA,\\npages 1817?1824, 2011.\\n[14] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, pages 609?616, 2009.\\n[15] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Discriminative learned dictionaries for local\\nimage analysis. In CVPR, pages 1?8, 2008.\\n[16] J. Mairal, M. Leordeanu, F. Bach, M. Hebert, and J. Ponce. Discriminative sparse image models for\\nclass-specific edge detection and image interpretation. ECCV, pages 43?56, 2008.\\n[17] D. Martin, C. Fowlkes, and J. Malik. Learning to detect natural image boundaries using brightness and\\ntexture. In Advances in Neural Information Processing Systems 15, 2002.\\n[18] Y. Pati, R. Rezaiifar, and P. Krishnaprasad. Orthogonal Matching Pursuit: Recursive Function Approximation with Applications to Wavelet Decomposition. In The Twenty-Seventh Asilomar Conference on\\nSignals, Systems and Computers, pages 40?44, 1993.\\n[19] F. Perronnin, J. S?anchez, and T. Mensink. Improving the fisher kernel for large-scale image classification.\\nIn ECCV, pages 143?156, 2010.\\n[20] M. Prasad, A. Zisserman, A. Fitzgibbon, M. Kumar, and P. Torr. Learning class-specific edges for object\\ndetection and segmentation. Computer Vision, Graphics and Image Processing, pages 94?105, 2006.\\n[21] X. Ren. Multi-scale improves boundary detection in natural images. In ECCV, pages 533?545, 2008.\\n[22] X. Ren, L. Bo, and D. Fox. RGB-(D) scene labeling: features and algorithms. In Computer Vision and\\nPattern Recognition (CVPR), 2012 IEEE Conference on, pages 2759?2766. IEEE, 2012.\\n[23] X. Ren, C. Fowlkes, and J. Malik. Cue integration in figure/ground labeling. In Advances in Neural\\nInformation Processing Systems 18, 2005.\\n[24] R. Rubinstein, M. Zibulevsky, and M. Elad. Efficient Implementation of the K-SVD Algorithm using\\nBatch Orthogonal Matching Pursuit. Technical report, CS Technion, 2008.\\n[25] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Realtime human pose recognition in parts from single depth images. In CVPR, volume 2, page 3, 2011.\\n[26] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost: Joint appearance, shape and context\\nmodeling for multi-class object recognition and segmentation. In ECCV, 2006.\\n[27] N. Silberman and R. Fergus. Indoor scene segmentation using a structured light sensor. In IEEE Workshop\\non 3D Representation and Recognition (3dRR), 2011.\\n[28] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. Robust face recognition via sparse representation.\\nIEEE Trans. PAMI, 31(2):210?227, 2009.\\n[29] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding for image\\nclassification. In CVPR, pages 1794?1801, 2009.\\n[30] K. Yu, Y. Lin, and J. Lafferty. Learning image representations from the pixel level via hierarchical sparse\\ncoding. In CVPR, pages 1713?1720, 2011.\\n[31] Q. Zhu, G. Song, and J. Shi. Untangling cycles for contour grouping. In ICCV, 2007.\\n\\n9\\n\\n\\f\",\n          \"A Bayesian Model for Simultaneous Image\\nClustering, Annotation and Object Segmentation\\n\\nLan Du, Lu Ren, 1 David B. Dunson and Lawrence Carin\\nDepartment of Electrical and Computer Engineering\\n1\\nStatistics Department\\nDuke University\\nDurham, NC 27708-0291, USA\\n{ld53, lr, lcarin}@ee.duke.edu, dunson@stats.duke.edu\\n\\nAbstract\\nA non-parametric Bayesian model is proposed for processing multiple images.\\nThe analysis employs image features and, when present, the words associated\\nwith accompanying annotations. The model clusters the images into classes, and\\neach image is segmented into a set of objects, also allowing the opportunity to\\nassign a word to each object (localized labeling). Each object is assumed to be\\nrepresented as a heterogeneous mix of components, with this realized via mixture\\nmodels linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models\\nare inferred nonparametrically. To constitute spatially contiguous objects, a new\\nlogistic stick-breaking process is developed. Inference is performed efficiently\\nvia variational Bayesian analysis, with example results presented on two image\\ndatabases.\\n\\n1 Introduction\\nThere has recently been much interest in developing statistical models for analyzing and organizing images, based on image features and, when available, auxiliary information, such as words\\n(e.g., annotations). Three important aspects of this problem are: (i) sorting multiple images\\ninto scene-level classes, (ii) image annotation, and (iii) segmenting and labeling localized objects\\nwithin images. Probabilistic topic models, originally developed for text analysis [8, 12], have been\\nadapted and extended successfully for many image-understanding problems [3, 6, 9?11, 16, 23, 24].\\nMoreover, recent work has also used the Dirichlet process (DP) [5] or similar non-parametric priors to enhance the topic-model structure [2, 20, 26]. Using such statistical models, researchers\\n[2, 3, 6, 10, 16, 20, 23, 24, 26] have addressed two or all three of the objectives simultaneously\\nwithin a single setting. Such unified formalisms have realized marked improvements in overall algorithm performance. A relatively complete summary of the literature may be found in [16, 23],\\nwhere the advantages of the approaches in [16, 23] are described relative to previous related approaches [3, 6, 10, 11, 18, 24, 27]. The work in [16, 23] is based on the correspondence LDA\\n(Corr-LDA) model [6]. The approach in [23] integrates the Corr-LDA model and the supervised\\nLDA (sLDA) model [7] into a single framework. Although good classification performance was\\nachieved using this approach, the model is employed in a supervised manner, utilizing scene-labeled\\nimages for scene classification. A class label variable is introduced in [16] to cluster all images in\\nan unsupervised manner, and a switching variable to address noisy annotations. Nevertheless, to\\nimprove performance, in [16] some images are required for supervised learning, based on the segmented and labeled objects obtained via the method proposed in [10], with these used to initialize\\nthe algorithm.\\nThe research reported here seeks to build upon and extend recent research on unified image-analysis\\nmodels. Specifically, motivated by [16, 23], we develop a novel non-parametric Bayesian model\\n1\\n\\n\\fthat simultaneously addresses all three objectives discussed above. The four main contributions of\\nthis paper are:\\n? Each object in an image is represented as a mixture of image-feature model parameters, accounting for the heterogeneous character of individual objects. This framework captures the idea that a\\nparticular object may be composed as an aggregation of distinct parts. By contrast, each object is\\nonly associated with one image-feature component/atom in the Corr-LDA-like models [6, 16, 23].\\n? Multiple images are processed jointly; all, none or a subset of the images may be annotated. The\\nmodel infers the linkage between image-feature parameters and object types, with this linkage used\\nto yield localized labeling of objects within all images. The unsupervised framework is executed\\nwithout the need for a human to constitute training data.\\n? A novel logistic stick-breaking process (LSBP) is proposed, imposing the belief that proximate\\nportions of an image are more likely to reside within the same segment (object). This spatially constrained prior yields contiguous objects with sharp boundaries, and via the aforementioned mixture\\nmodels the segmented objects may be composed of heterogeneous building blocks.\\n? The proposed model is nonparametric, based on use of stick-breaking constructions [13], which\\ncan be easily implemented by fast variational Bayesian (VB) inference [14]. The number of image\\nclasses, number of object types, number of image-feature mixture components per object, and the\\nlinkage between words and image model parameters are inferred nonparametrically.\\n\\n2 The Hierarchical Generative Model\\n2.1 Bag of image features\\nWe jointly process data from M images, and each image is assumed to come from an associated\\nclass type (e.g., city scene, beach scene, office scene, etc.). The class type associated with image m\\nis denoted by zm ? {1, . . . , I}, and it is drawn from the mixture model\\nI\\nX\\nzm ?\\nui ?i , u ? StickI (?u )\\n(1)\\ni=1\\n\\nwhere StickI (?u ) is a stick-breaking process [13] that is truncated to I sticks, with hyper-parameter\\n?u > 0. The symbol ?i represents a unit measure at the integer i, and the parameter ui denotes the\\nprobability that image type i will be observed across the M images.\\nThe observed data are image feature vectors, each tied to a local region in the image (for example,\\nassociated with an over-segmented portion of the image). The Lm observed image feature vectors\\nm\\nassociated with image m are {xml }L\\nl=1 , and the lth feature vector is assumed drawn xml ? F (? ml ).\\nThe expression F (?) represents the feature model, and ? ml represents the model parameters.\\nEach image is assumed to be composed of a set of latent objects. An indicator variable ?ml defines\\nwhich object type the lth feature vector from image m is associated with, and it is drawn\\nK\\nX\\n?ml ?\\nwzm k ?k , w i ? StickK (?w )\\n(2)\\nk=1\\n\\nwhere index k corresponds to the kth type of object that may reside within an image. The vector\\nwi defines the probability that each of the K object types will occur, conditioned on the image type\\ni ? {1, . . . , I}; the kth component of w zm , wzm k , denotes the probability of observing object type\\nk in image m, when image m was drawn from class zm ? {1, . . . , I}.\\nm\\nThe image class zm and corresponding objects {?ml }L\\nl=1 associated with image m are latent varim\\nables. The generative process for the observed data, {xml }L\\nl=1 , is manifested via mixture models\\nwith respect to model parameter ?. Specifically, a separate such mixture model is manifested for\\neach of the K object types, motivated by the idea that each object will in general be composed of a\\ndifferent set of image-feature building blocks. The mixture model for object type k ? {1, . . . , K}\\nis represented as\\nJ\\nX\\nGk =\\nhkj ???j , hk ? StickJ (?h ) , ??j ? H\\n(3)\\n\\nj=1\\n\\nwhere H is a base measure, usually selected to be conjugate to F (?).\\n2.2 Bag of clustered image features\\nWhile the model described above is straightforward to understand, it has been found to be ineffecPK\\ntive. This is because each of the ?ml is drawn i.i.d. from k=1 wzm k ?k , and therefore there is\\n2\\n\\n\\fnothing in the model that encourages the image features, xml and xml? , which are associated with\\nthe same image-feature atom ??j , to be assigned to the same object k.\\nTo address this limitation, we add a clustering step within each of the images; this is similar to\\nthe structure of the hierarchical Dirichlet process (HDP) [21]. Specifically, consider the following\\naugmented model:\\nT\\nK\\nI\\nX\\nX\\nX\\nxml ? F (? ml ) , ?ml ? Gcml , cml ?\\nvmt ??mt , ?mt ?\\nwzm k ?k , zm ?\\nui ?i (4)\\nt=1\\n\\nk=1\\n\\ni=1\\n\\nwhere v m ? StickT (?v ), and Gk is as defined in (3). We make truncation level T < K, to\\nencourage a relatively small number of objects in a given image.\\n2.3 Linking words with images\\nIn the above discussion it was assumed that the only observed data are the image feature vectors\\nm\\n{xml }L\\nl=1 . However, there are situations for which annotations (words) may be available for at\\nleast a subset of the M images. In this setting we assume that we have a K-dimensional dictionary\\nof words associated with objects in images, and a word is assigned to each of the objects k ?\\n{1, . . . , K}. Of the collection of M images, some may be annotated and some not, and all will\\nbe processed simultaneously by the joint model; in so doing, annotations will be inferred for the\\noriginally non-annotated images.\\nFor an image for which no annotation is given, the image is assumed generated via (4). When\\nan annotation is available, the words associated with image m are represented as a vector y m =\\n[ym1 , ? ? ? , ymK ]T , where ymk denotes the number of times word k is present in the annotation to\\nimage m (typically ymk will either be one or zero), and y m is assumed drawn from a multinomial\\ndistribution associated with a parameter ?m : y m ? Mult(?m ). If image m is in class zm , then we\\nsimply set\\ny m ? Mult(wzm ) , wi ? StickK (?w )\\n(5)\\nNamely, ?m = w zm , recalling that wi defines the probability of observing each object type\\nfor image class i. When a dictionary of K words is available, we generally use wi ?\\nDir(?w /K, . . . , ?w /K), consistent with LDA [8].\\n\\n3 Encouraging Spatially Contiguous Objects\\n3.1 Logistic stick-breaking process (LSBP)\\nIn (5), note that once the image class zm is drawn for image m, the order/location of the xml within\\nthe image may be interchanged, and nothing in the generative process will change. This is because\\nthe indicator variable cml , which defines the object class associated with feature vector l in image m,\\nPT\\nis drawn i.i.d. cml ? t=1 vmt ??mt . It is therefore desirable to impose that if two feature vectors\\nare proximate within the image, they are likely to be associated with the same object.\\nWith each feature vector xml there is an associated spatial location, which we denote sml (this is a\\ntwo-dimensional vector). We wish to draw\\nT\\nK\\nX\\nX\\ncml ?\\nvmt (sml )??mt , ?mt ?\\nwzm k ?k\\n(6)\\nt=1\\n\\nk=1\\n\\nwhere the cluster probabilities vmt (sml ) are now a function of position sml (the ?mt ? {1, . . . , K}\\ncorrespond to object types). The challenge, therefore, becomes development of a means of constructing vmt (s) to encourage nearby feature vectors to come from the same object type. Toward this goal,\\nlet ?[gmt (s)] represent a logistic link function, which is a function of s. For t = 1, . . . , T ? 1 we\\nimpose\\nt?1\\nY\\nvmt (s) = ?[gmt (s)]\\n{1 ? ?[gm? (s)]}\\n(7)\\nPT ?1\\n\\n? =1\\n\\nPLm (m)\\n(m)\\nwhere vmT (s) = 1 ? t=1 vmt (s). We define gmt (s) =\\nl=1 Wtl K(s, sml ) + Wt0\\nwhere K(s, sml ) is a kernel, and here we utilize the radial basis function kernel K(s, sml ) =\\nexp[?ks ? sml k2 /?mt ]. The parameter kernel width ?mt plays an important role in dictating the\\nsize of segments associated with stick t, and therefore these parameters should be learned by the\\ndata in the analysis. In practice we define a library of discrete kernel widths ?? = {??d }D\\nd=1 , and\\ninfer each ?mt , placing a uniform prior on the elements of ?? .\\n3\\n\\n\\fWe desire that a given stick vmt (s) has importance (at most) over a localized region, and therefore\\n(m)\\n(m)\\n(m)\\nm\\nwe impose sparseness priors on parameters {Wtl }L\\n? N (0, (?tl )?1 ), and\\nl=0 . Specifically, Wtl\\n(m)\\n(m)\\n?tl is drawn from a gamma prior, with hyper-parameters set to encourage most ?tl ? ?. Such a\\nStudent-t prior is also applied in [4]. The model described above is termed a logistic stick-breaking\\nPT\\nPK\\nprocess (LSBP). For notational convenience, cml ? t=1 vmt (sml )??mt and ?mt ? k=1 wzm k ?k\\nconstructed as above is represented as a draw from LSBPT (wzm ). Figure 1 depicts the detailed\\ngenerative process of the proposed model with LSBP.\\n(\\n\\n)\\n\\nS\\n\\nc\\n\\ne\\n\\nn\\n\\ne\\n\\n1\\n\\nS\\n\\nc\\n\\ne\\n\\nn\\n\\ne\\n\\n2\\n\\nS\\n\\nc\\n\\ne\\n\\nn\\n\\ne\\n\\ni\\n\\ni\\n\\nS\\n\\nc\\n\\ne\\n\\nn\\n\\ne\\n\\nI\\n\\nI\\n\\ni\\n\\nL\\n\\nL\\nI\\n\\n?\\n\\n?\\n\\n?\\n\\nz\\n\\n~\\n\\nm\\n\\n=\\n\\ni\\n\\n(\\n\\nd\\n\\nu\\n\\ni\\n\\n~\\n\\n~\\n\\nS\\n\\nm\\n\\nl\\n\\nk\\n\\nB\\n\\ny\\n\\nm\\n\\nG\\n\\nl\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\ng\\n\\nG\\n\\ni\\n\\n1\\n\\n)\\n\\ni\\n\\ni\\n\\n(\\n\\n)\\n\\ni\\n\\nS\\n\\nk\\n\\nB\\n\\nT\\n\\nr\\n\\ne\\n\\ni\\n\\ni\\n\\ni\\n\\n(\\n\\nv\\n\\n)\\n\\ny\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\ng\\n\\ne\\n\\nG\\n\\nr\\n\\na\\n\\ns\\n\\ns\\n\\n?\\n\\nc\\n\\nG\\n\\n?\\n\\nr\\n\\na\\n\\ns\\n\\ns\\n\\nw\\n\\n~\\n\\nL\\n\\nS\\n\\nB\\n\\nP\\n\\n(\\n\\n)\\n~\\n\\n~\\n\\nT\\n\\nm\\n\\nl\\n\\nT\\n\\nz\\n\\nm\\n\\nl\\n\\nr\\n\\ne\\n\\ne\\n\\nm\\n\\nG\\n\\nl\\n\\nG\\n\\nm\\n\\nFigure 1: Depiction of the generative process. (i) A scene-class indicator zm ? {1, . . . , I} is drawn to define\\nthe image class; (ii) conditioned on zm , and using the LSBP, contiguous segmented blocks are constituted,\\nwith associated words defined by object indicator cml ? {1, ? ? ? , K}, where w i defines the probability of\\nobserving each object type for image class i; (iii) conditioned on cml , image-feature atoms are drawn from\\nappropriate mixture models Gcml , linked to over-segmented regions within each of the object clusters; (iv) the\\nimage-feature model parameters are responsible for generating the image features, via the model F (?), where\\n? is the image-feature parameter.\\n\\n3.2 Discussion of LSBP properties and comparison with KSBP\\n(m)\\n\\nThere are two key components of the LSBP construction: (i) sparseness promotion on the Wtl ,\\n(m)\\nand (ii) the use of a logistic link function to define spatial stick weights. A particular non-zero Wtl\\nis (via the kernel) associated with the lth local spatial region, with spatial extent defined by ?mt . If\\n(m)\\nWtl is sufficiently large, the ?clipping? property of the logistic link yields a spatially contiguous\\n(t)\\nand extended region over which the tth LSBP layer will dominate. Specifically, cml will likely be\\n(m)\\nthe same for data samples located near (defined by ?mt ) where a large Wtl resides, since in this\\nregion ?[gmt (s)] ? 1. All locations s for which (roughly) gmt (s) ? 4 will have ? via the ?clipping?\\nmanifested via the logistic ? nearly the same high probability of being associated with model layer\\nt. Sharp segment boundaries are also encouraged by the steep slope of the logistic function.\\nA related use of spatial information is constituted via the kernel stick-breaking process (KSBP) [2].\\nWith the KSBP, rather than assuming exchangeable data, the vmt (s) in (6) is defined as:\\nt?1\\nY\\n(8)\\nvmt (s) = Vmt K(s, ?mt ) [1 ? Vmt K(s, ?m? ; ?)] , Vmt ? Beta(1, ?0 )\\n?\\n\\nwhere K(s, ?mt ) represents a kernel distance between the feature-vector spatial coordinate s and a\\nlocal basis location ?mt associated with the tth stick. Although such a model also establishes spatial\\ndependence within local regions, the form of the prior has not been found explicit enough to impose\\nsmooth segments with sharp boundaries, as demonstrated in [2].\\n\\n4 Using the Proposed Model\\n4.1 Inference\\nBayesian inference seeks to estimate the posterior distribution of the latent variables ? , given the\\nobserved data D and hyper-parameters ?. We employ variational Bayesian (VB) [14] inference as a\\ncompromise between accuracy and efficiency. This method approximates an intractableQ\\njoint posterior p(?|D) of all the hidden variables by a product of marginal distributions q(?) = f qf (?f ),\\neach over only a single hidden variable ?f . The optimal parameterization of qf (?f ) for each\\nvariable is obtained by minimizing the Kullback-Leibler divergence between the variational approximation q(?) and the true joint posterior p(?).\\n4\\n\\n\\f4.2 Processing images with no words given\\nm\\nIf one is given M images, all non-annotated, then the model may be employed on the data {xml }L\\nl=1 ,\\nfor m = 1, . . . , M , from which a posterior distribution is inferred on the image model parameters\\n{??j }Jj=1 , and on {Gk }K\\nk=1 . Note that properties of the image classes and of the objects within\\nimages is inferred by processing all M images jointly. By placing all images within the context of\\neach other, the model is able to infer which building blocks (classes and objects) are responsible for\\nall of the data. In this sense the simultaneous processing of multiple images is critical: the learning\\nof properties of objects in one image is aided by the properties being learned for objects in all other\\nimages, through the inference of inter-relationships and commonalities.\\n\\nAfter the M images are analyzed in the absence of annotations, one may observe example portions\\nof the M images, to infer the link between actual object characteristics within imagery and the\\nassociated latent object indicator to which it was assigned. With this linkage made, one may assign\\nwords to all or a subset of the K object types. After words are assigned to previously latent object\\ntypes, the results of the analysis (with no additional processing) may be used to automatically label\\nregions (objects) in all of the images. This is manifested because each of the cluster indicators cml\\nis associated with a latent localized object type (to which a word may now be assigned).\\n4.3 Joint processing of images and annotations\\nWe may consider problems for which a subset of the images are provided with annotations (but not\\nthe explicit location and segmented-out objects); the words are assumed to reside in a prescribed\\ndictionary of object types. The generation of the annotations (and images) is constituted via the\\nmodel in (5), with the LSBP employed as discussed. We do not require that all images are annotated\\n(the non-annotated images help learn the properties of the image features, and are therefore useful\\neven if they do not provide information about the words). It is desirable that the same word be\\nannotated for multiple images. The presence of the same word within the annotations of multiple\\nimages encourages the model to infer what objects (represented in terms of image features) are\\ncommon to the associated images, aiding the learning. Hence, the presence of annotations serves as\\na learning aid (encourages looking for commonalities between particular images, if words are shared\\nin the associated annotations). Further, the annotations associated with images may disambiguate\\nobjects that appear similar in image-feature space (because they will have different annotations).\\nFrom the above discussion, the model performance will improve as more images are annotated\\nwith each word, but presumably this annotation is much easier for the human than requiring one to\\nsegment out and localize words within a scene.\\n\\n5 Experimental Results\\nExperiments are performed on two real-world data sets: subsets of Microsoft Research (MSRC)\\ndata ( http://research.microsoft.com/en-us/projects/objectclassrecognition/ ) and UIUC-Sport data from\\n[15, 16], the latter images originally obtained from the Flickr website and available online (\\nhttp://vision.cs.princeton.edu/lijiali/event dataset/ ).\\nFor the MSRC dataset, 10 categories of images with manual annotations are selected: ?tree?, ?building?, ?cow?, ?face?, ?car?, ?sheep?, ?flower?, ?sign?, ?book? and ?chair?. The number of images\\nin the ?cow? class is 45, and in the ?sheep? class there are 35; there are 30 images in all other\\nclasses. From each category, we randomly choose 10 images, and remove the annotations, treating\\nthese as non-annotated images within the analysis (to allow quantification of inferred-annotation\\nquality). Each image is of size 213 ? 320 or 320 ? 213. In addition, we remove all words that\\noccur less that 8 times (approximately 1% of all words). There are 14 unique words: ?void?, ?building?, ?grass?, ?tree?, ?cow?, ?sheep?, ?sky?, ?face?, ?car?, ?flower?, ?sign?, ?book?, ?chair? and\\n?road?. We assume that each word corresponds to a visual object in the image. Regarding the case\\nin which multiple words may refer to the same object, one may use the method mentioned in [16] to\\ngroup synonyms in the preprocessing phase (not necessary here). The following analysis, in which\\nannotated and non-annotated images are processed jointly, is executed as discussed in Section 4.3.\\nThe UIUC-Sport dataset [15, 16] contains 8 types of sports: ?badminton?, ?bocce?, ?croquet?,\\n?polo?, ?rock climbing?, ?rowing?, ?sailing? and ?snowboarding?. Here we randomly choose 25\\nimages for each category, and each image is resized to a dimension of 240 ? 320 or 320 ? 240.\\nSince the annotations are not available at the cited website, the analysis is initially performed with\\nno words, as discussed in Section 4.2. After performing this analysis, and upon examining the\\nproperties of segmented data associated with each (latent) object class on a small subset of the data,\\n5\\n\\n\\fwe can infer words associated with some important Gk , and then label portions (objects) within each\\nimage via the inferred words. This process is different than in [6, 16, 23], in which annotations were\\nemployed.\\nWhen investigating algorithm performance, we make comparisons to Corr-LDA [6]. Our objectives\\nare related to those in [16, 23], but to the authors? knowledge the associated software is not currently\\navailable. The Corr-LDA model [6] is relatively simple, and has been coded ourselves. We also\\nexamine our model with the proposed LSBP replaced with with KSBP.\\n5.1 Image preprocessing\\nEach image is first segmented into 800 ?superpixels?, which are local, coherent and\\npreserve most of the structure necessary for segmentation at the scale of interest [19].\\nThe software used for over-segmentation is discussed in [17] and is available online\\n(http://www.cs.sfu.ca/?mori/research/superpixels/ ). Each superpixel is represented by both color and\\ntexture descriptors, based on the local RGB, hue [25] feature vectors and also the output of maximum response (MR) filter banks [22] (http://www.robots.ox.ac.uk/?vgg/research/texclass/filters.html).\\nWe discretize these features using a codebook of size 64 (other codebook sizes gave similar performance), and then calculate the distribution [1] for each feature within each superpixel as visual\\nwords [3, 6, 10, 11, 20, 23, 24].\\nSince each superpixel is represented by three visual\\nN words, the\\nN mixture atoms\\n??j are three multinomial distributions {Mult(??1j ) Mult(??2j ) Mult(??3j )} for\\nj = 1, ? ? ? , J.\\nAccordingly,\\nthe Nvariational distribution in the VB [14] analysis is\\nN\\nq(? ?j ) = Dir(??1j |?\\n?1j ) Dir(??2j |?\\n?2j ) Dir(??3j |?\\n?3j ).\\nThe center of each superpixel is recorded as the location coordinate sml . The set of discrete kernel widths ?? are defined by 30, 35, ? ? ? , 160, and a uniform multinomial prior is placed on these\\nparameters (the size of each kernel is inferred, for each of the T LSBP layers, and separately in\\neach of the M images). To save computational resources, rather than centering a kernel at each of\\nthe Lm points associated with the superpixels, the kernel spatial centers are placed once every 20\\nsuperpixels.\\nWe set truncation levels I = 20, J = 50 and T = 10 (similar results were found for larger truncations). For analysis on UIUC-Sport dataset, K = 40. All gamma priors for precision parameters\\n(m)\\nm ,M\\n?6\\n?w , ?v or {?tl }T,L\\n, 10?6 ). All these hyper-parameters\\nt=1,l=0,m=1 , ?u and ?h are set as (10\\nand truncation levels have not been optimized or tuned. In the following comparisons, the number\\nof topics is set to be same as the atom number, J = 50, and the Dirichlet hyperparameters are\\nset as (1/J, . . . , 1/J)T for Corr-LDA model; a gamma prior is also used for the KSBP precision\\nparameter, ?0 in (8), also set as (10?6 , 10?6 ).\\n5.2 Scene clustering\\nThe proposed model automatically learns a posterior distribution on mixture-weights u and in so\\ndoing infers an estimate of the proper number of scene classes. As shown in Figure 2, although we\\ninitialized the truncation level to I = 20, for the MSRC dataset only the first 10 clusters are selected\\nas being important (the mixture weights for other clusters are very small); recall that ?truth? indicated that there were 10 classes. In addition, based on the learned posterior word distribution wi\\nfor each image class i, we can further infer which words/objects are probable for each scene class.\\nIn Figure 2, we show two example w i for the MSRC ?building? and ?cow? classes. Although not\\nshown here for brevity, the analysis on UIUC features correctly inferred the 8 image classes associated with that data (without using annotations). By examining the words and segmented objects\\nextracted with high probability as represented by wi , we may also assign names to each of the 18\\nimage classes across both the MSRC and UIUC data, consistent with the associated class labels\\nprovided with the data.\\nFor each image m ? {1, . . . , M } we also have a posterior distribution on the associated class\\nindicator zm . We approximate the membership for each image by assigning it to the mixture with\\nlargest probability. This ?hard? decision is employed to provide scene-level label for each image (the\\nBayesian analysis can also yield a ?soft? decision in terms of a full posterior distribution). Figure 3\\npresents the confusion matrices for the proposed model with and without LSBP, on both the MSRC\\nand UIUC datasets. Both forms of the model yield relatively good results, but the average accuracy\\nindicates that the model with LSBP performs better than that without LSBP for both datasets. Note\\n6\\n\\n\\fthat the results in Figure 3 for the UIUC-Sport data cannot be directly compared with those in [6, 16],\\nsince our experiments were performed on non-annotated images.\\nUsing the concepts discussed in Section 4.2, and employing results from the processed nonannotated UIUC-Sport data, we examined the properties of segmented data associated with each\\n(latent) object type. We inferred the presence of 12 unique objects, and these objects were assigned\\nthe following words: ?human?, ?horse?, ?grass?, ?sky?, ?tree?, ?ground?,?water?, ?rock?, ?court?,\\n?boat?, ?sailboat? and ?snow?. Using these words, we annotated each image and re-trained our\\nmodel in the presence of annotations. After doing so, the average accuracies of scene-level clustering are improved to 72.0% and 69.0% with and without LSBP, respectively. The improvement\\nin performance, relative to processing the images without annotations, is attributed to the ability of\\nwords to disambiguate distinct objects that have similar properties in image-feature space (e.g., the\\ndistinct use of ?boat? and ?sailboat?, which helps distinguish rowing and sailing).\\nbuilding\\n\\nMicrosoft Research Data\\n\\ncow\\n\\n0.4\\n\\n0.2\\n\\n0.1\\n\\n0.3\\nProbability\\n\\n0.15\\n\\nProbability\\n\\nMixture Weight\\n\\n0.4\\n\\n0.2\\n0.1\\n\\n0.05\\n0\\n0\\n\\n5\\n\\n10\\nCluster Index\\n\\n15\\n\\n0\\n\\n20\\n\\n0.3\\n0.2\\n0.1\\n\\nBuilding\\n\\nSky\\nGrass\\nTree\\nObject Index\\n\\n0\\n\\nVoid\\n\\nGrass\\n\\nCow\\nTree\\nVoid\\nObject Index\\n\\nBuilding\\n\\nFigure 2: Example inferred latent properties associated with MSRC dataset. Left: Posterior distribution on\\nthe mixture-weights u, quantifying the probability of scene classes (10 classes are inferred). Middle and Right:\\nExample probability of objects for a given class, w i (probability of object/words); here we only give the top 5\\nwords for each class.\\nwithout LSBP\\ntree .83\\nbuilding .10\\ncow .04\\nface .03\\ncar .03\\nsheep .03\\nflower .00\\nsign .03\\nbook .00\\nchair .10\\n\\n.13\\n.80\\n.02\\n.10\\n.10\\n.03\\n.07\\n.03\\n.00\\n.03\\n\\n.00\\n.00\\n.87\\n.00\\n.00\\n.09\\n.00\\n.00\\n.00\\n.00\\n\\n.03\\n.03\\n.00\\n.73\\n.00\\n.00\\n.03\\n.00\\n.03\\n.00\\n\\n.00\\n.00\\n.00\\n.00\\n.87\\n.00\\n.00\\n.00\\n.00\\n.00\\n\\n.00\\n.00\\n.07\\n.00\\n.00\\n.86\\n.00\\n.00\\n.00\\n.00\\n\\nwithout LSBP\\n\\nwith LSBP\\n\\n.00\\n.00\\n.00\\n.07\\n.00\\n.00\\n.83\\n.10\\n.00\\n.00\\n\\n.00\\n.00\\n.00\\n.07\\n.00\\n.00\\n.07\\n.80\\n.13\\n.00\\n\\n.00\\n.07\\n.00\\n.00\\n.00\\n.00\\n.00\\n.03\\n.83\\n.00\\n\\n.00\\ntree .87 .10 .00 .03 .00 .00 .00 .00\\n.00 building .13 .83 .00 .03 .00 .00 .00 .00\\n.00\\ncow .04 .00 .89 .00 .00 .07 .00 .00\\n.00\\nface .03 .07 .00 .80 .00 .00 .07 .03\\n.00\\ncar .00 .17 .00 .00 .83 .00 .00 .00\\n.00 sheep .00 .00 .11 .00 .00 .89 .00 .00\\n.00 flower .00 .00 .00 .10 .00 .00 .87 .03\\n.00\\nsign .00 .07 .00 .00 .00 .00 .03 .87\\n.00\\nbook .00 .00 .00 .03 .00 .00 .00 .10\\n.87\\nchair .07 .03 .00 .00 .00 .00 .00 .00\\n\\n.00\\n.00\\n.00\\n.00\\n.00\\n.00\\n.00\\n.03\\n.87\\n.00\\n\\nwith LSBP\\n\\n.00 badmi. .76 .00 .08 .04 .00 .04 .04\\n.00 bocce .08 .44 .24 .04 .04 .08 .00\\n.00\\ncroquet .04 .08 .72 .08 .04 .04 .00\\n.00\\npolo .04 .04 .12 .64 .04 .04 .04\\n.00\\n.00\\nrockc .00 .04 .04 .00 .76 .04 .04\\n.00 sailing .04 .04 .04 .04 .00 .44 .32\\n.00\\nrowing .04 .04 .04 .04 .04 .28 .44\\n.00\\n.90 snowb. .04 .08 .04 .04 .08 .04 .04\\n\\n.04 badmi. .76 .04 .04 .04 .00 .04 .04 .04\\n.08\\n\\nbocce .04 .48 .24 .04 .04 .04 .04 .08\\n\\n.00 croquet .04 .08 .72 .08 .04 .04 .00 .00\\n.04\\n\\npolo .04 .04 .12 .64 .04 .04 .04 .04\\n\\n.08\\n\\nrockc .00 .08 .04 .00 .76 .04 .00 .08\\n\\n.08\\n\\nsailing .04 .04 .00 .04 .00 .52 .28 .08\\n\\n.08 rowing .04 .04 .04 .04 .04 .24 .52 .04\\n.64 snowb. .04 .08 .00 .04 .08 .04 .04 .68\\n\\nFigure 3: Comparisons using confusion matrices for all images in each dataset (all of the annotated and nonannotated images in MSRC; all the non-annotated images in UIUC-Sport). The left two results are for MSRC,\\nand the right two for UIUC-Sport. In each pair, the result is without LSBP, and the right is with LSBP. Average\\nperformance, left to right: 82.90%, 86.80%, 60.50% and 63.50%.\\n\\n5.3 Image annotation\\nThe proposed model infers a posterior distribution for the indicator variables cml (defining the object/word for super-pixel l in image m). Similar to the ?hard? image-class assignment discussed\\nabove, a ?hard? segmentation is employed here to provide object labels for each super-pixel. For the\\nMSRC images for which annotations were held out, we evaluate whether the words associated with\\nobjects in a given image were given in the associated annotation (thus, our annotation is defined by\\nthe words we have assigned to objects in an image).\\nTable 1: Comparison of precision and recall values for annotation and segmentation with Corr-LDA [6], our\\nmodel without LSBP (Simp. Model) and the extended models with KSBP (Ext. with KSBP) and LSBP (Ext.\\nwith LSBP) on MSRC datasets. To evaluate annotation performance, the results are just calculated based on\\nnon-annotated images; while for segmentation, the results are based on all images.\\nAnnotation\\nCorr-LDA\\nF\\n\\nSegmentation\\n\\nSimp. Model\\n\\nExt. with LSBP\\n\\nPrec Rec\\n\\nPrec Rec\\n\\nF\\n\\nF\\n\\nCorr-LDA\\nPrec Rec\\n\\nF\\n\\nSimp. Model\\n\\nExt. with KSBP\\n\\nExt. with LSBP\\n\\nPrec Rec\\n\\nPrec Rec\\n\\nPrec Rec\\n\\nObject\\n\\nPrec Rec\\n\\nF\\n\\nF\\n\\ncar\\n\\n.18\\n\\n.60\\n\\n.28\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.13\\n\\n.08\\n\\n.10\\n\\n.49\\n\\n.38\\n\\n.43\\n\\n.56\\n\\n.50\\n\\n.53\\n\\n.61\\n\\n.58\\n\\n.60\\n\\nF\\n\\ntree\\n\\n.30\\n\\n.50\\n\\n.38\\n\\n.50\\n\\n.60\\n\\n.55\\n\\n.55\\n\\n.60\\n\\n.57\\n\\n.06\\n\\n.03\\n\\n.04\\n\\n.43\\n\\n.38\\n\\n.40\\n\\n.48\\n\\n.44\\n\\n.46\\n\\n.51\\n\\n.48\\n\\n.50\\n\\nsheep\\n\\n.17\\n\\n.60\\n\\n.27\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.02\\n\\n.02\\n\\n.02\\n\\n.53\\n\\n.63\\n\\n.58\\n\\n.57\\n\\n.63\\n\\n.60\\n\\n.60\\n\\n.62\\n\\n.61\\n\\nsky\\n\\n.38\\n\\n.65\\n\\n.48\\n\\n.66\\n\\n.60\\n\\n.63\\n\\n.68\\n\\n.60\\n\\n.64\\n\\n.39\\n\\n.29\\n\\n.33\\n\\n.40\\n\\n.51\\n\\n.45\\n\\n.49\\n\\n.54\\n\\n.51\\n\\n.55\\n\\n.55\\n\\n.55\\n\\nchair\\n\\n.14\\n\\n.60\\n\\n.22\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.70\\n\\n.13\\n\\n.16\\n\\n.15\\n\\n.57\\n\\n.55\\n\\n.56\\n\\n.58\\n\\n.55\\n\\n.57\\n\\n.59\\n\\n.55\\n\\n.57\\n\\nMean\\n\\n.23\\n\\n.63\\n\\n.32\\n\\n.65\\n\\n.63\\n\\n.64\\n\\n.67\\n\\n.65\\n\\n.65\\n\\n.17\\n\\n.18\\n\\n.16\\n\\n.49\\n\\n.51\\n\\n.50\\n\\n.53\\n\\n.53\\n\\n.53\\n\\n.56\\n\\n.54\\n\\n.54\\n\\nWe use precision-recall and F-measures [16, 23] to quantitatively evaluate the annotation performance. The left part of Table 1 lists detailed annotation results for five objects, as well as the overall\\nscores from all objects classes for the MSRC data. Our annotation results consistently and significantly outperform Corr-LDA, especially for the precision values.\\n7\\n\\n\\f5.4 Object segmentation\\nFigure 4 shows some detailed object-segmentation results of Corr-LDA and the proposed model\\n(with and without LSBP). We observe that our models generally yield visibly better segmentation\\nrelative to Corr-LDA. For example, for complicated objects the Corr-LDA segmentation results are\\nvery sensitive to the feature variance, and an object is generally segmented into many small, detailed\\nparts. By contrast, due to the imposed mixture structure on each object, our models cluster small\\nparts into one aggregate object. Furthermore, LSBP encourages local contiguous regions to be\\ngrouped in the same segment, and therefore it is less sensitive to localized variability. In addition,\\ncompared with results shown in [2], which also used the MSRC dataset, one may observe KSBP\\ncannot do as well as LSBP in maintaining spatial contiguity, as discussed in Section 3.2. Due to\\nspace limitations, detailed example comparison between LSBP and KSBP will be shown elsewhere\\nin a longer report; the quantitative comparison in Table 1 further demonstrate the advantages of\\nLSBP over KSBP.\\nt\\n\\nr\\n\\ne\\n\\ne\\n\\nb\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\ng\\n\\ns\\n\\ni\\n\\ng\\n\\nn\\n\\nc\\n\\nr\\n\\no\\n\\nq\\n\\nu\\n\\ne\\n\\nt\\n\\np\\n\\no\\n\\nl\\n\\no\\n\\nr\\n\\no\\n\\nc\\n\\nk\\n\\nS\\n\\nW\\n\\nR\\n\\no\\n\\na\\n\\nr\\n\\ne\\n\\na\\n\\nt\\n\\ne\\n\\na\\n\\ni\\n\\nl\\n\\nb\\n\\no\\n\\na\\n\\nt\\n\\nr\\n\\nd\\na\\n\\nT\\n\\nc\\n\\ne\\n\\nC\\n\\na\\n\\nn\\n\\nr\\n\\na\\n\\nW\\n\\na\\n\\nt\\n\\ne\\n\\nr\\n\\nB\\n\\nH\\n\\nt\\n\\nu\\n\\nm\\n\\nC\\n\\no\\n\\nu\\n\\nr\\n\\nt\\n\\no\\n\\nT\\n\\nS\\n\\nS\\n\\nk\\n\\nk\\n\\ny\\n\\ny\\n\\nH\\n\\nB\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\ng\\n\\nr\\n\\nC\\n\\no\\n\\ne\\n\\ne\\n\\nu\\n\\nm\\n\\na\\n\\nn\\n\\nB\\n\\nw\\n\\nH\\n\\nV\\n\\no\\n\\ni\\n\\nu\\n\\nm\\n\\na\\n\\nn\\n\\nR\\n\\nd\\n\\no\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\nc\\n\\nk\\n\\ng\\n\\nS\\n\\na\\n\\ni\\n\\nl\\n\\nb\\n\\na\\n\\nt\\n\\nR\\n\\no\\n\\nc\\n\\nk\\n\\no\\nG\\n\\nr\\n\\na\\n\\ns\\n\\ns\\n\\nB\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\ng\\n\\nr\\n\\ne\\n\\ne\\n\\nr\\n\\na\\n\\ns\\n\\nR\\n\\no\\n\\nc\\n\\nk\\n\\nT\\n\\nS\\n\\nS\\n\\nk\\n\\nk\\n\\ny\\n\\nB\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\ng\\n\\ny\\n\\nT\\n\\nr\\n\\ne\\n\\ne\\n\\nH\\n\\nT\\n\\nr\\n\\ne\\n\\nu\\n\\nm\\n\\na\\n\\nn\\n\\ne\\n\\nS\\n\\ni\\n\\ng\\n\\nn\\n\\nr\\nH\\n\\ns\\n\\ne\\n\\no\\n\\nH\\n\\nT\\n\\nr\\n\\ne\\n\\ne\\n\\nB\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\nr\\n\\na\\n\\ns\\n\\nm\\n\\na\\n\\nn\\n\\ng\\n\\nG\\n\\nr\\n\\nG\\n\\nu\\n\\nG\\n\\na\\n\\ns\\n\\nr\\n\\na\\n\\ns\\n\\ns\\n\\nG\\n\\ns\\n\\ns\\n\\ns\\n\\nT\\n\\nS\\n\\nk\\n\\nr\\n\\ne\\n\\ne\\n\\ny\\n\\nR\\n\\nS\\n\\nk\\n\\no\\n\\nc\\n\\nk\\n\\ny\\n\\nT\\n\\nT\\n\\nr\\n\\ne\\n\\nr\\n\\ne\\n\\ne\\n\\nH\\n\\nu\\n\\nm\\n\\na\\n\\nn\\n\\ne\\n\\nS\\nB\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\ni\\n\\ng\\n\\nn\\n\\ng\\n\\nH\\n\\no\\n\\nr\\n\\ns\\n\\ne\\n\\nH\\n\\nT\\n\\nr\\n\\ne\\n\\nB\\n\\nu\\n\\ni\\n\\nl\\n\\nd\\n\\ni\\n\\nn\\n\\nr\\n\\na\\n\\ns\\n\\na\\n\\ns\\n\\na\\n\\nn\\n\\ns\\n\\nG\\n\\nr\\n\\nm\\n\\ng\\n\\nG\\n\\nG\\nG\\n\\nu\\n\\ne\\n\\nr\\n\\na\\n\\ns\\n\\nr\\n\\na\\n\\ns\\n\\ns\\n\\ns\\n\\ns\\n\\nFigure 4: Example segmentation and labeling results. First row: original images; second row: Corr-LDA [6];\\nthird row: proposed model without LSBP; fourth row: proposed model with LSBP. Columns 1-3 from MSRC\\ndataset; Columns 4-6 from UIUC-Sport dataset. The name of original images are inferred by scene-level\\nclassification via our model. The UIUC-Sport results are based on the words inferred by our model.\\n\\nThe MSRC database provides manually defined segmentations, to which we quantitatively compare.\\nThe right part of Table 1 compares results of the proposed model with Corr-LDA. As indicated in\\nTable 1, the proposed model (with and without LSBP) significantly outperforms Corr-LDA for all\\nobjects. Moreover, due to imposed spatial contiguity, the models with KSBP and LSBP are better\\nthan without.\\nThe experiments have been performed in non-optimized software written in Matlab, on a Pentium\\nPC with 1.73 GHz CPU and 4G RAM. One VB run of our model with LSBP, for 70 VB iterations,\\nrequired nearly 7 hours for 320 images from MSRC dataset. Typically 50 VB iterations are required\\nto achieve convergence. The UIUC-Sport data required comparable CPU time. It typically took less\\nthan half the CPU time for our model without LSBP on a same dataset. All results are based on a\\nsingle VB run, with random initialization.\\n\\n6 Conclusions\\nA nonparametric Bayesian model has been developed for clustering M images into classes; the images are represented as a aggregation of distinct localized objects, to which words may be assigned.\\nTo infer the relationships between image objects and words (labels), we only need to make the association between inferred model parameters and words. This may be done as a post-processing step if\\nno words are provided, and it may done in situ if all or a subset of the M images are annotated. Spatially contiguous objects are realized via a new logistic stick-breaking process. Quantitative model\\nperformance is highly competitive relative to competing approaches, with relatively fast inference\\nrealized via variational Bayesian analysis. The authors acknowledge partial support from ARO,\\nAFOSR, DOE, NGA and ONR.\\n8\\n\\n\\fReferences\\n[1] T. Ahonen and M. Pietik?ainen. Image description using joint distribution of filter bank responses. Pattern\\nRecogntion Letters, 30:368?376, 2009.\\n[2] Q. An, C. Wang, I. Shterev, E. Wang, L. Carin, and D. B. Dunson. Hierarchical kernel stick-breaking\\nprocess for multi-task image analysis. In ICML, 2008.\\n[3] K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth, D. M. Blei, and M. I. Jordan. Matching words and\\npictures. JMLR, 3:1107?1135, 2003.\\n[4] C. M. Bishop and M. E. Tipping. Variational relevance vector machines. In UAI, 2000.\\n[5] D. Blackwell and J. B. MacQueen. Ferguson distributions via Polya urn schemes. Ann. Statist., 1(2):353?\\n355, 1973.\\n[6] D. M. Blei and M. Jordan. Modeling annotated data. In SIGIR, 2003.\\n[7] D. M. Blei and J. D. McAuliffe. Supervised topic model. In NIPS, 2007.\\n[8] D. M. Blei, A. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3:993?1022, 2003.\\n[9] A. Bosch, A. Zisserman, and X. Munoz. Scene classification via plsa. In ECCV, 2006.\\n[10] L. Cao and L. Fei-Fei. Spatially coherent latent topic model for concurrent segmentation and classification\\nof objects and scenes. In ICCV, 2007.\\n[11] L. Fei-Fei and P. Perona. A Bayesian hieratchical model for learning natural scence categories. In CVPR,\\n2005.\\n[12] T. Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Mach. Learn., 42(1-2):177?\\n196, 2001.\\n[13] H. Ishwaran and L. F. James. Gibbs sampling methods for stick-breaking priors. JASA, 96(453):161?173,\\n2001.\\n[14] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. Saul. An introduction to variational methods for\\ngraphical models. Mach. Learn., 37(2):183?233, 1999.\\n[15] J. Li and L. Fei-Fei. What, where and who? classfying events by scene and object recognition. In ICCV,\\n2007.\\n[16] J. Li, R. Socher, and L. Fei-Fei. Towards total scene understaning: classification, annotation and segmentation in an automatic framework. In CVPR, 2009.\\n[17] G. Mori. Guiding model search using segmentation. In ICCV, 2005.\\n[18] A. Rabinovich, A. Vedaldi, C. Galleguillos, and E. Wiewiora. Objects in context. In ICCV, 2007.\\n[19] X. Ren and J. Malik. Learning a classification model foe segmentation. In ICCV, 2003.\\n[20] E. B. Sudderth and M. I. Jordan. Shared segementation of natural scenes using dependent pitman-yor\\nprocesses. In NIPS, 2008.\\n[21] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical Dirichlet processes. JASA, 101:1566?1582, 2005.\\n[22] M. Varma and A. Zisserman. Classifying images of materials: Achieving viewpoint and illumination\\nindependence. In ECCV, 2002.\\n[23] C. Wang, D. M. Blei, and L. Fei-Fei. Simultaneous image classification and annotation. In CVPR, 2009.\\n[24] X. Wang and E. Grimson. Spatial latent dirichlet allocation. In NIPS, 2007.\\n[25] J. V. D. Weijer and C. Schmid. Coloring local feature extraction. In ECCV, 2006.\\n[26] O. Yakhnenko and V. Honavar. Multi-modal hierarchical Dirichlet process model for predicting image\\nannotation and image-object label correspondence. In SIAM SDM, 2009.\\n[27] Z.-H. Zhou and M.-L. Zhang. Mutlti-instance multi-label learning with application to scene classification.\\nIn NIPS, 2006.\\n\\n9\\n\\n\\f\",\n          \"Extended and Unscented Gaussian Processes\\nDaniel M. Steinberg\\nNICTA\\ndaniel.steinberg@nicta.com.au\\n\\nEdwin V. Bonilla\\nThe University of New South Wales\\ne.bonilla@unsw.edu.au\\n\\nAbstract\\nWe present two new methods for inference in Gaussian process (GP) models\\nwith general nonlinear likelihoods. Inference is based on a variational framework where a Gaussian posterior is assumed and the likelihood is linearized about\\nthe variational posterior mean using either a Taylor series expansion or statistical\\nlinearization. We show that the parameter updates obtained by these algorithms\\nare equivalent to the state update equations in the iterative extended and unscented\\nKalman filters respectively, hence we refer to our algorithms as extended and unscented GPs. The unscented GP treats the likelihood as a ?black-box? by not\\nrequiring its derivative for inference, so it also applies to non-differentiable likelihood models. We evaluate the performance of our algorithms on a number of\\nsynthetic inversion problems and a binary classification dataset.\\n\\n1\\n\\nIntroduction\\n\\nNonlinear inversion problems, where we wish to infer the latent inputs to a system given observations of its output and the system?s forward-model, have a long history in the natural sciences,\\ndynamical modeling and estimation. An example is the robot-arm inverse kinematics problem. We\\nwish to infer how to drive the robot?s joints (i.e. joint torques) in order to place the end-effector in a\\nparticular position, given we can measure its position and know the forward kinematics of the arm.\\nMost of the existing algorithms either estimate the system inputs at a particular point in time like the\\nLevenberg-Marquardt algorithm [1], or in a recursive manner such as the extended and unscented\\nKalman filters (EKF, UKF) [2].\\nIn many inversion problems we have a continuous process; a smooth trajectory of a robot arm for\\nexample. Non-parametric regression techniques like Gaussian processes [3] seem applicable, and\\nhave been used in linear inversion problems [4]. Similarly, Gaussian processes have been used to\\nlearn inverse kinematics and predict the motion of a dynamical system such as robot arms [3, 5]\\nand a human?s gait [6, 7, 8]. However, in [3, 5] the inputs (torques) to the system are observable\\n(not latent) and are used to train the GPs. Whereas [7, 8] are not concerned with inference over\\nthe original latent inputs, but rather they want to find a low dimensional representation of high\\ndimensional outputs for prediction using Gaussian process latent variable models [6]. In this paper\\nwe introduce inference algorithms for GPs that can infer and predict the original latent inputs to a\\nsystem, without having to be explicitly trained on them.\\nIf we do not need to infer the latent inputs to a system it is desirable to still incorporate domain/system specific information into an algorithm in terms of a likelihood model specific to the\\ntask at hand. For example, non-parametric classification or robust regression problems. In these\\nsituations it is useful to have an inference procedure that does not require re-derivation for each\\nnew likelihood model without having to resort to MCMC. An example of this is the variational\\nalgorithm presented in [9] for factorizing likelihood models. In this model, the expectations arising from the use of arbitrary (non-conjugate) likelihoods are only one-dimensional, and so they\\ncan be easily evaluated using sampling techniques or quadrature. We present two alternatives to\\nthis algorithm that are also underpinned by variational principles but are based on linearizing the\\n1\\n\\n\\fnonlinear likelihood models about the posterior mean. These methods are straight-forwardly applicable to non-factorizing likelihoods and would retain computational efficiency, unlike [9] which\\nwould require evaluation of multidimensional intractable integrals. One of our algorithms, based on\\nstatistical linearization, does not even require derivatives of the likelihood model (like [9]) and so\\nnon-differentiable likelihoods can be incorporated.\\nInitially we formulate our models in ?2 for the finite Gaussian case because the linearization methods\\nare more general and comparable with existing algorithms. In fact we show we can derive the update\\nsteps of the iterative EKF [10] and similar updates to the iterative UKF [11] using our variational\\ninference procedures. Then in ? 3 we specifically derive a factorizing likelihood Gaussian process\\nmodel using our framework, which we use for experiments in ?4.\\n\\n2\\n\\nVariational Inference in Nonlinear Gaussian Models with Linearization\\n\\nGiven some observable quantity y ? Rd , and a likelihood model for the system of interest, in many\\nsituations it is desirable to reason about the latent input to the system, f ? RD , that generated the\\nobservations. Finding these inputs is an inversion problem and in a probabilistic setting it can be\\ncast as an application of Bayes? rule. The following forms are assumed for the prior and likelihood:\\np(f ) = N (f |?, K)\\n\\np(y|f ) = N (y|g(f ) , ?) ,\\n\\nand\\n\\n(1)\\n\\nwhere g(?) : RD ? Rd is a nonlinear function or forward model. Unfortunately the marginal likelihood, p(y), is intractable as the nonlinear function makes the likelihood and prior non-conjugate.\\nThis also makes the posterior p(f |y), which is the solution to the inverse problem, intractable to\\nevaluate. So, we choose to approximate the posterior with variational inference [12].\\n2.1\\n\\nVariational Approximation\\n\\nUsing variational inference procedures we can put a lower bound on the log-marginal likelihood\\nusing Jensen?s inequality,\\nZ\\np(y|f ) p(f )\\nlog p(y) ? q(f ) log\\ndf ,\\n(2)\\nq(f )\\nwith equality iff KL[q(f ) k p(f |y)] = 0, and where q(f ) is an approximation to the true posterior,\\np(f |y). This lower bound is often referred to as ?free energy?, and can be re-written as follows\\nF = hlog p(y|f )iqf ? KL[q(f ) k p(f )] ,\\n\\n(3)\\n\\nwhere h?iqf is an expectation with respect to the variational posterior, q(f ). We assume the posterior\\ntakes a Gaussian form, q(f ) = N (f |m, C), so we can evaluate the expectation and KL term in (3),\\n\\u0014\\nD\\nE \\u0015\\n1\\n>\\nhlog p(y|f )iqf = ? D log 2? + log |?| + (y ? g(f )) ?-1 (y ? g(f ))\\n,\\n(4)\\n2\\nqf\\n\\u0014\\n\\u0015\\n\\u0001\\n1\\n>\\ntr K-1 C + (? ? m) K-1 (? ? m) ? log |C| + log |K| ? D . (5)\\nKL[q(f ) k p(f )] =\\n2\\nwhere the expectation involving g(?) may be intractable. One method of dealing with these expectations is presented in [9] by assuming that the likelihood factorizes across observations. Here we\\nprovide two alternatives based on linearizing g(?) about the posterior mean, m.\\n2.2\\n\\nParameter Updates\\n\\nTo find the optimal posterior mean, m, we need to find the derivative,\\nE\\n?F\\n1 ? D\\n>\\n>\\n=?\\n(? ? f ) K-1 (? ? f ) + (y ? g(f )) ?-1 (y ? g(f )) ,\\n(6)\\n?m\\n2 ?m\\nqf\\nwhere all terms in F independent of m have been dropped, and we have placed the quadratic and\\ntrace terms from the KL component in Equation (5) back into the expectation. We can represent this\\nas an augmented Gaussian,\\nE\\n?F\\n1 ? D\\n>\\n=?\\n(z ? h(f )) S-1 (z ? h(f )) ,\\n(7)\\n?m\\n2 ?m\\nqf\\n2\\n\\n\\fwhere\\nz=\\n\\n\\u0014 \\u0015\\ny\\n,\\n?\\n\\nh(f ) =\\n\\n\\u0014\\n\\u0015\\ng(f )\\n,\\nf\\n\\nS=\\n\\n\\u0014\\n?\\n0\\n\\n\\u0015\\n0\\n.\\nK\\n\\n(8)\\n\\nNow we can see solving for m is essentially a nonlinear least squares problem, but about the\\nexpected posterior value of f . Even without the expectation, there is no closed form solution to\\n?F/?m = 0. However, we can use an iterative Newton method to find m. It begins with an initial\\nguess, m0 , then proceeds with the iterations,\\n-1\\n\\nmk+1 = mk ? ? (?m ?m F) ?m F,\\n\\n(9)\\n\\nfor some step length, ? ? (0, 1]. Though evaluating ?m F is still intractable because of the nonlinear\\nterm within the expectation in Equation (7). If we linearize g(f ), we can evaluate the expectation,\\ng(f ) ? Af + b,\\nfor some linearization matrix A ? R\\n>\\n\\n-1\\n\\nd?D\\n\\n(10)\\nd\\n\\nand an intercept term b ? R . Using this we get,\\n\\n-1\\n\\n?m F ? A ? (y ? Am ? b) + K (? ? m)\\n\\nand\\n\\n?m ?m F ? ?K-1 ? A> ?-1 A. (11)\\n\\nSubstituting (11) into (9) and using the Woodbury identity we can derive the iterations,\\nmk+1 = (1 ? ?) mk + ?? + ?Hk (y ? bk ? Ak ?) ,\\n\\n(12)\\n\\nwhere Hk is usually referred to as a ?Kalman gain? term,\\nHk = KA>k ? + Ak KA>k\\n\\n\\u0001-1\\n\\n,\\n\\n(13)\\n\\nand we have assumed that the linearization Ak and intercept, bk are in some way dependent on the\\niteration. We can find the posterior covariance by setting ?F/?C = 0 where,\\nE\\n1 ? D\\n1 ?\\n?F\\n>\\n=?\\n(z ? h(f )) S-1 (z ? h(f ))\\n+\\nlog |C| .\\n(14)\\n?C\\n2 ?C\\n2 ?C\\nqf\\nAgain we do not have an analytic solution, so we once more apply the approximation (10) to get,\\n\\u0002\\n\\u0003-1\\nC = K-1 + A> ?-1 A = (ID ? HA)K,\\n(15)\\nwhere we have once more made use of the Woodbury identity and also the converged values of A\\nand H. At this point it is also worth noting the relationship between Equations (15) and (11).\\n2.3\\n\\nTaylor Series Linearization\\n\\nNow we need to find expressions for the linearization terms A and b. One method is to use a first\\norder Taylor Series expansion to linearize g(?) about the last calculation of the posterior mean, mk ,\\ng(f ) ? g(mk ) + Jmk (f ? mk ) ,\\n\\n(16)\\n\\nwhere Jmk is the Jacobian ?g(mk )/?mk . By linearizing the function in this way we end up with a\\nGauss-Newton optimization procedure for finding m. Equating coefficients with (10),\\nA = Jmk ,\\n\\nb = g(mk ) ? Jmk mk ,\\n\\n(17)\\n\\nand then substituting these values into Equations (12) ? (15) we get,\\nmk+1 = (1 ? ?) mk + ?? + ?Hk (y ? g(mk ) + Jmk (mk ? ?)) ,\\n\\u0001-1\\nHk = KJ>mk ? + Jmk KJ>mk ,\\nC = (ID ? HJm )K.\\n\\n(18)\\n(19)\\n(20)\\n\\nHere Jm and H without the k subscript are constructed about the converged posterior, m.\\nRemark 1 A single step of the iterated extended Kalman filter [10, 11] corresponds to an update\\nin our variational framework when using the Taylor series linearization of the non-linear forward\\nmodel g(?) around the posterior mean.\\nHaving derived the updates in our variational framework, the proof of this is trivial by making ? = 1,\\nand using Equations (18) ? (20) as the iterative updates.\\n3\\n\\n\\f2.4\\n\\nStatistical Linearization\\n\\nAnother method for linearizing g(?) is statistical linearization (see e.g. [13]), which finds a least\\nsquares best fit to g(?) about a point. The advantage of this method is that it does not require derivatives ?g(f )/?f . To obtain the fit, multiple observations of the forward model output for different\\ninput points are required. Hence, the key question is where to evaluate our forward model so as to\\nobtain representative samples to carry out the linearization. One method of obtaining these points is\\nthe unscented transform [2], which defines 2D + 1 ?sigma? points,\\nM0 = m,\\n(21)\\n\\u0010p\\n\\u0011\\nMi = m +\\n(D + ?) C\\nfor i = 1 . . . D,\\n(22)\\n\\u0011i\\n\\u0010p\\n(D + ?) C\\nfor i = D + 1 . . . 2D,\\n(23)\\nMi = m ?\\ni\\n\\nYi = g(Mi ) ,\\n(24)\\n?\\nfor a free parameter ?. Here ( ?)i refers to columns of the matrix square root, we follow [2] and use\\nthe Cholesky decomposition. Unlike the usual unscented transform, which uses the prior to create\\nthe sigma points, here we have used the posterior because of the expectation in Equation (7). Using\\nthese points we can define the following statistics,\\n?=\\ny\\n\\n2D\\nX\\n\\nwi Yi ,\\n\\n?ym =\\n\\n2D\\nX\\n\\n>\\n\\n? ) (Mi ? m) ,\\nwi (Yi ? y\\n\\n(25)\\n\\ni=0\\n\\ni=0\\n\\n?\\n1\\n,\\nwi =\\nfor i = 1 . . . 2D.\\n(26)\\nD+?\\n2 (D + ?)\\nAccording to [2] various settings of ? can capture information about the higher order moments of\\nthe distribution of y; or setting ? = 0.5 yields uniform weights. To find the linearization coefficients\\nstatistical linearization solves the following objective,\\nw0 =\\n\\nargmin\\nA,b\\n\\n2D\\nX\\n\\n2\\n\\nkYi ? (AMi + b)k2 .\\n\\n(27)\\n\\ni=0\\n\\nThis is simply linear least-squares and has the solution [13]:\\n? ? Am.\\nA = ?ym C-1 ,\\nb=y\\nSubstituting b back into Equation (12), we obtain,\\n? k + Ak (mk ? ?)) .\\nmk+1 = (1 ? ?) mk + ?? + ?Hk (y ? y\\n\\n(28)\\n(29)\\n\\n? k have been evaluated using the statistics from the kth iteration. This implies\\nHere Hk , Ak and y\\nthat the posterior covariance, Ck , is now estimated at every iteration of (29) since we use it to form\\nAk and bk . Hk and Ck have the same form as Equations (13) and (15) respectively.\\nRemark 2 A single step of the iterated unscented sigma-point Kalman filter (iSPKF, [11]) can be\\nseen as an ad hoc approximation to an update in our statistically linearized variational framework.\\nEquations (29) and (15) are equivalent to the equations for a single update of the iterated sigma-point\\n? k appearing in Equation (29) as opposed to\\nKalman filter (iSPKF) for ? = 1, except for the term y\\ng(mk ). The main difference is that we have derived our updates from variational principles. These\\nupdates are also more similar to the regular recursive unscented Kalman filter [2], and statistically\\nlinearized recursive least squares [13].\\n2.5\\n\\nOptimizing the Posterior\\n\\nBecause of the expectations involving an arbitrary function in Equation (4), no analytical solution\\nexists for the lower bound on the marginal likelihood, F. We can use our approximation (10) again,\\n\\u0014\\n1\\n>\\nF ? ? D log 2? + log |?| ? log |C| + log |K| + (? ? m) K-1 (? ? m)\\n2\\n\\u0015\\n> -1\\n+ (y ? Am ? b) ? (y ? Am ? b) . (30)\\n\\n4\\n\\n\\fHere the trace \\u0001term from Equation\\n\\u0001 (5) has cancelled with a trace term from the expected likelihood,\\ntr A> ?-1 AC = D ? tr K-1 C , once we have linearized g(?) and substituted (15). Unfortunately\\nthis approximation is no longer a lower bound on the log marginal likelihood in general. In practice\\nwe only calculate this approximation F if we need to optimize some model hyperparameters, like\\nfor a Gaussian process as described in ? 3. When optimizing m, the only terms of F dependent on\\nm in the Taylor series linearization case are,\\n1\\n1\\n>\\n>\\n? (y ? g(m)) ?-1 (y ? g(m)) ? (? ? m) K-1 (? ? m) .\\n(31)\\n2\\n2\\nThis is also the maximum a-posteriori objective. A global convergence proof exists for this objective when optimized by a Gauss-Newton procedure, like our Taylor series linearization algorithm,\\nunder some conditions on the Jacobians, see [14, p255]. No such guarantees exist for statistical\\nlinearization, though monitoring (31) works well in practice (see the experiment in ?4.1).\\nA line search could be used to select an optimal value for the step length, ? in Equation (12).\\nHowever, we find that setting ? = 1, and then successively multiplying ? by some number in (0, 1)\\nuntil the MAP objective (31) decreases, or some maximum number of iterations is exceeded is fast\\nand works well in practice. If the maximum number of iterations is exceeded we call this a ?diverge?\\ncondition, and terminate the search for m (and return the last good value). This only tends to happen\\nfor statistical linearization, but does not tend to impact the algorithms performance since we always\\nmake sure to improve (approximate) F.\\n\\n3\\n\\nVariational Inference in Gaussian Process Models with Linearization\\n\\nWe now present two inference methods for Gaussian Process (GP) models [3] with arbitrary nonlinear likelihoods using the framework presented previously. Both Gaussian process models have the\\nfollowing likelihood and prior,\\n\\u0001\\ny ? N g(f ) , ? 2 IN ,\\nf ? N (0, K) .\\n(32)\\nHere y ? RN are the N noisy observed values of the transformed latent function, g(f ), and f ? RN\\nis the latent function we are interested in inferring. K ? RN ?N is the kernel matrix, where each\\nelement kij = k(xi , xj ) is the result of applying a kernel function to each input, x ? RP , in a pairwise manner. It is also important to note that the likelihood noise model is isotropic with a variance\\nof ? 2 . This is not a necessary condition, and we can use a correlated noise likelihood model, however\\nthe factorized likelihood case is still useful and provides some computational benefits.\\nAs before, we make the approximation that the posterior is Gaussian, q(f |m, C) = N (f |m, C)\\nwhere m ? RN is the mean posterior latent function, and C ? RN ?N is the posterior covariance. Since the likelihood is isotropic and factorizes over the N observations we have the following\\nexpectation under our variational inference framework:\\nN\\nE\\nN\\n1 XD\\n2\\n2\\nhlog p(y|f )iqf = ? log 2?? ? 2\\n(yn ? g(fn ))\\n.\\n2\\n2? n=1\\nqfn\\n\\nAs a consequence, the linearization is one-dimensional, that is g(fn ) ? an fn + bn . Using this we\\ncan derive the approximate gradients,\\n1\\nA (y ? Am ? b) ? K-1 m,\\n?m ?m F ? ?K-1 ? A?-1 A,\\n(33)\\n?2\\n\\u0002 2\\n\\u0003\\u0001\\nwhere A = diag([a1 , . . . , aN ]) and ? = diag ? , . . . , ? 2 . Because of the factorizing likelihood\\nwe obtain C-1 = K-1 + A?-1 A, that is, the inverse posterior covariance is just the prior inverse\\ncovariance, but with a modified diagonal. This means if we were to use this inverse parameterization\\nof the Gaussian, which is also used in [9], we would only have to infer 2N parameters (instead of\\nN + N (N + 1)/2). We can obtain the iterative steps for m straightforwardly:\\n?m F ?\\n\\nmk+1 = (1 ? ?) mk + ?Hk (y ? bk ) ,\\n\\n-1\\n\\nwhere Hk = KAk (? + Ak KAk ) ,\\n\\n(34)\\n\\nand also an expression for posterior covariance,\\nC = (IN ? HA)K.\\n5\\n\\n(35)\\n\\n\\fThe values for an and bn for the linearization methods are,\\n?g(mn )\\n,\\n?mn\\n?my,n\\n=\\n,\\nCnn\\n\\nbn = g(mn ) ?\\n\\nTaylor : an =\\nStatistical : an\\n\\n?g(mn )\\nmn ,\\n?mn\\n\\nbn = y?n ? an mn .\\n\\n(36)\\n(37)\\n\\nCnn is the nth diagonal element of C, and ?my,n and y?n are scalar versions\\np of Equations (21) ?\\n\\b\\n(26). The sigma points for each observation, n, are Mn = mn , mn + (1 + ?) Cnn , mn ?\\np\\n\\t\\n(1 + ?) Cnn . We refer to the Taylor series linearized GP as the extended GP (EGP), and the\\nstatistically linearized GP as the unscented GP (UGP).\\n3.1\\n\\nPrediction\\n\\nThe\\ndistribution of a latent value, f ? , given a query point, x? , requires the marginalization\\nR predictive\\n?\\np(f |f ) q(f |m, C) df , where p(f ? |f ) is a regular predictive GP. This gives f ? ? N (m? , C ? ), and,\\n\\u0002\\n\\u0003\\nm? = k?>K-1 m,\\nC ? = k ?? ? k?>K-1 IN ? CK-1 k? ,\\n(38)\\n>\\n\\nwhere k ?? = k(x? , x? ) and k? = [k(x1 , x? ) , . . . , k(xN , x? )] . We can also find the predicted\\nobservations, y?? by evaluating the one-dimensional integral,\\nZ\\n(39)\\ny?? = hy ? iqf ? = g(f ? ) N (f ? |m? , C ? ) df ? ,\\nfor which we use quadrature. Alternatively, if we were to use the UGP we can use another \\u0001application of the unscented transform to approximate the predictive distribution y ? ? N y?? , ?y2? where,\\ny?? =\\n\\n2\\nX\\n\\nwi M?i ,\\n\\n?y2? =\\n\\ni=0\\n\\n2\\nX\\n\\n2\\n\\nwi (Yi? ? y?? ) .\\n\\n(40)\\n\\ni=0\\n\\nThis works well in practice, see Figure 1 for a demonstration.\\n3.2\\n\\nLearning the Linearized GPs\\n\\nLearning the extended and unscented GPs consists of an inner and outer loop. Much like the Laplace\\napproximation for binary Gaussian Process classifiers [3], the inner loop is for learning the posterior\\nmean, m, and the outer loop is to optimize the likelihood parameters (e.g. the variance ? 2 ) and kernel hyperparameters, k(?, ?|?). The dominant computational cost in learning the parameters is the\\ninversion in Equation (34), and so the computational complexity of the EGP and UGP is about the\\nsame as for the Laplace GP approximation. To learn the kernel hyperparameters and ? 2 we use numerical techniques to find the gradients, ?F/??, for both the algorithms, where F is approximated,\\n\\u0014\\n\\u0015\\n1\\n1\\n>\\nN log 2?? 2 ? log |C| + log |K| + m> K-1 m + 2 (y ? Am ? b) (y ? Am ? b) .\\n2\\n?\\n(41)\\nSpecifically we use derivative-free optimization methods (e.g. BOBYQA) from the NLopt library [15], which we find fast and effective. This also has the advantage of not requiring knowledge\\nof ?g(f )/?f or higher order derivatives for any implicit gradient dependencies between f and ?.\\nF ??\\n\\n4\\n4.1\\n\\nExperiments\\nToy Inversion Problems\\n\\nIn this experiment we generate ?latent? function data from f ? N (0, K) where a Mat?rn 25 kernel\\nfunction is used with amplitude ?m52 = 0.8, length scale lm52 = 0.6 and x ? R are uniformly\\nspaced between [?2?, 2?] to build K. Observations\\nused to test and train the GPs are then generated\\n\\u0001\\nas y = g(f ) + \\u000f where \\u000f ? N 0, 0.22 . 1000 points are generated in this way, and we use 5-fold\\ncross validation to train (200 points) and test (800 points) the GPs. We use standardized mean\\n6\\n\\n\\fTable 1: The negative log predictive density (NLPD) and the standardized mean squared error\\n(SMSE) on test data for various differentiable forward models. Lower values are better for both\\nmeasures. The predicted f ? and y ? are the same for g(f ) = f , so we do not report y ? in this case.\\ng(f )\\n\\nAlgorithm\\n\\nNLPD f ?\\nmean\\nstd.\\n\\nSMSE f ?\\nmean\\nstd.\\n\\nSMSE y ?\\nmean\\nstd.\\n\\nf\\n\\nUGP\\nEGP\\n[9]\\nGP\\n\\n-0.90046\\n-0.89908\\n-0.27590\\n-0.90278\\n\\n0.06743\\n0.06608\\n0.06884\\n0.06988\\n\\n0.01219\\n0.01224\\n0.01249\\n0.01211\\n\\n0.00171\\n0.00178\\n0.00159\\n0.00160\\n\\n?\\n?\\n?\\n?\\n\\n?\\n?\\n?\\n?\\n\\nf3 + f2 + f\\n\\nUGP\\nEGP\\n[9]\\n\\n-0.23622\\n-0.22325\\n-0.14559\\n\\n1.72609\\n1.76231\\n0.04026\\n\\n0.01534\\n0.01518\\n0.06733\\n\\n0.00202\\n0.00203\\n0.01421\\n\\n0.02184\\n0.02184\\n0.02686\\n\\n0.00525\\n0.00528\\n0.00266\\n\\nexp(f )\\n\\nUGP\\nEGP\\n[9]\\n\\n-0.75475\\n-0.75706\\n-0.08176\\n\\n0.32376\\n0.32051\\n0.10986\\n\\n0.13860\\n0.13971\\n0.17614\\n\\n0.04833\\n0.04842\\n0.04845\\n\\n0.03865\\n0.03872\\n0.05956\\n\\n0.00403\\n0.00411\\n0.01070\\n\\nsin(f )\\n\\nUGP\\nEGP\\n[9]\\n\\n-0.59710\\n-0.59705\\n-0.04363\\n\\n0.22861\\n0.21611\\n0.03883\\n\\n0.03305\\n0.03480\\n0.05913\\n\\n0.00840\\n0.00791\\n0.01079\\n\\n0.11513\\n0.11478\\n0.11890\\n\\n0.00521\\n0.00532\\n0.00652\\n\\ntanh(2f )\\n\\nUGP\\nEGP\\n[9]\\n\\n0.01101\\n0.57403\\n0.15743\\n\\n0.60256\\n1.25248\\n0.14663\\n\\n0.15703\\n0.18739\\n0.16049\\n\\n0.06077\\n0.07869\\n0.04563\\n\\n0.08767\\n0.08874\\n0.09434\\n\\n0.00292\\n0.00394\\n0.00425\\n\\n(a) g(f ) = 2 ? sign(f ) + f 3\\n\\n(b) MAP trace from learning m\\n\\nFigure 1: Learning the UGP with a non-differentiable forward model in (a), and a corresponding\\ntrace from the MAP objective function used to learn m is shown in (b). The optimization shown terminated because of a ?divergence? condition, though the objective function value has still improved.\\n\\nsquared error (SMSE) to test the predictions with the held out data in both the latent and observed\\nspaces. We also use average\\nnegative log predictive density (NLPD) on the latent test data, which\\nP\\nis calculated as ? N1? n log N (fn? |m?n , Cn? ). All GP methods use Mat?rn 52 covariance functions\\nwith the hyperparameters and ? 2 initialized at 1.0 and lower-bounded at 0.1 (and 0.01 for ? 2 ).\\nTable 1 shows results for multiple differentiable forward models, g(?). We test the EGP and UGP\\nagainst the model in [9] ? which uses 10,000 samples to evaluate the one dimensional expectations.\\nAlthough this number of samples may seem excessive for these simple problems, our goal here is\\nto have a competitive baseline algorithm. We also test against normal GP regression for a linear\\nforward model, g(f ) = f . In Figure 1 we show the results of the UGP using a forward model\\nfor which no derivative exists at the zero crossing points, as well as an objective function trace for\\nlearning the posterior mean. We use quadrature for the predictions in observation space in Table 1\\nand the unscented transform, Equation (40), for the predictions in Figure 1. Interestingly, there is\\nalmost no difference in performance between the EGP and UGP, even though the EGP has access to\\nthe derivatives of the forward models and the UGP does not. Both the UGP and EGP consistently\\noutperformed [9] in terms of NLPD and SMSE, apart from the tanh experiment for inversion. In\\nthis experiment, the UGP had the best performance but the EGP was outperformed by [9].\\n7\\n\\n\\fTable 2: Classification performance on the USPS handwritten-digits dataset for numbers ?3? and ?5?.\\nLower values of the negative\\n\\u0001 log probability (NLP) and error rate indicate better performance. The\\n2\\nlearned signal variance ?se\\nand length scale(lse ) are also shown for consistency with [3, ?3.7.3].\\n\\n4.2\\n\\nAlgorithm\\n\\nNLP y ?\\n\\nError rate (%)\\n\\nlog(?se )\\n\\nlog(lse )\\n\\nGP ? Laplace\\nGP ? EP\\nGP ? VB\\nSVM (RBF)\\nLogistic Reg.\\n\\n0.11528\\n0.07522\\n0.10891\\n0.08055\\n0.11995\\n\\n2.9754\\n2.4580\\n3.3635\\n2.3286\\n3.6223\\n\\n2.5855\\n5.2209\\n0.9045\\n?\\n?\\n\\n2.5823\\n2.5315\\n2.0664\\n?\\n?\\n\\nUGP\\nEGP\\n\\n0.07290\\n0.08051\\n\\n1.9405\\n2.1992\\n\\n1.5743\\n2.9134\\n\\n1.5262\\n1.7872\\n\\nBinary Handwritten Digit Classification\\n\\nFor this experiment we evaluate the EGP and UGP on a classification task. We are just interested\\nin a probabilistic prediction of class labels, and not the values of the latent function. We use the\\nUSPS handwritten digits dataset with the task of distinguishing between ?3? and ?5? ? this is the\\nsame experiment from [3, ?3.7.3]. A logistic sigmoid is used as the forward model, g(?), in our\\nalgorithms. We test against Laplace, expectation propagation and variational Bayes logistic GP\\nclassifiers (from the GPML Matlab toolbox [3]), a support vector machine (SVM) with a radial\\nbasis kernel function (and probabilistic outputs [16]), and logistic regression (both from the scikitlearn python library [17]). A squared exponential kernel with amplitude ?se and length scale lse is\\nused for the GPs in this experiment. We initialize these hyperparameters at 1.0, and put a lower\\nbound of 0.1 on them. We initialize ? 2 and place a lower bound at 10?14 for the EGP and UGP (the\\noptimized values are near or at this value). The hyperparameters for the SVM are learned using grid\\nsearch with three-fold cross validation.\\nThe results are summarized in Table 2, where we report the average Bernoulli negative logprobability (NLP), the error rate and the learned hyperparameter values for the GPs. Surprisingly,\\nthe UGP outperforms the other classifiers on this dataset, despite the other classifiers being specifically formulated for this task.\\n\\n5\\n\\nConclusion and Discussion\\n\\nWe have presented a variational inference framework with linearization for Gaussian models with\\nnonlinear likelihood functions, which we show can be used to derive updates for the extended and\\nunscented Kalman filter algorithms, the iEKF and the iSPKF. We then generalize these results and\\ndevelop two inference algorithms for Gaussian processes, the EGP and UGP. The UGP does not\\nuse derivatives of the nonlinear forward model, yet performs as well as the EGP for inversion and\\nclassification problems.\\nOur method is similar to the Warped GP (WGP) [18], however, we wish to infer the full posterior\\nover the latent function f . The goal of the WGP is to infer a transformation of a non-Gaussian\\nprocess observation to a space where a GP can be constructed. That is, the WGP is concerned with\\ninferring an inverse function g ?1 (?) so the transformed (latent) function is well modeled by a GP.\\nAs future work we would like to create multi-task EGPs and UGPs. This would extend their applicability to inversion problems where the forward models have multiple inputs and outputs, such as\\ninverse kinematics for dynamical systems.\\nAcknowledgments\\nThis research was supported by the Science Industry Endowment Fund (RP 04-174) Big Data Knowledge\\nDiscovery project. We thank F. Ramos, L. McCalman, S. O?Callaghan, A. Reid and T. Nguyen for their helpful\\nfeedback. NICTA is funded by the Australian Government through the Department of Communications and\\nthe Australian Research Council through the ICT Centre of Excellence Program.\\n\\n8\\n\\n\\fReferences\\n[1] D. W. Marquardt, ?An algorithm for least-squares estimation of nonlinear parameters,? Journal\\nof the Society for Industrial & Applied Mathematics, vol. 11, no. 2, pp. 431?441, 1963.\\n[2] S. Julier and J. Uhlmann, ?Unscented filtering and nonlinear estimation,? Proceedings of the\\nIEEE, vol. 92, no. 3, pp. 401?422, Mar 2004.\\n[3] C. E. Rasmussen and C. K. I. Williams, Gaussian processes for machine learning.\\nPress, Cambridge, Massachusetts, 2006.\\n\\nThe MIT\\n\\n[4] A. Reid, S. O?Callaghan, E. V. Bonilla, L. McCalman, T. Rawling, and F. Ramos, ?Bayesian\\njoint inversions for the exploration of Earth resources,? in Proceedings of the Twenty-Third\\ninternational joint conference on Artificial Intelligence. AAAI Press, 2013, pp. 2877?2884.\\n[5] K. M. A. Chai, C. K. I. Williams, S. Klanke, and S. Vijayakumar, ?Multi-task Gaussian process\\nlearning of robot inverse dynamics,? in Advances in Neural Information Processing Systems\\n(NIPS). Curran Associates, Inc., 2009, pp. 265?272.\\n[6] N. D. Lawrence, ?Gaussian process latent variable models for visualisation of high dimensional\\ndata.? in Advances in Neural Information Processing Systems (NIPS), vol. 2, 2003, p. 5.\\n[7] J. M. Wang, D. J. Fleet, and A. Hertzmann, ?Gaussian process dynamical models,? in Advances\\nin Neural Information Processing Systems (NIPS), vol. 18, 2005, p. 3.\\n[8] ??, ?Gaussian process dynamical models for human motion,? Pattern Analysis and Machine\\nIntelligence, IEEE Transactions on, vol. 30, no. 2, pp. 283?298, 2008.\\n[9] M. Opper and C. Archambeau, ?The variational Gaussian approximation revisited,? Neural\\ncomputation, vol. 21, no. 3, pp. 786?792, 2009.\\n[10] B. M. Bell and F. W. Cathey, ?The iterated Kalman filter update as a Gauss-newton method,?\\nIEEE Transactions on Automatic Control, vol. 38, no. 2, pp. 294?297, 1993.\\n[11] G. Sibley, G. Sukhatme, and L. Matthies, ?The iterated sigma point kalman filter with applications to long range stereo.? in Robotics: Science and Systems, vol. 8, no. 1, 2006, pp. 235?244.\\n[12] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, ?An introduction to variational\\nmethods for graphical models,? Machine Learning, vol. 37, no. 2, pp. 183?233, 1999.\\n[13] M. Geist and O. Pietquin, ?Statistically linearized recursive least squares,? in Machine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop on. IEEE, 2010, pp.\\n272?276.\\n[14] J. Nocedal and S. J. Wright, Numerical Optimization, 2nd ed. New York: Springer, 2006.\\n[15] S. G. Johnson, ?The nlopt nonlinear-optimization package.? [Online]. Available: http:\\n//ab-initio.mit.edu/wiki/index.php/Citing_NLopt\\n[16] J. Platt et al., ?Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods,? Advances in large margin classifiers, vol. 10, no. 3, pp. 61?74,\\n1999.\\n[17] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, ?Scikit-learn: Machine learning in Python,? Journal of Machine Learning Research, vol. 12, pp. 2825?2830, 2011.\\n[18] E. Snelson, C. E. Rasmussen, and Z. Ghahramani, ?Warped Gaussian processes,? in NIPS,\\n2003.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-wBW__j5JsZ"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of `paper_text` column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "b-kJ-Q3c5Jsa",
        "outputId": "6576ff1b-4e40-4946-e98b-aea4e48ea9b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5010    transportability from multiple environments\\nw...\n",
              "1184    550\\n\\nackley and littman\\n\\ngeneralization an...\n",
              "3337    generalized roof duality and bisubmodular func...\n",
              "2031    response analysis of neuronal population with\\...\n",
              "6452    simplifying neural nets by\\ndiscovering flat m...\n",
              "Name: paper_text_processed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5010</th>\n",
              "      <td>transportability from multiple environments\\nw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1184</th>\n",
              "      <td>550\\n\\nackley and littman\\n\\ngeneralization an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3337</th>\n",
              "      <td>generalized roof duality and bisubmodular func...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2031</th>\n",
              "      <td>response analysis of neuronal population with\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6452</th>\n",
              "      <td>simplifying neural nets by\\ndiscovering flat m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUYAfkEL5Jsa"
      },
      "source": [
        "** **\n",
        "#### Step 3: Exploratory Analysis <a class=\"anchor\\\" id=\"eda\"></a>\n",
        "** **\n",
        "\n",
        "To verify whether the preprocessing, we’ll make a simple word cloud using the `wordcloud` package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "zNxgrX_A5Jsa",
        "outputId": "4457f3a1-6901-49f2-c2d7-ca725c7026ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vge_7saF5Jsb"
      },
      "source": [
        "** **\n",
        "#### Step 4: Prepare text for LDA analysis <a class=\"anchor\\\" id=\"data_preparation\"></a>\n",
        "** **\n",
        "\n",
        "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q636JN4C5Jsb",
        "outputId": "aef0514f-aabf-4923-ced4-dff92fb188b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['multiple', 'environments', 'limited', 'experiments', 'completeness', 'results', 'judea', 'pearl', 'computer', 'science', 'ucla', 'judea', 'csuclaedu', 'elias', 'bareinboim', 'computer', 'science', 'ucla', 'eb', 'csuclaedu', 'abstract', 'paper', 'addresses', 'problem', 'mz', 'transferring', 'causal', 'knowledge', 'collected', 'several']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhPAYFFm5Jsb",
        "outputId": "f75de7a7-408b-4c7f-dd0b-1a1be29c3255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 6), (1, 3), (2, 1), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 4), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 6), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 12), (29, 1)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xTTQY7G5Jsb"
      },
      "source": [
        "** **\n",
        "#### Step 5: LDA model tranining <a class=\"anchor\\\" id=\"train_model\"></a>\n",
        "** **\n",
        "\n",
        "To keep things simple, we'll keep all the parameters to default except for inputting the number of topics. For this tutorial, we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caQXKwf45Jsb",
        "outputId": "f5b4028e-2252-4ff0-cebb-ce71a762c7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.006*\"model\" + 0.005*\"function\" + 0.005*\"learning\" + 0.004*\"time\" + '\n",
            "  '0.004*\"figure\" + 0.004*\"two\" + 0.004*\"algorithm\" + 0.004*\"using\" + '\n",
            "  '0.004*\"one\" + 0.004*\"data\"'),\n",
            " (1,\n",
            "  '0.006*\"model\" + 0.005*\"image\" + 0.004*\"learning\" + 0.004*\"data\" + '\n",
            "  '0.004*\"using\" + 0.004*\"problem\" + 0.004*\"function\" + 0.004*\"set\" + '\n",
            "  '0.004*\"one\" + 0.004*\"images\"'),\n",
            " (2,\n",
            "  '0.005*\"data\" + 0.005*\"learning\" + 0.005*\"model\" + 0.004*\"set\" + '\n",
            "  '0.004*\"problem\" + 0.004*\"time\" + 0.003*\"algorithm\" + 0.003*\"using\" + '\n",
            "  '0.003*\"one\" + 0.003*\"function\"'),\n",
            " (3,\n",
            "  '0.007*\"model\" + 0.006*\"data\" + 0.005*\"time\" + 0.005*\"using\" + '\n",
            "  '0.004*\"function\" + 0.004*\"algorithm\" + 0.004*\"set\" + 0.004*\"learning\" + '\n",
            "  '0.003*\"figure\" + 0.003*\"problem\"'),\n",
            " (4,\n",
            "  '0.006*\"data\" + 0.005*\"model\" + 0.004*\"learning\" + 0.004*\"using\" + '\n",
            "  '0.003*\"one\" + 0.003*\"function\" + 0.003*\"time\" + 0.003*\"neural\" + '\n",
            "  '0.003*\"problem\" + 0.003*\"set\"'),\n",
            " (5,\n",
            "  '0.006*\"data\" + 0.005*\"learning\" + 0.004*\"model\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"time\" + 0.004*\"image\" + 0.004*\"one\" + 0.003*\"function\" + 0.003*\"set\" '\n",
            "  '+ 0.003*\"images\"'),\n",
            " (6,\n",
            "  '0.006*\"model\" + 0.005*\"using\" + 0.005*\"time\" + 0.005*\"function\" + '\n",
            "  '0.004*\"algorithm\" + 0.004*\"data\" + 0.004*\"learning\" + 0.004*\"figure\" + '\n",
            "  '0.003*\"one\" + 0.003*\"network\"'),\n",
            " (7,\n",
            "  '0.006*\"learning\" + 0.005*\"function\" + 0.005*\"model\" + 0.005*\"data\" + '\n",
            "  '0.004*\"using\" + 0.004*\"time\" + 0.003*\"algorithm\" + 0.003*\"figure\" + '\n",
            "  '0.003*\"number\" + 0.003*\"two\"'),\n",
            " (8,\n",
            "  '0.006*\"algorithm\" + 0.006*\"data\" + 0.006*\"learning\" + 0.006*\"model\" + '\n",
            "  '0.005*\"set\" + 0.004*\"using\" + 0.004*\"function\" + 0.004*\"figure\" + '\n",
            "  '0.004*\"time\" + 0.004*\"also\"'),\n",
            " (9,\n",
            "  '0.006*\"learning\" + 0.006*\"model\" + 0.005*\"algorithm\" + 0.004*\"time\" + '\n",
            "  '0.004*\"data\" + 0.004*\"function\" + 0.004*\"set\" + 0.003*\"network\" + '\n",
            "  '0.003*\"input\" + 0.003*\"first\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMwZGnhw5Jsb"
      },
      "source": [
        "** **\n",
        "#### Step 6: Analyzing our LDA model <a class=\"anchor\\\" id=\"results\"></a>\n",
        "** **\n",
        "\n",
        "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
        "\n",
        "1. Better understanding and interpreting individual topics, and\n",
        "2. Better understanding the relationships between the topics.\n",
        "\n",
        "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
        "\n",
        "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll2mYOtk6wjN",
        "outputId": "db3e2fe6-6b5b-4b1c-9d7f-e240085728df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./results', exist_ok=True)"
      ],
      "metadata": {
        "id": "UJeLLFMy6Nny"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "vzHu_M2v5Jsc",
        "outputId": "904ad2ba-bdc2-483f-dea3-a3921c62840d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "8     -0.000933 -0.002122       1        1  22.603299\n",
              "9      0.001730 -0.002590       2        1  16.289624\n",
              "1     -0.010252 -0.000672       3        1  13.910325\n",
              "6      0.002769  0.007275       4        1  10.392839\n",
              "0     -0.000456 -0.001531       5        1   9.747563\n",
              "3      0.000624  0.002559       6        1   8.482493\n",
              "2      0.007930 -0.005451       7        1   6.824852\n",
              "7      0.000687  0.009225       8        1   6.311699\n",
              "5     -0.006244 -0.003639       9        1   3.933758\n",
              "4      0.004146 -0.003055      10        1   1.503546, topic_info=            Term         Freq        Total Category  logprob  loglift\n",
              "230         data  1117.000000  1117.000000  Default  30.0000  30.0000\n",
              "609        model  1303.000000  1303.000000  Default  29.0000  29.0000\n",
              "556     learning  1165.000000  1165.000000  Default  28.0000  28.0000\n",
              "1010       using   878.000000   878.000000  Default  27.0000  27.0000\n",
              "658          one   722.000000   722.000000  Default  26.0000  26.0000\n",
              "...          ...          ...          ...      ...      ...      ...\n",
              "387       figure     8.322990   765.762734  Topic10  -6.0266  -0.3245\n",
              "1008        used     7.546426   621.317162  Topic10  -6.1246  -0.2134\n",
              "34          also     7.301712   602.486414  Topic10  -6.1576  -0.2156\n",
              "1264      linear     6.604754   450.719315  Topic10  -6.2579  -0.0257\n",
              "1329  parameters     6.606977   514.212088  Topic10  -6.2575  -0.1572\n",
              "\n",
              "[888 rows x 6 columns], token_table=       Topic      Freq Term\n",
              "term                       \n",
              "2562       1  0.149158  abc\n",
              "2562       2  0.099439  abc\n",
              "2562       3  0.149158  abc\n",
              "2562       4  0.099439  abc\n",
              "2562       5  0.198878  abc\n",
              "...      ...       ...  ...\n",
              "10516      4  0.077782   zm\n",
              "10516      5  0.155565   zm\n",
              "10516      6  0.155565   zm\n",
              "10516      8  0.077782   zm\n",
              "10516      9  0.077782   zm\n",
              "\n",
              "[3963 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[9, 10, 2, 7, 1, 4, 3, 8, 6, 5])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el272140274952825520569640130\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el272140274952825520569640130_data = {\"mdsDat\": {\"x\": [-0.0009333143226348683, 0.0017301704543998648, -0.010252344874070597, 0.002768871529505778, -0.00045631234311054736, 0.0006236123763514362, 0.007929773332822415, 0.0006872399618116119, -0.006243723448807666, 0.004146027333732583], \"y\": [-0.002121656247116544, -0.0025895394723963183, -0.0006718913798187312, 0.007275490283497792, -0.0015308283104544117, 0.0025589548090016816, -0.005451206072756337, 0.009224684476703474, -0.0036394788246398692, -0.003054529262020722], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [22.603299359960356, 16.289623917316707, 13.910325471466823, 10.392839112960841, 9.747563474970816, 8.48249293132731, 6.82485214174917, 6.311699346843794, 3.9337578370708415, 1.5035464063333417]}, \"tinfo\": {\"Term\": [\"data\", \"model\", \"learning\", \"using\", \"one\", \"function\", \"time\", \"neural\", \"problem\", \"set\", \"first\", \"image\", \"algorithm\", \"input\", \"two\", \"given\", \"number\", \"models\", \"figure\", \"images\", \"results\", \"based\", \"training\", \"used\", \"distribution\", \"output\", \"parameters\", \"vector\", \"case\", \"matrix\", \"saga\", \"suboptimality\", \"datapoint\", \"ups\", \"svrg\", \"err\", \"kyk\", \"polytope\", \"yxi\", \"ann\", \"egi\", \"bilmes\", \"offs\", \"maximin\", \"hxi\", \"sgd\", \"players\", \"classif\", \"eki\", \"practically\", \"lipschitz\", \"orlin\", \"condence\", \"aff\", \"stefanie\", \"renumber\", \"kxk\", \"wolfes\", \"oscillates\", \"tracked\", \"opponents\", \"bf\", \"round\", \"best\", \"formulation\", \"linear\", \"classier\", \"mtl\", \"xt\", \"least\", \"theorem\", \"classication\", \"algorithm\", \"also\", \"policy\", \"minimax\", \"fi\", \"dag\", \"empirical\", \"major\", \"set\", \"dimension\", \"data\", \"work\", \"kernel\", \"value\", \"performance\", \"learning\", \"figure\", \"size\", \"feature\", \"using\", \"approach\", \"different\", \"let\", \"two\", \"model\", \"algorithms\", \"matrix\", \"function\", \"error\", \"time\", \"number\", \"one\", \"results\", \"image\", \"used\", \"training\", \"problem\", \"input\", \"given\", \"parameters\", \"network\", \"first\", \"sse\", \"lateral\", \"copy\", \"lambon\", \"aoa\", \"age\", \"autoencoding\", \"lmc\", \"spelling\", \"hitting\", \"neill\", \"momentum\", \"casp\", \"latency\", \"scan\", \"q_\", \"spoken\", \"lexical\", \"oldfield\", \"gerhand\", \"bin\", \"iac\", \"trer\", \"ralph\", \"gridded\", \"clutter\", \"morrison\", \"hop\", \"qi\", \"vsconsistency\", \"templates\", \"rnn\", \"hops\", \"push\", \"acquired\", \"kalman\", \"frequency\", \"graphs\", \"lasso\", \"output\", \"units\", \"statistic\", \"connectionist\", \"ii\", \"network\", \"ji\", \"log\", \"state\", \"first\", \"input\", \"learning\", \"convex\", \"well\", \"models\", \"dynamics\", \"world\", \"neural\", \"states\", \"xi\", \"model\", \"time\", \"algorithm\", \"weights\", \"matrix\", \"probability\", \"gaussian\", \"let\", \"set\", \"distribution\", \"networks\", \"results\", \"data\", \"function\", \"training\", \"method\", \"also\", \"error\", \"problem\", \"two\", \"used\", \"case\", \"given\", \"figure\", \"one\", \"using\", \"number\", \"dps\", \"lsbp\", \"ljt\", \"vmt\", \"inv\", \"mailxidianeducn\", \"annotations\", \"wzm\", \"saliency\", \"mra\", \"qv\", \"scg\", \"hji\", \"bmb\", \"nwa\", \"oliva\", \"bumps\", \"closure\", \"memorable\", \"fragments\", \"memorability\", \"cameras\", \"ellipses\", \"sailing\", \"prec\", \"sethuraman\", \"msrc\", \"matched\", \"gyrus\", \"contour\", \"attributes\", \"descriptions\", \"cml\", \"images\", \"photo\", \"pictures\", \"image\", \"activation\", \"dictionary\", \"types\", \"dp\", \"template\", \"boundary\", \"scene\", \"captions\", \"pv\", \"fig\", \"hierarchical\", \"models\", \"color\", \"problem\", \"parameters\", \"objects\", \"cycle\", \"sequence\", \"features\", \"one\", \"information\", \"mixture\", \"model\", \"distribution\", \"using\", \"used\", \"set\", \"level\", \"given\", \"network\", \"function\", \"learning\", \"data\", \"neural\", \"shown\", \"new\", \"number\", \"output\", \"state\", \"different\", \"input\", \"memory\", \"also\", \"algorithm\", \"figure\", \"time\", \"two\", \"results\", \"training\", \"dydx\", \"bellman\", \"temperature\", \"acquisitions\", \"sabes\", \"asignificant\", \"nanostructure\", \"reinforce\", \"messages\", \"adds\", \"acquisition\", \"vjn\", \"sharpen\", \"quoted\", \"ininin\", \"lgi\", \"uij\", \"scattering\", \"paige\", \"nanocrystalline\", \"haar\", \"fampi\", \"backup\", \"alan\", \"reinf\", \"lowering\", \"ojpm\", \"floating\", \"ylx\", \"complicating\", \"symbolic\", \"electron\", \"initializing\", \"lightweight\", \"execution\", \"bopp\", \"optimizer\", \"hierarchy\", \"optimization\", \"video\", \"opi\", \"observe\", \"reference\", \"program\", \"constraints\", \"memory\", \"energy\", \"line\", \"using\", \"random\", \"et\", \"next\", \"shows\", \"figure\", \"distance\", \"vector\", \"time\", \"space\", \"model\", \"training\", \"function\", \"network\", \"convergence\", \"networks\", \"probability\", \"second\", \"used\", \"algorithm\", \"first\", \"one\", \"data\", \"number\", \"learning\", \"different\", \"problem\", \"distribution\", \"output\", \"results\", \"input\", \"error\", \"neural\", \"given\", \"based\", \"two\", \"set\", \"buffer\", \"regret\", \"vesicle\", \"restless\", \"stripe\", \"arm\", \"elvgren\", \"ucb\", \"interspike\", \"probs\", \"summable\", \"cursive\", \"fires\", \"wheeler\", \"release\", \"rostamizadeh\", \"chunks\", \"timepoints\", \"bubeck\", \"seg\", \"plastic\", \"mutate\", \"magleby\", \"remix\", \"cachan\", \"cbe\", \"prij\", \"ojj\", \"pulls\", \"ade\", \"ps\", \"arms\", \"circuit\", \"ohc\", \"responses\", \"motor\", \"rule\", \"bci\", \"spike\", \"chunk\", \"wji\", \"trains\", \"synaptic\", \"waiting\", \"two\", \"statement\", \"function\", \"weights\", \"convergence\", \"figure\", \"term\", \"pattern\", \"et\", \"one\", \"based\", \"analysis\", \"section\", \"model\", \"weight\", \"time\", \"given\", \"bopp\", \"neural\", \"used\", \"learning\", \"using\", \"number\", \"probability\", \"see\", \"training\", \"algorithm\", \"first\", \"set\", \"shown\", \"also\", \"data\", \"image\", \"case\", \"input\", \"parameters\", \"problem\", \"results\", \"network\", \"error\", \"distribution\", \"wettschereck\", \"chris\", \"dietrich\", \"heard\", \"eager\", \"dietterich\", \"bnge\", \"localism\", \"yale\", \"pay\", \"moore\", \"deferring\", \"allocating\", \"vote\", \"lazy\", \"rfs\", \"fld\", \"manifolds\", \"isomap\", \"firstnearest\", \"penalties\", \"binocular\", \"ko\", \"radical\", \"hourglass\", \"supu\", \"federico\", \"talks\", \"nettalk\", \"extremes\", \"isometry\", \"leigs\", \"plds\", \"disparity\", \"face\", \"fisherface\", \"riemannian\", \"vl\", \"motion\", \"kernel\", \"eigenface\", \"hopfield\", \"manifold\", \"hk\", \"basis\", \"message\", \"method\", \"data\", \"model\", \"bopp\", \"time\", \"metric\", \"using\", \"methods\", \"log\", \"given\", \"set\", \"neural\", \"problem\", \"function\", \"thus\", \"algorithm\", \"figure\", \"local\", \"based\", \"space\", \"results\", \"algorithms\", \"one\", \"matrix\", \"learning\", \"state\", \"training\", \"image\", \"network\", \"used\", \"parameters\", \"two\", \"distribution\", \"also\", \"number\", \"models\", \"rgcs\", \"wiesel\", \"deprivation\", \"monocular\", \"ocular\", \"ohc\", \"tanaka\", \"firings\", \"tnwiesel\", \"ipsilateral\", \"hubel\", \"potts\", \"strabismus\", \"retract\", \"pathways\", \"dag\", \"dhhubel\", \"segregation\", \"isb\", \"arm\", \"ohcs\", \"pulls\", \"tps\", \"pq\", \"violated\", \"wavelength\", \"hdbl\", \"reichardt\", \"pl\", \"stripe\", \"bm\", \"lpf\", \"fluid\", \"mixing\", \"remix\", \"dominance\", \"rbfs\", \"cochlea\", \"arms\", \"nodes\", \"iout\", \"cochlear\", \"picking\", \"hard\", \"velocity\", \"output\", \"line\", \"first\", \"values\", \"problem\", \"test\", \"data\", \"see\", \"machine\", \"node\", \"input\", \"basis\", \"one\", \"set\", \"learning\", \"information\", \"neural\", \"two\", \"noise\", \"thus\", \"given\", \"model\", \"training\", \"time\", \"network\", \"using\", \"error\", \"figure\", \"results\", \"algorithm\", \"number\", \"also\", \"function\", \"distribution\", \"used\", \"matrix\", \"parameters\", \"sia\", \"herz\", \"garch\", \"engle\", \"virus\", \"alk\", \"lbcr\", \"microscopic\", \"ngp\", \"vmf\", \"vilk\", \"peretto\", \"bru\", \"padua\", \"battisti\", \"volatility\", \"ilk\", \"cab\", \"regulating\", \"zippelius\", \"economic\", \"crisis\", \"jji\", \"fontanari\", \"precission\", \"overtures\", \"lett\", \"hemmen\", \"scarpa\", \"ti\", \"flexible\", \"bcr\", \"bvcs\", \"shrinkage\", \"modality\", \"randomness\", \"tpe\", \"vgpds\", \"secretary\", \"financial\", \"plots\", \"stream\", \"place\", \"adaptive\", \"war\", \"constraints\", \"delays\", \"pool\", \"unknown\", \"function\", \"scattering\", \"prior\", \"demonstrate\", \"online\", \"learning\", \"functions\", \"points\", \"gradient\", \"patterns\", \"distribution\", \"using\", \"number\", \"random\", \"models\", \"process\", \"statistical\", \"mean\", \"results\", \"data\", \"based\", \"model\", \"time\", \"two\", \"figure\", \"performance\", \"vector\", \"used\", \"shows\", \"first\", \"error\", \"input\", \"algorithm\", \"parameters\", \"one\", \"problem\", \"state\", \"probability\", \"training\", \"given\", \"also\", \"set\", \"image\", \"network\", \"cx\", \"psbd\", \"xml\", \"sbd\", \"zm\", \"disparity\", \"projector\", \"vp\", \"uiuc\", \"kbk\", \"ipw\", \"pis\", \"eigenspace\", \"fcp\", \"kpf\", \"independences\", \"cxi\", \"annotated\", \"stickk\", \"qik\", \"hkh\", \"disambiguate\", \"annotations\", \"retinas\", \"rfs\", \"bootstrap\", \"hiv\", \"women\", \"kskop\", \"sticki\", \"mnar\", \"cholesky\", \"missingness\", \"prec\", \"missing\", \"kpca\", \"trial\", \"eigenspaces\", \"objects\", \"depth\", \"object\", \"cma\", \"mt\", \"images\", \"associated\", \"caption\", \"image\", \"trials\", \"lda\", \"non\", \"features\", \"data\", \"eigenvalues\", \"firing\", \"see\", \"one\", \"neural\", \"computational\", \"analysis\", \"learning\", \"adaptive\", \"based\", \"information\", \"time\", \"et\", \"feature\", \"algorithm\", \"matrix\", \"parameters\", \"results\", \"first\", \"function\", \"model\", \"set\", \"fig\", \"order\", \"used\", \"using\", \"two\", \"distribution\", \"let\", \"training\", \"input\", \"problem\", \"network\", \"also\", \"figure\", \"efferent\", \"quantum\", \"membrane\", \"ornstein\", \"tunneling\", \"fohc\", \"estimator\", \"tosdyks\", \"oli\", \"depressing\", \"thermal\", \"electron\", \"partners\", \"testable\", \"kingdom\", \"luscher\", \"ihc\", \"uhlenbeck\", \"basilar\", \"deprivation\", \"compartment\", \"hkhzl\", \"ou\", \"sem\", \"stp\", \"abc\", \"prij\", \"atsushi\", \"unacceptable\", \"trumpington\", \"tuneable\", \"matsuura\", \"cochlear\", \"minimax\", \"fluid\", \"winning\", \"iin\", \"presynaptic\", \"leverage\", \"imem\", \"ut\", \"cochlea\", \"depression\", \"reference\", \"circuit\", \"loss\", \"clustering\", \"plasticity\", \"neural\", \"gate\", \"data\", \"test\", \"section\", \"vector\", \"consider\", \"work\", \"task\", \"ie\", \"case\", \"one\", \"input\", \"log\", \"problem\", \"given\", \"show\", \"mean\", \"first\", \"method\", \"using\", \"models\", \"gaussian\", \"model\", \"number\", \"problems\", \"time\", \"function\", \"set\", \"learning\", \"training\", \"two\", \"based\", \"algorithm\", \"results\", \"figure\", \"used\", \"also\", \"linear\", \"parameters\"], \"Freq\": [1117.0, 1303.0, 1165.0, 878.0, 722.0, 962.0, 912.0, 554.0, 676.0, 839.0, 551.0, 585.0, 1024.0, 578.0, 668.0, 575.0, 572.0, 497.0, 765.0, 398.0, 579.0, 457.0, 557.0, 621.0, 519.0, 442.0, 514.0, 345.0, 401.0, 391.0, 39.22024837501867, 7.918762686627958, 2.7359592104157726, 2.261018484593174, 4.641766429117021, 14.197166555685087, 1.7345778302473287, 9.523967196963524, 1.7279107706171049, 27.06573264823409, 1.2492464983089941, 2.1969424044102226, 1.6357089814931172, 3.426272309165428, 2.0532827436863443, 22.348554195023226, 2.5396322115024126, 10.1053507896657, 3.195154616237565, 2.4654877415873706, 4.5068387663813, 2.545594676219381, 2.7612880396556343, 3.447996690337262, 2.0572690987938107, 1.6332008423733653, 5.719168244297444, 11.42124289529781, 0.795231260710544, 5.984302255781076, 11.486369644180375, 10.217081890316075, 5.583827178978786, 88.34766249231929, 28.295234175411956, 153.82436118764528, 11.66609641711279, 39.5471713621582, 55.908583421795484, 47.909029196805555, 78.20305882564989, 10.19895177302455, 309.6345514717968, 185.29936129446023, 98.58692629787025, 22.248865330561618, 33.75949295550287, 27.64095971086444, 48.791474729301086, 20.28173440270634, 240.2760749435708, 45.823491621846586, 296.1982795278435, 94.78813073526808, 88.66834219519532, 104.61757679195763, 108.31041455676932, 295.6719003558161, 203.78642304974989, 79.36588485302772, 69.2961945624461, 218.57233070797545, 109.51074253524757, 124.38042909356584, 103.89568535603433, 169.3305187416108, 290.22261459482405, 99.18763344378769, 105.1572693376779, 210.37138907435255, 122.42597031276932, 195.95910078991594, 137.7016604530524, 162.06778974366068, 137.24008590782196, 135.96644384791986, 135.16738795323087, 124.02575985277582, 135.0196131262845, 124.14786147406059, 122.75477322552324, 114.04867174077236, 113.36366667838074, 103.97087031046128, 5.817314805138051, 10.14663536859908, 3.1180175855334484, 1.249707787205147, 18.31101597019533, 9.821622291501711, 1.7905304162727835, 1.1939805689071996, 2.683142740193024, 3.154208588518899, 1.5061180137805512, 3.183816888511976, 1.086879398625782, 5.839955215066563, 15.509164663407468, 1.1091046519163226, 1.7570029669407345, 1.1370389375923038, 0.8471107758327169, 1.398674606008739, 2.991526445983881, 0.8384474646183564, 1.6186253947124616, 1.399259318353329, 2.841745759788516, 3.0437278569989332, 2.18442021543427, 6.831711075992752, 17.860259565570697, 0.5416209000527283, 3.077456870349352, 10.47972187974309, 8.442984752732437, 7.803374687621852, 4.548072569533583, 12.572315753379073, 52.19929941789152, 25.691843973449263, 4.157598077623343, 103.90034889234045, 45.98796351835142, 10.503040784330874, 11.563739966356247, 38.10442968845987, 129.2319991882103, 11.961697743312616, 80.97916395598261, 99.00973329234922, 117.88325395501776, 121.84277481354786, 228.0186904092976, 35.969222218363015, 67.90338450613015, 102.3339134916656, 38.85940287394266, 22.118882399065974, 109.19730450539113, 29.598316118138605, 66.83528603539267, 227.9248841733283, 166.41365439343463, 181.308230113289, 56.144593635200565, 77.12451401555764, 87.18358877851254, 65.88832060795919, 71.72113912479476, 138.10137705937666, 93.04259023027647, 65.9713927104728, 100.10036996388385, 164.86953972732, 145.68483357823033, 94.0365936047615, 74.27934234703882, 97.82445531449544, 84.58813793920426, 104.68668940784042, 103.49357394680708, 97.16792474515363, 73.52148120992938, 87.39055574553936, 99.57311872308206, 89.39936966406049, 89.35168366762282, 78.17668525910115, 11.741200641056276, 13.210790965563932, 2.82975072695328, 5.331192188943465, 1.4178161968791345, 1.6214334524044065, 7.307245604268377, 2.7227827576157613, 8.115715038389286, 3.486388059271577, 8.178718372414972, 10.522988549629083, 5.220997636269088, 1.2800283213388708, 2.4439165879165303, 3.042193616418053, 2.4737961423856336, 2.397697986834847, 4.145825787372311, 11.537601839730664, 19.3361609136007, 1.8701758686654422, 1.2318135329618407, 1.4087633093362677, 2.2360591608428595, 0.9158973076480804, 7.048373341227685, 14.860088518493392, 1.507945115251009, 19.526871951842683, 6.790258280693817, 10.04648115136255, 2.9919383042408314, 112.28468414879652, 4.112579908331183, 4.124924955454985, 152.66172648126613, 35.26773136957197, 15.858563320831534, 19.602649770342758, 20.26202041336548, 8.48694482558154, 17.780266216552686, 17.03877229588626, 8.96314674300841, 6.8132738014024214, 49.34968126582412, 31.799203493647248, 103.7164312801525, 20.949458153109525, 127.642411168745, 99.76318609233117, 26.495125746891684, 23.061526680165805, 55.39412757525926, 60.91528613486596, 119.97356817312223, 76.61431974741764, 31.642437040622216, 180.6048332651838, 87.43862939761581, 128.9331782093882, 98.9426404156551, 124.32677227476194, 42.23004639425477, 89.66140826642378, 88.60367575227028, 125.49880847671422, 142.31462804233286, 137.25218099718427, 83.53572448092864, 63.45414574991862, 59.844443014463124, 83.77565971744984, 69.2836263995505, 69.60184644018602, 69.03875147285605, 79.6794309264027, 50.60618245568644, 80.02826961800695, 108.81067061694255, 90.95010286562584, 98.34360827380144, 83.76495581739017, 77.83248448467212, 68.88978824468366, 1.4295321904598568, 5.22721241879962, 3.0544109051137984, 4.562002083396234, 1.3136116317872195, 1.950711168627216, 2.7032941330449503, 4.197017303495022, 3.6117978828460267, 8.705988560597234, 34.36857897265911, 3.332430096936877, 0.6007125571958032, 1.1296316433575406, 0.9489627863953591, 0.39141824722702034, 3.195472181585808, 15.335758253104883, 0.7486424543573421, 0.5485741599164935, 11.104790579097903, 1.805598039836165, 5.721641766187092, 1.3688068683428354, 1.0022273100313275, 0.3871245896990664, 0.38463915993473036, 2.176680802846987, 4.255135187625028, 2.5520472945005483, 5.048597583707085, 3.126475707866956, 4.297991204525841, 5.102885432883802, 6.89960297372598, 31.05962856473311, 5.514675584691999, 4.764971736840991, 52.5228114567755, 11.167484942232955, 12.071029799196708, 16.448550940714497, 11.166977832008202, 21.429040707608213, 33.14413601417062, 44.698071552562645, 21.678051945877257, 29.80927650545935, 112.59870435596916, 58.25691141580966, 47.60128105787311, 25.772937180735134, 38.5840787442293, 95.65187148697368, 36.3485565060014, 47.77950636849068, 107.38384086892168, 44.27967717457163, 140.63847280183165, 69.52413751643405, 107.35824533013584, 71.46005820141309, 25.317950394898418, 44.10548099690257, 54.985066769445254, 40.57202019738028, 69.42618407767215, 101.47554473221845, 62.99297960414289, 74.63448773669603, 98.86347893082693, 60.89008854190658, 97.85904827542954, 51.46051335061491, 65.18028266658139, 55.12174650258001, 50.24616819846225, 56.66631961318311, 55.36275081032363, 51.12302629313328, 53.58257331809109, 52.54982773665613, 48.82904555364767, 53.072166530017746, 50.896706676549094, 9.440834391383454, 9.426466757143414, 1.4944002018186282, 3.3885335326802823, 1.8120998130114994, 17.562745725237054, 1.1685717869655967, 15.12554288710598, 3.2347669234655294, 0.9412622244809405, 1.1250938663492218, 1.0859348733721506, 2.1502686833875555, 0.7530984793283342, 12.513387002565869, 0.7257774780783515, 2.980468516115255, 1.4029841607527347, 0.9168150591373058, 0.9236408565697711, 2.326897639310848, 0.72020060166408, 0.5495240280381667, 4.537122827348204, 0.5260389694400857, 0.5202936155649397, 1.7049468465970536, 1.2121993568834486, 3.8336201598078117, 2.8410984950271323, 3.795685037857165, 5.774449277923301, 13.686736424812253, 3.873591488232247, 11.094154668905297, 12.876198070364332, 19.083376521683437, 14.047453703688316, 21.769020623634812, 3.854312428982654, 3.5803962995070537, 3.4632771776011104, 24.62864440088649, 4.471756663112304, 88.63989947679528, 7.810580276412269, 116.78834489774891, 37.38660911269802, 24.527377369226322, 89.55238023670742, 27.477803137420988, 26.004257216412277, 42.72266336454177, 81.86380513189431, 55.64820406284867, 40.95877697789583, 41.85515807556562, 133.1418331761748, 30.713345959982217, 95.90877465717811, 64.32758246807202, 22.56734108859086, 61.57462350771987, 67.01022968009904, 108.49894996957737, 87.07491653128601, 62.29481922951542, 51.60508594200876, 39.845641229370585, 57.73452353315811, 87.96002838590547, 56.62940898719202, 75.0272056224589, 42.176076448038515, 56.68598423093987, 79.57869413209973, 53.88062459170193, 42.49547230153117, 51.75990400022867, 48.43557099385561, 55.22664588448354, 46.77901829142187, 44.401723321917615, 43.25625811908031, 43.00352777614985, 1.0421695976010827, 0.7749629385684071, 0.7052595400035739, 0.7055115015721908, 0.6927001758924002, 0.8828789984914144, 0.42936569197203245, 0.42967880376404405, 1.9127458647475608, 0.4100902780942303, 2.7480375329109354, 0.4044941125692384, 0.3988067525270658, 0.3980427286132401, 1.4064163529819833, 1.6328897441102144, 1.392913194624319, 1.8962647471311533, 2.5768443683858138, 0.38241127353369947, 0.3850452627858476, 5.439477703041244, 2.0047654852896506, 0.38478926700110394, 1.4456511573103723, 0.8893989287337064, 0.3780101914544606, 0.7821095170490514, 0.3761813114224815, 0.3794653756768056, 2.037476183608178, 0.8838944177680849, 7.638682072384015, 5.599032224816466, 13.487991969850961, 5.421339867335458, 3.9509099883689287, 1.5588137506940547, 19.178218428745133, 45.083985130939894, 4.599469823898556, 7.473459264623571, 3.636591611856838, 7.6387234054053845, 24.955247629748037, 4.6365503568152695, 49.28558980251051, 117.1453380084889, 133.48512160486973, 21.78071305102192, 92.70784252575426, 17.315442205112028, 88.5430774665443, 41.42585646702403, 41.14512711553455, 60.171271733166414, 80.38772130293711, 57.12989516311682, 65.1211286567789, 85.90519271868362, 32.65055933864169, 85.80321226464602, 66.29836105004982, 29.134027414230815, 43.629112604872475, 33.00352069289881, 50.79812019476984, 35.62444871029445, 58.868214828380154, 37.55759072247011, 78.73730848103826, 40.60320930616482, 46.143994810786324, 47.284455536389245, 47.352228102495246, 48.21612222144571, 42.819497236065594, 49.04510635730708, 42.594047686524895, 43.60732500675926, 41.61039878193679, 39.55514430328815, 1.0056752886465894, 0.6434352582382252, 1.8115422194349797, 1.9458622389730484, 3.545291924459505, 3.8338489755934364, 1.7125491572514333, 1.3381070527730439, 0.6117589179039483, 1.2165385676473657, 0.7451219231904065, 0.591375584906821, 0.44294541352675054, 0.43444099287895993, 0.42856189818868445, 13.75463659345798, 0.5554598753002815, 0.4159720585262314, 0.7075106253203924, 13.207660410899527, 0.9969265178766008, 3.2398049575180763, 0.9610658738883707, 7.312773334073352, 0.7534356357143935, 0.7155690543212347, 0.5500498918443727, 1.1269244273529513, 5.5334871530020076, 1.316132852993384, 10.617441148265737, 1.6548948284887561, 3.017059691148947, 12.642897407234384, 3.4794849230576173, 3.5356527648499245, 4.336241482441732, 3.76904465817003, 4.341810227990553, 23.275827540174525, 2.5088786318369656, 5.17099616687504, 2.0557786798190327, 9.519310401753724, 3.5995400432689317, 41.69884320338147, 20.098504691274627, 49.229605774412164, 29.65730015943039, 57.832632487548615, 26.882344687701718, 83.2087207937352, 30.099809518177203, 23.61583990951358, 19.715890325146997, 45.418389849895014, 18.19345068799735, 52.82110407350566, 59.28140286911981, 75.51013416264058, 33.41038136888077, 41.84654980570358, 46.69507578950008, 19.448439293423807, 25.0439097186258, 40.8887037045044, 72.2075764948309, 39.32339420224298, 55.25060416445734, 40.25267132676859, 52.846433119032035, 34.94445050017066, 46.28593699267591, 38.5424140836906, 54.579406324810165, 38.11658362808789, 38.931643157390944, 50.68534073238637, 35.129662312206726, 37.79060792333083, 29.845438478453175, 31.274370204006694, 1.3444931617804223, 3.0094460035647463, 3.4073675377497437, 0.7766124225725247, 1.3703942842927967, 0.9131894893226543, 2.952517313679733, 0.4569973020301621, 0.8895386646085943, 2.430473292145785, 0.43903515431525764, 0.5974375292130825, 0.44045993039259196, 0.7419199189869137, 0.4377028547868079, 0.7302118402010197, 0.4390468718734164, 0.28841234276076044, 0.42300366521105065, 0.4334198371296535, 1.0341372293635414, 1.9359133430708932, 0.4318732511106505, 0.42843294970363943, 0.28300907388044333, 0.9813763037602644, 0.9828582457224587, 1.2828628082670936, 0.4168849406616719, 19.327897001153563, 2.823086588163712, 2.4758297742563635, 2.4312546031248576, 1.2485896089424275, 5.864455749186708, 5.14061926122454, 3.84975575690981, 3.731170908551927, 4.461917648699997, 3.133465060422804, 6.215754989000881, 8.576263347007153, 10.680731315354192, 16.190696435852917, 2.3929653094847354, 20.960903934984692, 3.0089532868270674, 5.2377339619391, 11.854241447176141, 76.8218760174893, 8.322088076627702, 23.153948848861113, 11.455424428231899, 7.534885561975589, 82.52418140572604, 28.32263042429115, 25.061898041899127, 21.598367747594082, 16.757311836769215, 40.41669579259296, 61.817995382555715, 43.68044472209211, 33.54559131020912, 38.08474309748806, 27.54721098079715, 17.10684707061446, 27.27374834637242, 41.3638039526941, 67.54319282436965, 34.2164612755026, 71.55991235713151, 54.652475902516485, 43.07902611958009, 46.76721165316095, 27.96094474092503, 26.10075473504675, 38.0223157393672, 21.369590455679145, 34.56258899077209, 31.966016020276587, 35.04891368198344, 49.63068668942864, 31.65770805370995, 36.99708406485707, 35.25223232744735, 29.20370822196136, 29.17083291415924, 30.965668331567464, 31.033208652773805, 31.088598524459268, 32.315847853167, 29.10291510325905, 29.03974624752527, 0.7200336978212041, 0.6066314269653205, 1.0221229749095162, 0.9279434517811562, 1.3419976475179634, 3.0001069617101233, 0.4355264823809776, 0.6848898586788544, 1.392601687965161, 0.8652915429869329, 1.2235111436599575, 0.7211649496749641, 0.6023167445627242, 0.5008515669419751, 1.0197021366420664, 0.5135247320761527, 0.24894316937343644, 2.100123345103824, 0.25053421179161933, 0.24235845055571142, 0.2437470288916127, 0.24926655064452014, 2.06369037905936, 0.31939819192716595, 0.702889058370503, 0.5022950240888636, 0.4931998786576422, 0.41995901794556345, 0.2431540127926221, 0.2443169381228192, 1.143154647738333, 3.686861190138797, 3.292507305945222, 0.6550946029834185, 5.704569427463867, 0.9776209199591135, 4.060678320433159, 1.1057689172648388, 8.334787426402926, 7.510508735686485, 13.251024046925048, 5.381006583663424, 3.71946550573826, 26.949436966012975, 8.10068544700909, 2.722214054350842, 34.37311481897155, 5.845167365744963, 2.2966832287995587, 20.993055720467012, 18.09633676256428, 53.970085097287885, 3.261361611374631, 9.882193968954788, 17.82465671428045, 34.095286243232806, 25.72380458563263, 8.470940968728184, 15.855792981936428, 43.54853902301381, 8.648255719918229, 20.742787336121676, 19.03555111426322, 34.39260861288397, 15.881056140609664, 12.045360201558863, 35.016515049298334, 17.48835725992619, 21.200093255812625, 22.78338423887712, 21.486935610611518, 31.332496791631115, 38.06338033343786, 27.898337296562147, 11.098363036665365, 14.479564741709673, 22.0273233237324, 26.679328184738466, 22.621125712672765, 19.185472673695205, 15.464782766542431, 18.95348812364013, 18.87416224653729, 20.18021702992805, 18.17534544539118, 18.006957297351004, 18.574338033292413, 0.14037865784542095, 0.43702363051396553, 3.685233787189973, 0.10031809147390025, 0.2641386623758408, 0.2155361687686417, 2.8417271664912613, 0.06296496773111943, 0.18471763968438107, 0.382329990025466, 0.2420478424140089, 0.5592599576147647, 0.09263106325662551, 0.15412154956520713, 0.15348631074979174, 0.06172910858340617, 0.0892792551784902, 0.0917436345311286, 0.26921018547244846, 0.3330883936808366, 0.0913086359470161, 0.1156683371162928, 0.6183043492016714, 0.05941991337143227, 0.15410108834180988, 0.7141735233766228, 0.29905785396965284, 0.05847847128153512, 0.05986896258105361, 0.0899735693470494, 0.1473347461788034, 0.1499381890883752, 1.2530389980653174, 2.0230146810118783, 0.6363756113806615, 0.25850985634498913, 0.6584898908690127, 1.6460929021967754, 1.2305575958047426, 0.46360917030486487, 1.2590526657830976, 0.785980499559645, 1.275646354147418, 1.6936092071809337, 2.020107205147658, 3.8799116267579343, 4.127825680593089, 0.9984666538863597, 11.231626031994464, 1.4159306093527013, 19.18084127976187, 5.686865423409625, 6.603195031058063, 6.842939649775939, 5.27839968924236, 6.2961065671692396, 5.187221968744002, 5.4767074075489175, 7.442047883865028, 11.907023542346309, 9.817928368119949, 6.58149363923645, 10.81098964601145, 9.497953071960866, 5.3075463067978825, 6.164493956581564, 8.931740965205943, 7.0400522620213595, 12.421185709073427, 8.015637773351418, 5.926545810696603, 15.54648882200381, 8.689866271660927, 4.717205261579862, 11.68807224359573, 11.695969498024581, 10.642595709413197, 12.914209311892174, 8.128501229260108, 8.948112754048834, 7.162943047186038, 10.518796852394203, 7.712792999812705, 8.322989987553365, 7.546425564859269, 7.301711842068583, 6.604753847596657, 6.606976656552154], \"Total\": [1117.0, 1303.0, 1165.0, 878.0, 722.0, 962.0, 912.0, 554.0, 676.0, 839.0, 551.0, 585.0, 1024.0, 578.0, 668.0, 575.0, 572.0, 497.0, 765.0, 398.0, 579.0, 457.0, 557.0, 621.0, 519.0, 442.0, 514.0, 345.0, 401.0, 391.0, 76.94023967128615, 17.122571158532878, 6.1053607468077775, 5.0870747507315945, 10.831661516803598, 33.287640303899614, 4.072034634639064, 22.468930707922524, 4.087862076983631, 64.23479256809196, 2.978821871731159, 5.256045098214664, 3.9143210845943726, 8.209135841332886, 4.939428186263887, 54.37034887853488, 6.180344433564939, 24.68969758819584, 7.824823026708253, 6.055551652981368, 11.095004889400368, 6.286241324468576, 6.824304510595329, 8.535590738707336, 5.152705608003939, 4.094651037712775, 14.35323723220786, 28.740347047469434, 2.007764596443366, 15.165716796279339, 29.30871156917529, 26.06612456353197, 14.184433224408876, 241.32771839981646, 76.69685419880074, 450.7193152601116, 30.574995188428407, 111.57498905716704, 160.95391647910273, 136.98696858747329, 233.79439608728742, 26.894159887177473, 1024.7376425007296, 602.4864140424442, 306.81178618462815, 61.96000463110399, 97.1635367422619, 78.6026507850679, 146.2814240810992, 56.44096783582619, 839.1540416079167, 139.27497851611693, 1117.8103513189178, 314.41783289559896, 293.2654171754332, 353.6068360690249, 372.1187889657468, 1165.597589436764, 765.7627340788714, 268.18771952221886, 230.51656995645027, 878.8388333341857, 392.66970576240124, 457.1661851516394, 370.9716376167183, 668.6895612457299, 1303.3951176236164, 361.6734958208216, 391.5576813278483, 962.1424971153967, 489.74104783048176, 912.7005824324594, 572.3796955790533, 722.6277332017556, 579.8187937308272, 585.3298843959316, 621.3171616445462, 557.7258494493101, 676.9528424016491, 578.088978840387, 575.519583976074, 514.2120879198268, 588.0490558430073, 551.0179272659826, 16.967771082262022, 30.293592781559898, 9.597775372954278, 3.9068143492791503, 57.35848814719766, 31.02653186485493, 5.745430199394028, 3.8641549975681704, 8.68810939672085, 10.347303185631855, 4.945264355243951, 10.471727201749472, 3.6117349944765946, 19.443853860038296, 51.74838310533635, 3.7157614677747035, 5.923857496987522, 3.836677659367123, 2.865144700242344, 4.744848476719591, 10.14921342294838, 2.8691368599836755, 5.553189256100157, 4.819346835755841, 9.807706551033016, 10.55868120269918, 7.590264326573249, 23.762933935568956, 62.45158510799635, 1.8941022124827764, 10.852466406044892, 37.92382656760996, 30.458404443584914, 28.14603477955214, 16.18747848711258, 46.38842028261647, 207.2703956811572, 100.80197817444241, 14.860501661719159, 442.98819256300754, 188.34271718191073, 39.54637745735529, 43.877337216694, 158.11035985505995, 588.0490558430073, 45.67653992042437, 358.91729967097075, 452.6390309831054, 551.0179272659826, 578.088978840387, 1165.597589436764, 155.09187761641755, 314.2522427992213, 497.1262942547666, 171.07044833267616, 90.94957157214562, 554.9578690758188, 126.33540492597629, 318.9897015428115, 1303.3951176236164, 912.7005824324594, 1024.7376425007296, 267.8959239996827, 391.5576813278483, 454.0966541014372, 332.8806563023346, 370.9716376167183, 839.1540416079167, 519.2007574984889, 336.7415543869066, 579.8187937308272, 1117.8103513189178, 962.1424971153967, 557.7258494493101, 406.19545731704056, 602.4864140424442, 489.74104783048176, 676.9528424016491, 668.6895612457299, 621.3171616445462, 401.40365206669, 575.519583976074, 765.7627340788714, 722.6277332017556, 878.8388333341857, 572.3796955790533, 29.77578625303193, 34.16953060045916, 7.739189293811592, 14.75787809923053, 4.006495122428078, 4.7501544639872435, 21.55930672170217, 8.040887808770202, 24.004642028036827, 10.453041142012498, 24.610634899091828, 31.665229557090296, 15.813310625781742, 3.8853146229709457, 7.449480743581627, 9.341842887023569, 7.68635952654026, 7.465135347771402, 12.949536726643242, 36.19797349504783, 60.70456628629641, 5.888029752833192, 3.8845696931671636, 4.442753164875432, 7.069617771924318, 2.8981547625501203, 22.37318297249116, 47.28597957115996, 4.816312485763756, 62.42159804478867, 21.869789464191218, 32.62859232707339, 9.603049837228722, 398.8770279315848, 13.291519442394335, 13.387561415970895, 585.3298843959316, 126.85521175998348, 54.88494239806438, 68.80113062282298, 71.51730552526203, 28.79290257371073, 65.01589114724717, 62.525110277686785, 30.991125945832533, 22.851698361704457, 209.73940981760373, 128.90711222058061, 497.1262942547666, 82.70178080663436, 676.9528424016491, 514.2120879198268, 109.17935143230888, 93.43089792524736, 269.26208730812084, 303.16885144441153, 722.6277332017556, 413.68597446643406, 139.40210883943186, 1303.3951176236164, 519.2007574984889, 878.8388333341857, 621.3171616445462, 839.1540416079167, 207.850898934614, 575.519583976074, 588.0490558430073, 962.1424971153967, 1165.597589436764, 1117.8103513189178, 554.9578690758188, 376.76632063763043, 346.78038183990753, 572.3796955790533, 442.98819256300754, 452.6390309831054, 457.1661851516394, 578.088978840387, 279.30273196134084, 602.4864140424442, 1024.7376425007296, 765.7627340788714, 912.7005824324594, 668.6895612457299, 579.8187937308272, 557.7258494493101, 5.774867596994777, 22.106418770774113, 13.218662334928288, 19.763008047672926, 5.845598635407934, 8.747228499557705, 12.122735768935428, 18.853956800312844, 16.451982971261387, 39.81007731119556, 157.21800438921602, 15.33903423153329, 2.8044436782052404, 5.2897473793184, 4.468051800567968, 1.8491866918543163, 15.126196440566089, 72.59533088902427, 3.547207816404565, 2.6059295829773506, 52.98896200160098, 8.626145355764645, 27.365662779279795, 6.560318658377464, 4.807147699812197, 1.8610783241019035, 1.8504964830157287, 10.499268316067623, 20.553169289784645, 12.388356934958665, 24.52695294192535, 15.24576820061601, 21.039499871829182, 25.154362016433556, 34.393011496836, 160.26972678393577, 27.52870602472317, 23.7103969529702, 294.5733831010862, 58.392343218863225, 64.01814502767968, 89.16651531234793, 59.33731772989969, 120.36527297817818, 199.5417897230486, 279.30273196134084, 127.49300751507472, 189.7238277149612, 878.8388333341857, 413.2115829534304, 333.88756439303154, 163.94730250208812, 263.0058466334931, 765.7627340788714, 250.33646196008152, 345.73732892966024, 912.7005824324594, 318.19509939369885, 1303.3951176236164, 557.7258494493101, 962.1424971153967, 588.0490558430073, 166.62511955416983, 336.7415543869066, 454.0966541014372, 307.41706546556566, 621.3171616445462, 1024.7376425007296, 551.0179272659826, 722.6277332017556, 1117.8103513189178, 572.3796955790533, 1165.597589436764, 457.1661851516394, 676.9528424016491, 519.2007574984889, 442.98819256300754, 579.8187937308272, 578.088978840387, 489.74104783048176, 554.9578690758188, 575.519583976074, 457.76187458042455, 668.6895612457299, 839.1540416079167, 36.86420723263128, 38.51947792169445, 6.246146985288284, 14.471097203798267, 7.928944377475652, 77.5217852886902, 5.1777610170390505, 68.48304098944352, 14.85097658970295, 4.342574268993021, 5.22785954334444, 5.0482636107554315, 10.014643512836438, 3.5212506706781457, 59.19273052411491, 3.460672592961367, 14.263241607776356, 6.723983613747624, 4.399452014594603, 4.484687178091353, 11.348164421902924, 3.518411299528398, 2.6905847947933887, 22.264044648337276, 2.5918170165553924, 2.5639236828039227, 8.43179172356702, 6.035005124110871, 19.090861146805693, 14.169154096836385, 19.273461227866537, 29.796690667802554, 72.4796944445699, 19.901429980193544, 59.042960965781496, 69.15146630495828, 105.10292632700323, 76.81970600386484, 124.94502552563783, 19.911186256573185, 18.416692990197905, 18.06443653698149, 158.83843415933788, 24.028542741899813, 668.6895612457299, 44.73465105660976, 962.1424971153967, 267.8959239996827, 166.62511955416983, 765.7627340788714, 195.57383896495296, 183.59969460847523, 333.88756439303154, 722.6277332017556, 457.76187458042455, 318.5385979735816, 328.1760347354457, 1303.3951176236164, 230.72191114767836, 912.7005824324594, 575.519583976074, 160.26972678393577, 554.9578690758188, 621.3171616445462, 1165.597589436764, 878.8388333341857, 572.3796955790533, 454.0966541014372, 327.0091396572587, 557.7258494493101, 1024.7376425007296, 551.0179272659826, 839.1540416079167, 376.76632063763043, 602.4864140424442, 1117.8103513189178, 585.3298843959316, 401.40365206669, 578.088978840387, 514.2120879198268, 676.9528424016491, 579.8187937308272, 588.0490558430073, 489.74104783048176, 519.2007574984889, 3.5430642369489056, 2.652177626113808, 2.6340809677403394, 2.638574752603229, 2.7128887915008244, 3.580528956988674, 1.747818401963063, 1.7503979908695289, 8.210053704830598, 1.7695259675303157, 11.981689517424291, 1.7637670675580555, 1.7516031421551277, 1.7651477590261795, 6.324202670106833, 7.36365757506808, 6.329653242121032, 8.62181300882089, 11.732118284083715, 1.7540402737417853, 1.7690293182125176, 25.014467658030092, 9.22846574342919, 1.7737132879955164, 6.716913098632605, 4.160053649762016, 1.7784292287896064, 3.6834280474121006, 1.7822320446676905, 1.8055211227300796, 9.780476946908738, 4.215477483639531, 38.51316002224543, 28.8070461446302, 72.25325645171145, 28.446088339647048, 20.45040850765584, 7.6664859097387925, 114.62782725372281, 293.2654171754332, 25.2469151068411, 43.44494290189807, 19.83858918787367, 45.532587309449404, 183.9038520746567, 26.714128336181876, 406.19545731704056, 1117.8103513189178, 1303.3951176236164, 160.26972678393577, 912.7005824324594, 123.89844568625611, 878.8388333341857, 361.6010413684848, 358.91729967097075, 575.519583976074, 839.1540416079167, 554.9578690758188, 676.9528424016491, 962.1424971153967, 293.23234994206854, 1024.7376425007296, 765.7627340788714, 268.6871094235534, 457.76187458042455, 318.19509939369885, 579.8187937308272, 361.6734958208216, 722.6277332017556, 391.5576813278483, 1165.597589436764, 452.6390309831054, 557.7258494493101, 585.3298843959316, 588.0490558430073, 621.3171616445462, 514.2120879198268, 668.6895612457299, 519.2007574984889, 602.4864140424442, 572.3796955790533, 497.1262942547666, 4.796219305473413, 3.2154890724996705, 9.309490597889285, 10.000508232763547, 18.33410138361448, 19.901429980193544, 8.912294447775103, 6.9741499119133366, 3.195391227457849, 6.3635720485839204, 3.9542685163028866, 3.2360704336663875, 2.4312511562947035, 2.432193142196592, 2.436840614480099, 78.6026507850679, 3.2082244986716004, 2.414357403229145, 4.12300564338242, 77.5217852886902, 5.860180716190667, 19.090861146805693, 5.6727830468387115, 43.28669752781137, 4.477815214099862, 4.258031928974211, 3.278569327866805, 6.7466532827772845, 33.289579273566204, 7.928944377475652, 65.71765200695364, 10.1260834650817, 18.992135880063, 85.65156004997942, 22.264044648337276, 22.896922216054648, 28.766690109220058, 25.104316034635968, 29.796690667802554, 192.90450253869588, 16.325171366395683, 36.72260739613922, 13.122676505127963, 75.799657318026, 25.098157980231637, 442.98819256300754, 189.7238277149612, 551.0179272659826, 303.75422259740026, 676.9528424016491, 272.42020045203594, 1117.8103513189178, 327.0091396572587, 248.15760544474537, 199.8200914348028, 578.088978840387, 183.9038520746567, 722.6277332017556, 839.1540416079167, 1165.597589436764, 413.68597446643406, 554.9578690758188, 668.6895612457299, 208.72624262248743, 293.23234994206854, 575.519583976074, 1303.3951176236164, 557.7258494493101, 912.7005824324594, 588.0490558430073, 878.8388333341857, 489.74104783048176, 765.7627340788714, 579.8187937308272, 1024.7376425007296, 572.3796955790533, 602.4864140424442, 962.1424971153967, 519.2007574984889, 621.3171616445462, 391.5576813278483, 514.2120879198268, 6.555008076408797, 15.85289981006983, 19.25540955221434, 4.408384412324874, 7.838795786583848, 5.408105861942709, 17.708770129732518, 2.742264314220402, 5.385115645613517, 14.842175445189104, 2.6821337727741312, 3.653464361901476, 2.6973231766904178, 4.576010185015275, 2.7011062720956454, 4.50629561025586, 2.72706989781329, 1.7930549570712913, 2.658400842273506, 2.7263823992192364, 6.512232323208042, 12.23041503600217, 2.735312529305029, 2.7238440830928563, 1.8081323013400343, 6.279427854105684, 6.325430235732494, 8.26565520906021, 2.690059093353303, 124.8570671217218, 18.367668117459477, 16.53957882418957, 16.421262202280523, 8.268914921215545, 42.28695247543344, 36.878663551945266, 27.2877609248566, 26.69687778451071, 32.67558792451042, 22.39303561211495, 47.75645777400983, 68.14776424342645, 89.13970168598524, 146.92758163697692, 17.147892365925497, 199.5417897230486, 22.345729424212667, 41.98368520906584, 108.16779881065196, 962.1424971153967, 72.59533088902427, 241.87765216987162, 106.72372879904557, 65.6525417610537, 1165.597589436764, 322.2230493059698, 282.5414471458666, 237.88815884646232, 174.15448308189966, 519.2007574984889, 878.8388333341857, 572.3796955790533, 413.2115829534304, 497.1262942547666, 329.9479591597707, 182.03765714014105, 333.39420162815924, 579.8187937308272, 1117.8103513189178, 457.76187458042455, 1303.3951176236164, 912.7005824324594, 668.6895612457299, 765.7627340788714, 372.1187889657468, 345.73732892966024, 621.3171616445462, 263.0058466334931, 551.0179272659826, 489.74104783048176, 578.088978840387, 1024.7376425007296, 514.2120879198268, 722.6277332017556, 676.9528424016491, 452.6390309831054, 454.0966541014372, 557.7258494493101, 575.519583976074, 602.4864140424442, 839.1540416079167, 585.3298843959316, 588.0490558430073, 5.9689227161758005, 5.074628247386245, 9.386194052899086, 8.727203534202365, 12.856379566360996, 28.8070461446302, 4.294050948524735, 6.766068237056587, 13.917054313139516, 8.658610956925084, 12.283408799927091, 7.307726172839918, 6.1445300650681745, 5.115449386286846, 10.457467276652881, 5.278275800437719, 2.56428722786351, 21.654738641733136, 2.5923589788515007, 2.5090328047151877, 2.529397321027937, 2.5947546431938373, 21.55930672170217, 3.344834500570423, 7.36365757506808, 5.268993626043109, 5.181386990411519, 4.430610239207083, 2.567643556635854, 2.5848654126364727, 12.205365327739354, 40.25951611648686, 37.03130597508868, 7.069617771924318, 67.71795412956048, 10.772363835959345, 49.36072365907128, 12.415994455085254, 109.17935143230888, 97.97189504648165, 182.24450508529378, 69.0032712588577, 46.24525709100078, 398.8770279315848, 114.3676965644171, 33.674764110609814, 585.3298843959316, 79.67423442087585, 28.007303288550798, 352.0339242502556, 303.16885144441153, 1117.8103513189178, 41.898947017120435, 157.2343627276432, 327.0091396572587, 722.6277332017556, 554.9578690758188, 141.38016107262507, 318.5385979735816, 1165.597589436764, 146.92758163697692, 457.76187458042455, 413.68597446643406, 912.7005824324594, 333.88756439303154, 230.51656995645027, 1024.7376425007296, 391.5576813278483, 514.2120879198268, 579.8187937308272, 551.0179272659826, 962.1424971153967, 1303.3951176236164, 839.1540416079167, 209.73940981760373, 318.49409213674124, 621.3171616445462, 878.8388333341857, 668.6895612457299, 519.2007574984889, 370.9716376167183, 557.7258494493101, 578.088978840387, 676.9528424016491, 588.0490558430073, 602.4864140424442, 765.7627340788714, 3.361444804448955, 10.70726135477159, 91.13496170765859, 2.5223383739002982, 6.735982619363822, 5.562752814553263, 74.71412892041089, 1.6718760553047003, 4.91076477570916, 10.285959613793553, 6.552027025456433, 15.24576820061601, 2.5407503715012227, 4.235837326727657, 4.230535784561201, 1.701555315701958, 2.465545296058082, 2.5436321109090505, 7.479314082937253, 9.309490597889285, 2.5581749051227365, 3.2445416307099064, 17.34403607466615, 1.6675302765685522, 4.3375893170349205, 20.112862798266224, 8.43179172356702, 1.6557370198398123, 1.6951554052790718, 2.552249671561666, 4.202697465677657, 4.28293854001416, 36.72260739613922, 61.96000463110399, 18.992135880063, 7.525167327762231, 20.03591775147411, 53.293266863004206, 39.74026179313297, 14.086646040304077, 41.46534154264735, 25.104316034635968, 42.270971070500714, 59.33731772989969, 72.4796944445699, 150.24190100667641, 163.06230718649246, 34.31486318507369, 554.9578690758188, 51.27026052872733, 1117.8103513189178, 272.42020045203594, 328.1760347354457, 345.73732892966024, 253.10695558524714, 314.41783289559896, 251.80468561936692, 272.9647962863298, 401.40365206669, 722.6277332017556, 578.088978840387, 358.91729967097075, 676.9528424016491, 575.519583976074, 274.23638733765756, 333.39420162815924, 551.0179272659826, 406.19545731704056, 878.8388333341857, 497.1262942547666, 332.8806563023346, 1303.3951176236164, 572.3796955790533, 243.77555538175545, 912.7005824324594, 962.1424971153967, 839.1540416079167, 1165.597589436764, 557.7258494493101, 668.6895612457299, 457.76187458042455, 1024.7376425007296, 579.8187937308272, 765.7627340788714, 621.3171616445462, 602.4864140424442, 450.7193152601116, 514.2120879198268], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.1867, -8.7867, -9.8495, -10.0401, -9.3208, -8.2029, -10.3052, -8.6021, -10.309, -7.5577, -10.6334, -10.0689, -10.3639, -9.6245, -10.1365, -7.7492, -9.9239, -8.5429, -9.6943, -9.9535, -9.3503, -9.9216, -9.8402, -9.6181, -10.1346, -10.3654, -9.1121, -8.4205, -11.0851, -9.0668, -8.4148, -8.5319, -9.1361, -6.3747, -7.5132, -5.8201, -8.3992, -7.1784, -6.8322, -6.9866, -6.4966, -8.5336, -5.1205, -5.634, -6.265, -7.7536, -7.3367, -7.5366, -6.9684, -7.8462, -5.3741, -7.0311, -5.1649, -6.3043, -6.371, -6.2056, -6.1709, -5.1667, -5.5389, -6.4819, -6.6175, -5.4688, -6.1599, -6.0326, -6.2125, -5.7241, -5.1853, -6.2589, -6.2005, -5.5071, -6.0484, -5.578, -5.9308, -5.7679, -5.9342, -5.9435, -5.9494, -6.0354, -5.9505, -6.0345, -6.0457, -6.1193, -6.1253, -6.2118, -8.7675, -8.2112, -9.3912, -10.3055, -7.6209, -8.2438, -9.9459, -10.3511, -9.5414, -9.3796, -10.1188, -9.3703, -10.4451, -8.7636, -7.7869, -10.4248, -9.9648, -10.3999, -10.6943, -10.1928, -9.4326, -10.7046, -10.0468, -10.1924, -9.4839, -9.4153, -9.747, -8.6068, -7.6458, -11.1416, -9.4043, -8.1789, -8.395, -8.4738, -9.0137, -7.9969, -6.5733, -7.2822, -9.1034, -5.8849, -6.7, -8.1767, -8.0805, -6.888, -5.6668, -8.0467, -6.1342, -5.9331, -5.7587, -5.7256, -5.0989, -6.9457, -6.3103, -5.9001, -6.8684, -7.4319, -5.8352, -7.1406, -6.3261, -5.0994, -5.4139, -5.3282, -6.5004, -6.1829, -6.0604, -6.3404, -6.2556, -5.6004, -5.9953, -6.3391, -5.9222, -5.4232, -5.5469, -5.9847, -6.2205, -5.9452, -6.0906, -5.8774, -5.8889, -5.9519, -6.2308, -6.058, -5.9275, -6.0353, -6.0358, -6.1694, -7.9074, -7.7894, -9.3303, -8.6969, -10.0214, -9.8872, -8.3816, -9.3688, -8.2767, -9.1216, -8.2689, -8.0169, -8.7178, -10.1236, -9.4769, -9.2579, -9.4647, -9.496, -8.9484, -7.9249, -7.4085, -9.7444, -10.162, -10.0278, -9.5658, -10.4583, -8.4177, -7.6718, -9.9597, -7.3987, -8.455, -8.0632, -9.2745, -5.6494, -8.9564, -8.9534, -5.3422, -6.8075, -7.6068, -7.3948, -7.3617, -8.2319, -7.4924, -7.535, -8.1773, -8.4516, -6.4715, -6.911, -5.7288, -7.3284, -5.5212, -5.7677, -7.0935, -7.2323, -6.356, -6.261, -5.5832, -6.0317, -6.916, -5.1742, -5.8995, -5.5112, -5.7759, -5.5476, -6.6273, -5.8744, -5.8863, -5.5382, -5.4124, -5.4487, -5.9452, -6.2202, -6.2787, -5.9423, -6.1323, -6.1277, -6.1358, -5.9925, -6.4464, -5.9881, -5.6809, -5.8602, -5.782, -5.9425, -6.0159, -6.138, -9.7216, -8.4251, -8.9624, -8.5612, -9.8062, -9.4108, -9.0845, -8.6446, -8.7948, -7.9149, -6.5418, -8.8753, -10.5886, -9.9571, -10.1313, -11.0169, -8.9172, -7.3488, -10.3684, -10.6794, -7.6716, -9.4881, -8.3347, -9.765, -10.0767, -11.028, -11.0344, -9.3012, -8.6308, -9.1421, -8.4598, -8.939, -8.6208, -8.4491, -8.1475, -6.643, -8.3715, -8.5177, -6.1177, -7.6659, -7.5881, -7.2787, -7.666, -7.0142, -6.5781, -6.279, -7.0027, -6.6841, -5.3551, -6.0141, -6.2161, -6.8296, -6.4261, -5.5182, -6.4858, -6.2124, -5.4025, -6.2884, -5.1328, -5.8373, -5.4028, -5.8098, -6.8474, -6.2924, -6.0719, -6.3759, -5.8387, -5.4591, -5.9359, -5.7664, -5.4852, -5.9699, -5.4954, -6.1381, -5.9018, -6.0694, -6.162, -6.0418, -6.065, -6.1447, -6.0977, -6.1172, -6.1906, -6.1073, -6.1492, -7.7698, -7.7713, -9.6131, -8.7945, -9.4204, -7.1491, -9.8591, -7.2985, -8.8409, -10.0754, -9.897, -9.9324, -9.2493, -10.2984, -7.4881, -10.3354, -8.9228, -9.6763, -10.1017, -10.0943, -9.1703, -10.3431, -10.6136, -8.5026, -10.6572, -10.6682, -9.4813, -9.8224, -8.671, -8.9707, -8.681, -8.2614, -7.3984, -8.6607, -7.6084, -7.4595, -7.066, -7.3724, -6.9344, -8.6657, -8.7394, -8.7726, -6.8109, -8.5171, -5.5303, -7.9594, -5.2545, -6.3935, -6.8151, -5.52, -6.7015, -6.7566, -6.2601, -5.6098, -5.9958, -6.3023, -6.2806, -5.1234, -6.5902, -5.4515, -5.8509, -6.8984, -5.8946, -5.81, -5.3281, -5.5481, -5.883, -6.0712, -6.3298, -5.959, -5.538, -5.9783, -5.697, -6.273, -5.9773, -5.6381, -6.0281, -6.2655, -6.0682, -6.1346, -6.0034, -6.1694, -6.2216, -6.2477, -6.2536, -9.8345, -10.1308, -10.225, -10.2247, -10.243, -10.0004, -10.7213, -10.7206, -9.2273, -10.7672, -8.865, -10.781, -10.7951, -10.797, -9.5348, -9.3855, -9.5444, -9.236, -8.9293, -10.8371, -10.8302, -8.1822, -9.1803, -10.8309, -9.5073, -9.9931, -10.8487, -10.1216, -10.8535, -10.8448, -9.1641, -9.9993, -7.8426, -8.1532, -7.274, -8.1855, -8.5019, -9.4319, -6.9221, -6.0673, -8.3499, -7.8645, -8.5848, -7.8426, -6.6588, -8.3419, -5.9782, -5.1124, -4.9819, -6.7948, -5.3464, -7.0242, -5.3924, -6.1519, -6.1587, -5.7786, -5.489, -5.8305, -5.6996, -5.4226, -6.39, -5.4238, -5.6817, -6.5039, -6.1001, -6.3792, -5.948, -6.3028, -5.8005, -6.25, -5.5097, -6.172, -6.0441, -6.0197, -6.0182, -6.0001, -6.1188, -5.9831, -6.1241, -6.1006, -6.1475, -6.1981, -9.6528, -10.0993, -9.0642, -8.9927, -8.3928, -8.3145, -9.1204, -9.3672, -10.1498, -9.4624, -9.9526, -10.1837, -10.4727, -10.4921, -10.5057, -7.037, -10.2464, -10.5355, -10.0044, -7.0776, -9.6615, -8.4829, -9.6981, -7.6688, -9.9415, -9.9931, -10.2562, -9.5389, -7.9476, -9.3837, -7.2959, -9.1547, -8.5541, -7.1213, -8.4115, -8.3955, -8.1914, -8.3316, -8.1901, -6.511, -8.7386, -8.0153, -8.9378, -7.4051, -8.3776, -5.9279, -6.6578, -5.7619, -6.2687, -5.6009, -6.3669, -5.2371, -6.2539, -6.4965, -6.677, -5.8425, -6.7573, -5.6915, -5.5761, -5.3341, -6.1495, -5.9244, -5.8148, -6.6906, -6.4378, -5.9476, -5.3789, -5.9866, -5.6465, -5.9632, -5.691, -6.1046, -5.8236, -6.0067, -5.6588, -6.0178, -5.9966, -5.7328, -6.0994, -6.0263, -6.2624, -6.2156, -9.2842, -8.4785, -8.3543, -9.8331, -9.2651, -9.6711, -8.4976, -10.3633, -9.6973, -8.6922, -10.4034, -10.0953, -10.4002, -9.8788, -10.4065, -9.8947, -10.4034, -10.8236, -10.4406, -10.4163, -9.5467, -8.9197, -10.4199, -10.4279, -10.8425, -9.599, -9.5975, -9.3311, -10.4552, -6.6187, -8.5424, -8.6737, -8.6918, -9.3582, -7.8113, -7.9431, -8.2322, -8.2635, -8.0847, -8.4381, -7.7532, -7.4312, -7.2118, -6.7958, -8.7077, -6.5376, -8.4787, -7.9244, -7.1076, -5.2388, -7.4613, -6.4381, -7.1418, -7.5607, -5.1672, -6.2366, -6.3589, -6.5076, -6.7614, -5.881, -5.456, -5.8033, -6.0673, -5.9404, -6.2643, -6.7408, -6.2743, -5.8578, -5.3675, -6.0475, -5.3097, -5.5792, -5.8172, -5.7351, -6.2494, -6.3183, -5.9421, -6.5183, -6.0375, -6.1156, -6.0235, -5.6756, -6.1253, -5.9694, -6.0177, -6.2059, -6.2071, -6.1474, -6.1452, -6.1434, -6.1047, -6.2094, -6.2116, -9.4359, -9.6073, -9.0856, -9.1822, -8.8133, -8.0088, -9.9386, -9.4859, -8.7763, -9.2521, -8.9057, -9.4343, -9.6144, -9.7989, -9.0879, -9.7739, -10.498, -8.3654, -10.4916, -10.5248, -10.5191, -10.4967, -8.3829, -10.2488, -9.46, -9.796, -9.8143, -9.975, -10.5215, -10.5167, -8.9736, -7.8027, -7.9158, -9.5304, -7.3662, -9.1301, -7.7061, -9.0069, -6.987, -7.0911, -6.5234, -7.4246, -7.7939, -5.8135, -7.0155, -8.106, -5.5702, -7.3418, -8.276, -6.0632, -6.2117, -5.119, -7.9253, -6.8167, -6.2269, -5.5783, -5.86, -6.9708, -6.3439, -5.3336, -6.9501, -6.0752, -6.1611, -5.5696, -6.3423, -6.6188, -5.5516, -6.2459, -6.0534, -5.9814, -6.04, -5.6628, -5.4682, -5.7789, -6.7006, -6.4347, -6.0152, -5.8235, -5.9885, -6.1533, -6.3689, -6.1654, -6.1696, -6.1027, -6.2074, -6.2167, -6.1857, -10.1091, -8.9734, -6.8413, -10.4451, -9.4769, -9.6803, -7.1013, -10.9108, -9.8346, -9.1071, -9.5643, -8.7268, -10.5248, -10.0157, -10.0198, -10.9307, -10.5617, -10.5344, -9.4579, -9.245, -10.5392, -10.3027, -8.6264, -10.9688, -10.0158, -8.4823, -9.3528, -10.9848, -10.9613, -10.5539, -10.0607, -10.0432, -7.9201, -7.4411, -8.5976, -9.4985, -8.5635, -7.6473, -7.9382, -8.9144, -7.9153, -8.3865, -7.9022, -7.6188, -7.4425, -6.7899, -6.7279, -8.1472, -5.7269, -7.7979, -5.1918, -6.4075, -6.2581, -6.2224, -6.482, -6.3057, -6.4995, -6.4452, -6.1385, -5.6685, -5.8615, -6.2614, -5.7651, -5.8946, -6.4765, -6.3269, -5.9561, -6.194, -5.6263, -6.0643, -6.3662, -5.4018, -5.9835, -6.5944, -5.6871, -5.6864, -5.7808, -5.5873, -6.0503, -5.9542, -6.1767, -5.7925, -6.1028, -6.0266, -6.1246, -6.1576, -6.2579, -6.2575], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8132, 0.7159, 0.6844, 0.6762, 0.6397, 0.6349, 0.6337, 0.6288, 0.626, 0.6228, 0.6181, 0.6148, 0.6145, 0.6133, 0.6093, 0.598, 0.5977, 0.5938, 0.5914, 0.5885, 0.5862, 0.5831, 0.5823, 0.5806, 0.5689, 0.5679, 0.5669, 0.5642, 0.5609, 0.5572, 0.5504, 0.5505, 0.5548, 0.4822, 0.4899, 0.412, 0.5236, 0.4499, 0.4297, 0.4365, 0.3919, 0.5175, 0.2903, 0.308, 0.3518, 0.4629, 0.4299, 0.442, 0.3891, 0.4636, 0.2365, 0.3754, 0.159, 0.288, 0.2909, 0.2692, 0.2529, 0.1153, 0.1633, 0.2695, 0.2851, 0.0956, 0.2101, 0.1854, 0.2143, 0.1136, -0.015, 0.1933, 0.1724, -0.0332, 0.1007, -0.0514, 0.0624, -0.0078, 0.0461, 0.0273, -0.0383, -0.0163, -0.1251, -0.0512, -0.058, -0.0189, -0.1591, -0.1806, 0.7442, 0.7208, 0.6903, 0.6748, 0.6728, 0.6644, 0.6487, 0.6402, 0.6397, 0.6267, 0.6257, 0.624, 0.6138, 0.6118, 0.6097, 0.6056, 0.5993, 0.5985, 0.5961, 0.5931, 0.593, 0.5844, 0.5818, 0.5779, 0.5759, 0.5708, 0.5691, 0.5681, 0.5628, 0.5627, 0.5544, 0.5285, 0.5316, 0.5318, 0.5451, 0.5091, 0.4357, 0.4477, 0.5409, 0.3645, 0.4048, 0.4888, 0.4811, 0.3917, 0.2994, 0.4748, 0.3257, 0.2948, 0.2726, 0.2576, 0.1831, 0.3533, 0.2825, 0.234, 0.3325, 0.4008, 0.1889, 0.3634, 0.2517, 0.0709, 0.1127, 0.0826, 0.252, 0.1899, 0.1643, 0.1948, 0.1713, 0.0102, 0.0954, 0.1845, 0.0581, -0.0993, -0.0731, 0.0345, 0.1156, -0.0032, 0.0586, -0.052, -0.0512, -0.0408, 0.1173, -0.0702, -0.2253, -0.2751, -0.4714, -0.1762, 1.0419, 1.0222, 0.9664, 0.9543, 0.9337, 0.8977, 0.8906, 0.8897, 0.8881, 0.8745, 0.8709, 0.8709, 0.8644, 0.8622, 0.858, 0.8506, 0.8388, 0.8368, 0.8336, 0.8291, 0.8285, 0.8256, 0.824, 0.824, 0.8214, 0.8206, 0.8175, 0.815, 0.8113, 0.8104, 0.8029, 0.7946, 0.8064, 0.7049, 0.7995, 0.7953, 0.6286, 0.6925, 0.731, 0.717, 0.7113, 0.7509, 0.676, 0.6725, 0.732, 0.7624, 0.5256, 0.5729, 0.4054, 0.5994, 0.3042, 0.3327, 0.5565, 0.5735, 0.3913, 0.3677, 0.1769, 0.2862, 0.4897, -0.0039, 0.1912, 0.0532, 0.1352, 0.0631, 0.3788, 0.1133, 0.0799, -0.0643, -0.1304, -0.1248, 0.0789, 0.1912, 0.2156, 0.0509, 0.1172, 0.1002, 0.0822, -0.0092, 0.2643, -0.0461, -0.27, -0.158, -0.2554, -0.1048, -0.0356, -0.1188, 0.8679, 0.8221, 0.799, 0.798, 0.7711, 0.7635, 0.7634, 0.7617, 0.7478, 0.7439, 0.7436, 0.7374, 0.7232, 0.7202, 0.7147, 0.7113, 0.7094, 0.7093, 0.7084, 0.7058, 0.7013, 0.7001, 0.699, 0.697, 0.6962, 0.6939, 0.6931, 0.6905, 0.6892, 0.6842, 0.6834, 0.6797, 0.6758, 0.6688, 0.6577, 0.6231, 0.6562, 0.6594, 0.5398, 0.6099, 0.5957, 0.5738, 0.5938, 0.5383, 0.4689, 0.4317, 0.4923, 0.4133, 0.2093, 0.305, 0.3161, 0.4138, 0.3447, 0.1839, 0.3344, 0.285, 0.1241, 0.2919, 0.0375, 0.1819, 0.0711, 0.1564, 0.3798, 0.2313, 0.1528, 0.2389, 0.0725, -0.0483, 0.0953, -0.0062, -0.1613, 0.0233, -0.2134, 0.0798, -0.0764, 0.0213, 0.0874, -0.0615, -0.0818, 0.0044, -0.0736, -0.1295, 0.026, -0.2696, -0.5385, 0.966, 0.9205, 0.8979, 0.8764, 0.8521, 0.8434, 0.8396, 0.818, 0.804, 0.7992, 0.792, 0.7915, 0.7897, 0.7858, 0.7742, 0.7662, 0.7625, 0.7611, 0.7598, 0.7481, 0.7436, 0.7419, 0.7397, 0.7375, 0.7334, 0.7333, 0.7297, 0.723, 0.7228, 0.7213, 0.7033, 0.6872, 0.6613, 0.6915, 0.6563, 0.6472, 0.622, 0.6291, 0.5808, 0.6861, 0.6904, 0.6764, 0.4642, 0.6467, 0.3074, 0.5829, 0.2194, 0.3589, 0.4122, 0.1821, 0.3656, 0.3737, 0.2721, 0.1503, 0.2209, 0.277, 0.2688, 0.0468, 0.3116, 0.0751, 0.1369, 0.3678, 0.1295, 0.1012, -0.0461, 0.0163, 0.1102, 0.1535, 0.2232, 0.0601, -0.1272, 0.0529, -0.0864, 0.1384, -0.0354, -0.3142, -0.0573, 0.0826, -0.085, -0.0342, -0.178, -0.1891, -0.2554, -0.0986, -0.1629, 1.2435, 1.2368, 1.1494, 1.1481, 1.102, 1.0671, 1.0634, 1.0626, 1.0103, 1.0051, 0.9947, 0.9946, 0.9874, 0.9777, 0.9638, 0.961, 0.9533, 0.9528, 0.9514, 0.944, 0.9423, 0.9414, 0.9404, 0.939, 0.9311, 0.9244, 0.9186, 0.9176, 0.9116, 0.9073, 0.8985, 0.905, 0.8494, 0.8291, 0.7888, 0.8095, 0.8231, 0.8742, 0.6793, 0.5946, 0.7644, 0.707, 0.7706, 0.682, 0.4698, 0.7159, 0.358, 0.2115, 0.1884, 0.4713, 0.1802, 0.4993, 0.1721, 0.3005, 0.3012, 0.2091, 0.1216, 0.1936, 0.1258, 0.0512, 0.2721, -0.013, 0.0205, 0.2455, 0.1165, 0.2011, 0.0323, 0.1495, -0.0404, 0.1229, -0.2277, 0.0559, -0.0249, -0.0488, -0.052, -0.089, -0.0185, -0.1454, -0.0334, -0.1587, -0.1543, -0.064, 1.1224, 1.0757, 1.0477, 1.0477, 1.0415, 1.0377, 1.0352, 1.0336, 1.0315, 1.03, 1.0156, 0.9849, 0.9819, 0.9621, 0.9466, 0.9416, 0.9309, 0.926, 0.922, 0.9148, 0.9133, 0.9109, 0.9092, 0.9064, 0.9024, 0.9011, 0.8994, 0.895, 0.8902, 0.8888, 0.8617, 0.8732, 0.8449, 0.7714, 0.8285, 0.8165, 0.7924, 0.7884, 0.7585, 0.5698, 0.8117, 0.7243, 0.8309, 0.6098, 0.7426, 0.3215, 0.4397, 0.2693, 0.3581, 0.2246, 0.3687, 0.0868, 0.2991, 0.3325, 0.3686, 0.1408, 0.3712, 0.0686, 0.0345, -0.0521, 0.1684, 0.0997, 0.0229, 0.3113, 0.2243, 0.0402, -0.2086, 0.0326, -0.1199, 0.003, -0.1266, 0.0445, -0.1214, -0.0264, -0.2479, -0.0246, -0.0547, -0.2589, -0.0086, -0.1152, 0.1105, -0.1152, 1.1786, 1.1012, 1.0309, 1.0264, 1.0188, 0.9841, 0.9714, 0.9709, 0.9621, 0.9534, 0.953, 0.952, 0.9506, 0.9434, 0.9429, 0.9429, 0.9364, 0.9355, 0.9247, 0.9237, 0.9227, 0.9194, 0.9169, 0.9131, 0.9082, 0.9067, 0.9009, 0.8998, 0.8983, 0.8971, 0.89, 0.8636, 0.8526, 0.8723, 0.7872, 0.7923, 0.8043, 0.7949, 0.7717, 0.7962, 0.7237, 0.6901, 0.641, 0.5573, 0.7934, 0.5094, 0.7577, 0.6814, 0.5518, 0.2351, 0.5968, 0.4165, 0.531, 0.5979, 0.1149, 0.3312, 0.3403, 0.3636, 0.4217, 0.2097, 0.1084, 0.1899, 0.2517, 0.1937, 0.2797, 0.398, 0.2594, 0.1225, -0.0436, 0.1691, -0.1394, -0.0526, 0.0205, -0.0329, 0.1744, 0.1791, -0.0309, 0.2526, -0.0062, 0.0336, -0.0402, -0.2648, -0.0249, -0.2093, -0.1923, 0.022, 0.0176, -0.1282, -0.1575, -0.2015, -0.4941, -0.2386, -0.2454, 1.1206, 1.1115, 1.0182, 0.9943, 0.9759, 0.9736, 0.9471, 0.9452, 0.9336, 0.9323, 0.9291, 0.9198, 0.913, 0.9119, 0.9078, 0.9055, 0.9034, 0.9023, 0.8988, 0.8983, 0.896, 0.8929, 0.8893, 0.8868, 0.8865, 0.8852, 0.8837, 0.8794, 0.8785, 0.8766, 0.8675, 0.845, 0.8155, 0.8568, 0.7615, 0.836, 0.7378, 0.8171, 0.663, 0.6672, 0.6143, 0.6843, 0.7152, 0.5409, 0.5881, 0.7203, 0.4007, 0.6232, 0.7346, 0.416, 0.417, 0.2049, 0.6825, 0.4686, 0.3262, 0.1818, 0.1641, 0.4208, 0.2354, -0.0515, 0.403, 0.1414, 0.1568, -0.043, 0.1899, 0.2839, -0.1408, 0.127, 0.0469, -0.0011, -0.0087, -0.1889, -0.2979, -0.1683, 0.2965, 0.1447, -0.104, -0.2591, -0.1509, -0.0626, 0.058, -0.1463, -0.1864, -0.2773, -0.2412, -0.2747, -0.4835, 1.0216, 0.9987, 0.9893, 0.9727, 0.9586, 0.9466, 0.9281, 0.9182, 0.917, 0.9051, 0.8989, 0.8919, 0.8858, 0.8837, 0.8809, 0.8808, 0.8789, 0.875, 0.8729, 0.867, 0.8645, 0.8633, 0.8633, 0.8629, 0.8599, 0.8594, 0.8582, 0.854, 0.854, 0.8521, 0.8466, 0.8452, 0.8195, 0.7754, 0.8014, 0.8263, 0.782, 0.7199, 0.7224, 0.7834, 0.7028, 0.7335, 0.6967, 0.641, 0.6172, 0.5409, 0.521, 0.6602, 0.2972, 0.608, 0.1321, 0.3282, 0.2913, 0.2749, 0.3272, 0.2866, 0.3149, 0.2885, 0.2095, 0.0916, 0.1218, 0.1985, 0.0603, 0.0931, 0.2525, 0.2068, 0.0752, 0.1421, -0.0619, 0.0699, 0.169, -0.2315, 0.0097, 0.2523, -0.1605, -0.2126, -0.1702, -0.3053, -0.0311, -0.1165, 0.0399, -0.3817, -0.1225, -0.3245, -0.2134, -0.2156, -0.0257, -0.1572]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 5, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 6, 8, 1, 3, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 5, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 7, 3, 5, 7, 1, 2, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 5, 8, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 1, 2, 3, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 7, 1, 2, 3, 5, 6, 2, 1, 2, 3, 5, 6, 1, 2, 3, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 1, 2, 3, 8, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 3, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 1, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 1, 2, 3, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 3, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 5, 7, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 8, 1, 2, 3, 4, 5, 6, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 2, 3, 5, 3, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 1, 2, 3, 4, 5, 6, 8, 9], \"Freq\": [0.14915827896258543, 0.09943885264172361, 0.14915827896258543, 0.09943885264172361, 0.19887770528344723, 0.09943885264172361, 0.14915827896258543, 0.049719426320861806, 0.049719426320861806, 0.049719426320861806, 0.1853284316262354, 0.308880719377059, 0.061776143875411804, 0.061776143875411804, 0.12355228775082361, 0.12355228775082361, 0.061776143875411804, 0.061776143875411804, 0.061776143875411804, 0.1526542719661071, 0.08904832531356247, 0.10176951464407138, 0.21626021861865172, 0.1335724879703437, 0.12085129863983478, 0.06360594665254463, 0.082687730648308, 0.03180297332627231, 0.012721189330508923, 0.15179875415540536, 0.1011991694369369, 0.1011991694369369, 0.25299792359234224, 0.15179875415540536, 0.1011991694369369, 0.05059958471846845, 0.1011991694369369, 0.18130906630400942, 0.1655430605384434, 0.27590510089740566, 0.07883002882783019, 0.07883002882783019, 0.05518102017948113, 0.06306402306226415, 0.05518102017948113, 0.031532011531132074, 0.007883002882783019, 0.2314065175591463, 0.1429275549630021, 0.14973362900885936, 0.11570325877957315, 0.08167288855028693, 0.04083644427514346, 0.061254666412715195, 0.1088971847337159, 0.061254666412715195, 0.013612148091714488, 0.22607341175569587, 0.07535780391856528, 0.12559633986427549, 0.22607341175569587, 0.10047707189142038, 0.10047707189142038, 0.05023853594571019, 0.07535780391856528, 0.025119267972855096, 0.07057584335421088, 0.28230337341684353, 0.14115168670842176, 0.07057584335421088, 0.21172753006263262, 0.07057584335421088, 0.07057584335421088, 0.07057584335421088, 0.3514695223607138, 0.11715650745357127, 0.11715650745357127, 0.11715650745357127, 0.22561335667455623, 0.32230479524936606, 0.06446095904987321, 0.06446095904987321, 0.09669143857480982, 0.06446095904987321, 0.06446095904987321, 0.06446095904987321, 0.03223047952493661, 0.3048632397522366, 0.1524316198761183, 0.1524316198761183, 0.1524316198761183, 0.1524316198761183, 0.3025164560594145, 0.17663057595727105, 0.10636868938863285, 0.0985618131032286, 0.08587563913944671, 0.08392392006809564, 0.05367227446215419, 0.04879297678377654, 0.034155083748643573, 0.010734454892430838, 0.27372755024616474, 0.1465410117479468, 0.0967723662486441, 0.0967723662486441, 0.10506714049852789, 0.09953729099860537, 0.055298494999225206, 0.07188804349899276, 0.041473871249418905, 0.013824623749806302, 0.18490762302511185, 0.18490762302511185, 0.18490762302511185, 0.18490762302511185, 0.307060865918492, 0.16265926951357954, 0.1327830771539425, 0.07303069243466837, 0.09460794247218403, 0.07303069243466837, 0.06473175011254696, 0.051453442397152716, 0.02987619235963706, 0.011618519250969968, 0.2260322625202595, 0.13185215313681806, 0.14440950105461023, 0.08162276146564927, 0.12871281615736999, 0.08162276146564927, 0.07220475052730511, 0.06906541354785707, 0.05022939167116878, 0.012557347917792195, 0.4203329522918394, 0.14011098409727982, 0.06227154848767991, 0.12454309697535983, 0.04670366136575994, 0.04670366136575994, 0.06227154848767991, 0.04670366136575994, 0.04670366136575994, 0.015567887121919978, 0.1847170758409052, 0.0461792689602263, 0.2308963448011315, 0.0923585379204526, 0.1847170758409052, 0.1385378068806789, 0.0461792689602263, 0.0923585379204526, 0.1391510422262382, 0.0927673614841588, 0.32468576519455583, 0.0463836807420794, 0.1391510422262382, 0.0927673614841588, 0.0463836807420794, 0.0927673614841588, 0.15690790135373728, 0.31381580270747456, 0.06973684504610546, 0.0523026337845791, 0.13947369009221092, 0.1046052675691582, 0.0523026337845791, 0.06973684504610546, 0.03486842252305273, 0.28013365529797046, 0.14516016683622107, 0.14261349724260317, 0.10950679252557027, 0.07894675740215532, 0.06621340943406576, 0.06876007902768366, 0.06112007024682992, 0.03310670471703288, 0.01528001756170748, 0.20639359556047623, 0.10319679778023812, 0.09029719805770835, 0.05159839889011906, 0.23219279500553575, 0.05159839889011906, 0.16769479639288692, 0.02579919944505953, 0.05159839889011906, 0.02579919944505953, 0.23492541765935096, 0.1342430958053434, 0.10068232185400755, 0.0671215479026717, 0.2013646437080151, 0.0671215479026717, 0.1342430958053434, 0.03356077395133585, 0.03356077395133585, 0.03356077395133585, 0.11432192494463406, 0.11432192494463406, 0.11432192494463406, 0.2286438498892681, 0.11432192494463406, 0.11432192494463406, 0.11432192494463406, 0.11432192494463406, 0.18361828235450944, 0.1573870991610081, 0.18361828235450944, 0.07869354958050405, 0.12241218823633963, 0.11366846050517251, 0.043718638655835584, 0.043718638655835584, 0.06994982184933693, 0.017487455462334232, 0.18290070906029762, 0.1371755317952232, 0.32007624085552083, 0.045725177265074406, 0.09145035453014881, 0.045725177265074406, 0.045725177265074406, 0.045725177265074406, 0.09145035453014881, 0.17405137044489205, 0.3481027408897841, 0.17405137044489205, 0.17405137044489205, 0.2192528662065866, 0.14616857747105774, 0.1096264331032933, 0.2192528662065866, 0.07308428873552887, 0.07308428873552887, 0.07308428873552887, 0.036542144367764436, 0.036542144367764436, 0.20316239766634553, 0.16384064327931092, 0.12451888889227629, 0.1070425536091498, 0.12233434698188549, 0.09611984405719573, 0.048059922028597865, 0.07427442495328761, 0.045875380118207056, 0.015291793372735686, 0.13370210007376546, 0.13370210007376546, 0.13370210007376546, 0.13370210007376546, 0.13370210007376546, 0.13370210007376546, 0.13370210007376546, 0.22838020806083978, 0.163128720043457, 0.05437624001448566, 0.12506535203331703, 0.07612673602027993, 0.13594060003621417, 0.09787723202607419, 0.07068911201883137, 0.0326257440086914, 0.010875248002897133, 0.3702186805201681, 0.22129738428242254, 0.16922741151008783, 0.0650874659654184, 0.10413994554466943, 0.1822449047031715, 0.0650874659654184, 0.07810495915850207, 0.052069972772334716, 0.052069972772334716, 0.013017493193083679, 0.1813830951736462, 0.1813830951736462, 0.1813830951736462, 0.06046103172454874, 0.06046103172454874, 0.12092206344909748, 0.06046103172454874, 0.12092206344909748, 0.06046103172454874, 0.22617865208498952, 0.13570719125099373, 0.0904714608339958, 0.22617865208498952, 0.0904714608339958, 0.0904714608339958, 0.0452357304169979, 0.0452357304169979, 0.0452357304169979, 0.36464936801916464, 0.10773731327838955, 0.11602479891518874, 0.07873111354959236, 0.10359357045998994, 0.07458737073119276, 0.06629988509439357, 0.04558117100239558, 0.029006199728797186, 0.008287485636799196, 0.38363969203118836, 0.11509190760935652, 0.15345587681247536, 0.03836396920311884, 0.11509190760935652, 0.03836396920311884, 0.03836396920311884, 0.03836396920311884, 0.03836396920311884, 0.3805142388674225, 0.19025711943371126, 0.19025711943371126, 0.09852980308197093, 0.29558940924591276, 0.09852980308197093, 0.09852980308197093, 0.09852980308197093, 0.09852980308197093, 0.09852980308197093, 0.09852980308197093, 0.11993059540633264, 0.11993059540633264, 0.07995373027088842, 0.11993059540633264, 0.11993059540633264, 0.19988432567722106, 0.11993059540633264, 0.03997686513544421, 0.07995373027088842, 0.19781595359834916, 0.1065162827068034, 0.13694950633731864, 0.060866447261030514, 0.15216611815257627, 0.060866447261030514, 0.1673827299678339, 0.060866447261030514, 0.030433223630515257, 0.015216611815257628, 0.25737941377713697, 0.25737941377713697, 0.18978956342958733, 0.18978956342958733, 0.18978956342958733, 0.18978956342958733, 0.18978956342958733, 0.18978956342958733, 0.155987038236505, 0.07487377835352241, 0.11855014905974381, 0.19342392741326622, 0.1435080751775846, 0.1372685936481244, 0.056155333765141806, 0.08735274141244281, 0.0249579261178408, 0.0124789630589204, 0.16918940594211557, 0.09228513051388122, 0.2768553915416437, 0.10766598559952809, 0.07690427542823434, 0.07690427542823434, 0.06152342034258748, 0.09228513051388122, 0.04614256525694061, 0.01538085508564687, 0.3707379258969582, 0.22730103582960595, 0.22730103582960595, 0.05425316723560651, 0.21701266894242605, 0.16275950170681955, 0.08137975085340977, 0.2441392525602293, 0.08137975085340977, 0.05425316723560651, 0.05425316723560651, 0.027126583617803256, 0.027126583617803256, 0.2602012035859359, 0.13010060179296795, 0.2602012035859359, 0.13010060179296795, 0.13010060179296795, 0.18268997614467, 0.06089665871489, 0.12179331742978, 0.12179331742978, 0.12179331742978, 0.06089665871489, 0.12179331742978, 0.12179331742978, 0.06089665871489, 0.3858297069632763, 0.3858297069632763, 0.16983609831774743, 0.16983609831774743, 0.33967219663549486, 0.16983609831774743, 0.1781749674709554, 0.2375666232946072, 0.2375666232946072, 0.0593916558236518, 0.0890874837354777, 0.0593916558236518, 0.0296958279118259, 0.0296958279118259, 0.0890874837354777, 0.1936038080864512, 0.16133650673870933, 0.2904057121296768, 0.06453460269548374, 0.0968019040432256, 0.06453460269548374, 0.03226730134774187, 0.03226730134774187, 0.06453460269548374, 0.23168698022844297, 0.1843530810419869, 0.13951043970744953, 0.08221150911331847, 0.10463282978058715, 0.08719402481715595, 0.06228144629796854, 0.04982515703837483, 0.039860125630699864, 0.01743880496343119, 0.2768752418240248, 0.3900272097437759, 0.198710783727574, 0.12419423982973375, 0.12419423982973375, 0.1490330877956805, 0.099355391863787, 0.07451654389784025, 0.0496776959318935, 0.07451654389784025, 0.099355391863787, 0.02483884796594675, 0.37704865245593805, 0.37704865245593805, 0.050223024741676295, 0.25111512370838146, 0.15066907422502887, 0.10044604948335259, 0.20089209896670518, 0.050223024741676295, 0.050223024741676295, 0.050223024741676295, 0.050223024741676295, 0.07011028961710902, 0.21033086885132704, 0.14022057923421805, 0.07011028961710902, 0.21033086885132704, 0.07011028961710902, 0.07011028961710902, 0.07011028961710902, 0.17936057953365098, 0.1379696765643469, 0.06898483828217344, 0.12417270890791221, 0.19315754719008565, 0.11037574125147752, 0.09657877359504283, 0.04139090296930407, 0.02759393531286938, 0.02759393531286938, 0.3718279374388554, 0.07436558748777108, 0.03718279374388554, 0.14873117497554217, 0.11154838123165663, 0.07436558748777108, 0.03718279374388554, 0.03718279374388554, 0.03718279374388554, 0.39247757607306477, 0.09811939401826619, 0.0654129293455108, 0.1308258586910216, 0.09811939401826619, 0.09811939401826619, 0.0654129293455108, 0.0327064646727554, 0.0327064646727554, 0.4050272371412522, 0.08100544742825044, 0.04050272371412522, 0.12150817114237566, 0.12150817114237566, 0.08100544742825044, 0.04050272371412522, 0.04050272371412522, 0.04050272371412522, 0.13395604411895012, 0.13395604411895012, 0.26791208823790025, 0.13395604411895012, 0.13395604411895012, 0.13395604411895012, 0.2514376296240478, 0.1717135031578863, 0.11651987714285142, 0.1349177524811964, 0.0797241264661615, 0.0797241264661615, 0.042928375789471576, 0.06132625112781654, 0.04906100090225323, 0.024530500451126615, 0.2841263925302614, 0.2841263925302614, 0.09470879751008714, 0.09470879751008714, 0.09470879751008714, 0.09470879751008714, 0.23187306497365717, 0.101444465925975, 0.101444465925975, 0.13042859904768214, 0.14492066560853573, 0.07246033280426786, 0.05796826624341429, 0.05796826624341429, 0.07246033280426786, 0.014492066560853573, 0.1041335843247673, 0.1041335843247673, 0.31240075297430187, 0.1041335843247673, 0.1041335843247673, 0.1041335843247673, 0.1041335843247673, 0.1041335843247673, 0.19916893944059624, 0.11950136366435773, 0.11950136366435773, 0.0796675757762385, 0.11950136366435773, 0.0796675757762385, 0.159335151552477, 0.03983378788811925, 0.03983378788811925, 0.03983378788811925, 0.24508063664744575, 0.10892472739886479, 0.10892472739886479, 0.054462363699432394, 0.13615590924858098, 0.0816935455491486, 0.13615590924858098, 0.054462363699432394, 0.027231181849716197, 0.027231181849716197, 0.20555784693134752, 0.09673310443828119, 0.2539243991504881, 0.10882474249306634, 0.09673310443828119, 0.07254982832871089, 0.048366552219140595, 0.048366552219140595, 0.06045819027392574, 0.012091638054785149, 0.3909036860605205, 0.16144190956883123, 0.08072095478441561, 0.08072095478441561, 0.24216286435324685, 0.16144190956883123, 0.08072095478441561, 0.08072095478441561, 0.08072095478441561, 0.1768282042567333, 0.1556088197459253, 0.14146256340538663, 0.11317005072430932, 0.09902379438377065, 0.10609692255403998, 0.07073128170269331, 0.06365815353242399, 0.05658502536215466, 0.014146256340538664, 0.43960523674496615, 0.1465350789149887, 0.1465350789149887, 0.1465350789149887, 0.1595356610960592, 0.2734897047361015, 0.1595356610960592, 0.09116323491203383, 0.11395404364004229, 0.06837242618402538, 0.06837242618402538, 0.04558161745601692, 0.02279080872800846, 0.22125034008059508, 0.16198685613043567, 0.14223236148038254, 0.11457606897030816, 0.08691977646023377, 0.07901797860021252, 0.07111618074019127, 0.06716528181018065, 0.03160719144008501, 0.01975449465005313, 0.22050518871795838, 0.11025259435897919, 0.09521814967366385, 0.1653788915384688, 0.10524111279720741, 0.10022963123543563, 0.045103334055946034, 0.10524111279720741, 0.03006888937063069, 0.015034444685315345, 0.256321537755565, 0.06408038443889125, 0.32040192219445623, 0.08010048054861406, 0.06408038443889125, 0.06408038443889125, 0.048060288329168435, 0.048060288329168435, 0.048060288329168435, 0.01602009610972281, 0.22805685062177075, 0.11402842531088538, 0.10202543317289745, 0.15003740172484917, 0.15003740172484917, 0.06601645675893364, 0.06601645675893364, 0.08402094496591554, 0.030007480344969834, 0.012002992137987934, 0.16764256387626403, 0.23212047305944247, 0.1354036092846748, 0.10961244561140339, 0.0967168637747677, 0.0967168637747677, 0.08382128193813201, 0.05158232734654277, 0.01934337275495354, 0.012895581836635693, 0.10419081101000922, 0.3125724330300277, 0.10419081101000922, 0.10419081101000922, 0.10419081101000922, 0.10419081101000922, 0.10419081101000922, 0.24529012230321073, 0.1635267482021405, 0.1635267482021405, 0.08176337410107025, 0.08176337410107025, 0.08176337410107025, 0.08176337410107025, 0.1635267482021405, 0.19808791241992177, 0.19808791241992177, 0.19808791241992177, 0.19808791241992177, 0.16753441911552927, 0.16753441911552927, 0.16753441911552927, 0.16753441911552927, 0.16753441911552927, 0.16753441911552927, 0.26757743482249435, 0.10703097392899774, 0.2461712400366948, 0.09632787653609796, 0.06421858435739865, 0.06421858435739865, 0.042812389571599094, 0.07492168175029842, 0.032109292178699324, 0.010703097392899773, 0.35622208310205167, 0.10177773802915761, 0.0763333035218682, 0.05088886901457881, 0.08905552077551292, 0.0763333035218682, 0.17811104155102583, 0.0381666517609341, 0.0381666517609341, 0.012722217253644702, 0.2648034164746695, 0.14761001256189346, 0.1225610407332085, 0.08856600753713607, 0.07156849093909987, 0.10466891799843354, 0.0742523093493161, 0.06083321729823488, 0.048308731383892405, 0.016997516598036215, 0.49137145607138233, 0.16379048535712745, 0.16379048535712745, 0.2237564012827552, 0.17900512102620417, 0.13425384076965313, 0.08950256051310208, 0.08950256051310208, 0.04475128025655104, 0.04475128025655104, 0.13425384076965313, 0.17802976164537757, 0.1311798243702782, 0.11243984946023845, 0.14054981182529808, 0.1311798243702782, 0.10306986200521859, 0.056219924730119226, 0.10306986200521859, 0.028109962365059613, 0.009369987455019872, 0.19443980679430087, 0.09721990339715043, 0.09721990339715043, 0.09721990339715043, 0.09721990339715043, 0.09721990339715043, 0.09721990339715043, 0.09721990339715043, 0.1655982775584976, 0.14194138076442653, 0.1892551743525687, 0.11828448397035543, 0.11828448397035543, 0.09462758717628435, 0.047313793588142174, 0.047313793588142174, 0.047313793588142174, 0.023656896794071087, 0.10741726300542451, 0.21483452601084901, 0.10741726300542451, 0.10741726300542451, 0.21483452601084901, 0.10741726300542451, 0.1939331681905884, 0.12248410622563478, 0.1939331681905884, 0.11227709737349854, 0.08165607081708985, 0.10207008852136232, 0.06124205311281739, 0.040828035408544926, 0.08165607081708985, 0.010207008852136231, 0.1838877981573705, 0.1838877981573705, 0.3064796635956175, 0.0612959327191235, 0.09194389907868525, 0.0612959327191235, 0.03064796635956175, 0.03064796635956175, 0.0612959327191235, 0.3116988852912446, 0.3116988852912446, 0.3116988852912446, 0.20041926837091725, 0.10931960092959123, 0.29151893581224325, 0.09109966744132603, 0.07287973395306081, 0.07287973395306081, 0.036439866976530406, 0.07287973395306081, 0.036439866976530406, 0.018219933488265203, 0.3796390514365454, 0.3796390514365454, 0.27928834315056855, 0.27928834315056855, 0.27928834315056855, 0.27123615881360497, 0.1596793515596223, 0.150929798049506, 0.11155680725398269, 0.07874598159104661, 0.06999642808093032, 0.05905948619328495, 0.04812254430563959, 0.03281082566293608, 0.013124330265174435, 0.33028186749766303, 0.15796089315105624, 0.10770060896662925, 0.07898044657552812, 0.07180040597775283, 0.0933405277710787, 0.043080243586651704, 0.06462036537997755, 0.03590020298887642, 0.014360081195550567, 0.3853928935527861, 0.13885491695043586, 0.1041411877128269, 0.06942745847521793, 0.1041411877128269, 0.1041411877128269, 0.2082823754256538, 0.1041411877128269, 0.034713729237608966, 0.1041411877128269, 0.2556559260240939, 0.11983871532379402, 0.10386021994728815, 0.14380645838855283, 0.08388710072665581, 0.10386021994728815, 0.07190322919427641, 0.07190322919427641, 0.03195699075301174, 0.01597849537650587, 0.186825613405008, 0.17912146439861593, 0.1675652408890278, 0.10593204883789115, 0.0828196018187149, 0.0828196018187149, 0.06741130380593073, 0.07704149006392083, 0.0365947077803624, 0.011556223509588124, 0.08734798420189674, 0.13102197630284512, 0.13102197630284512, 0.08734798420189674, 0.17469596840379348, 0.08734798420189674, 0.17469596840379348, 0.04367399210094837, 0.08734798420189674, 0.04367399210094837, 0.2516873345241155, 0.08389577817470517, 0.2796525939156839, 0.08389577817470517, 0.08389577817470517, 0.05593051878313678, 0.02796525939156839, 0.09787840787048936, 0.02796525939156839, 0.013982629695784196, 0.20150601394745865, 0.06716867131581955, 0.4030120278949173, 0.033584335657909775, 0.10075300697372933, 0.033584335657909775, 0.033584335657909775, 0.10075300697372933, 0.033584335657909775, 0.17316414328189914, 0.17316414328189914, 0.17316414328189914, 0.17316414328189914, 0.17316414328189914, 0.17536634931627582, 0.22797625411115854, 0.13444753447581145, 0.09352871963534709, 0.09352871963534709, 0.09352871963534709, 0.046764359817673545, 0.08183762968092871, 0.03507326986325516, 0.01753663493162758, 0.36861076028360895, 0.36861076028360895, 0.15355717523102466, 0.15355717523102466, 0.15355717523102466, 0.15355717523102466, 0.15355717523102466, 0.29749112604094385, 0.29749112604094385, 0.3357031883946939, 0.3564791960488389, 0.07921759912196419, 0.11882639868294628, 0.039608799560982094, 0.07921759912196419, 0.19804399780491047, 0.039608799560982094, 0.039608799560982094, 0.039608799560982094, 0.16274637594907834, 0.16274637594907834, 0.16274637594907834, 0.16274637594907834, 0.16274637594907834, 0.16274637594907834, 0.24162381924802498, 0.16108254616535, 0.080541273082675, 0.080541273082675, 0.16108254616535, 0.080541273082675, 0.080541273082675, 0.080541273082675, 0.080541273082675, 0.23866948245534367, 0.14320168947320622, 0.11933474122767183, 0.09546779298213748, 0.11933474122767183, 0.07160084473660311, 0.07160084473660311, 0.04773389649106874, 0.07160084473660311, 0.02386694824553437, 0.3833952524881627, 0.12779841749605422, 0.12779841749605422, 0.12779841749605422, 0.12779841749605422, 0.12779841749605422, 0.1967759158163499, 0.1311839438775666, 0.0655919719387833, 0.1967759158163499, 0.1311839438775666, 0.0655919719387833, 0.0655919719387833, 0.0655919719387833, 0.0655919719387833, 0.0655919719387833, 0.2574287704913542, 0.2574287704913542, 0.2574287704913542, 0.19313367239414597, 0.19313367239414597, 0.19313367239414597, 0.19313367239414597, 0.33497076137865694, 0.12305048377175153, 0.11621434578443199, 0.07519751786051482, 0.11621434578443199, 0.0683613798731953, 0.06152524188587576, 0.05468910389855623, 0.03418068993659765, 0.02050841396195859, 0.1960891854954791, 0.14902778097656413, 0.11765351129728747, 0.17255848323602163, 0.09412280903782998, 0.10196637645764914, 0.06274853935855332, 0.06274853935855332, 0.023530702259457494, 0.01568713483963833, 0.22684047180736322, 0.22684047180736322, 0.22684047180736322, 0.22684047180736322, 0.42057652246260047, 0.12016472070360013, 0.15020590087950017, 0.06008236035180006, 0.12016472070360013, 0.03004118017590003, 0.03004118017590003, 0.03004118017590003, 0.03004118017590003, 0.24911124060450188, 0.17356111025723492, 0.12659751571704192, 0.10413666615434095, 0.08780150283601296, 0.07963392117684896, 0.07146633951768497, 0.06534065327331197, 0.030628431221864982, 0.012251372488745993, 0.18738089036564262, 0.21414958898930583, 0.10707479449465292, 0.09369044518282131, 0.09369044518282131, 0.10707479449465292, 0.05353739724732646, 0.06692174655915807, 0.040153047935494846, 0.040153047935494846, 0.19168129282186505, 0.14975101001708208, 0.12878586861469057, 0.14376096961639878, 0.12878586861469057, 0.07487550500854104, 0.06289542420717446, 0.05990040400683283, 0.04792032320546626, 0.011980080801366566, 0.14537837143048601, 0.0872270228582916, 0.1163026971443888, 0.2035297200026804, 0.1163026971443888, 0.1163026971443888, 0.0872270228582916, 0.1163026971443888, 0.0290756742860972, 0.30448454616994486, 0.09688144650861881, 0.12456185979679561, 0.055360826576353606, 0.09688144650861881, 0.17992268637314923, 0.041520619932265204, 0.041520619932265204, 0.055360826576353606, 0.013840206644088401, 0.23185326904599957, 0.11592663452299978, 0.11592663452299978, 0.23185326904599957, 0.11592663452299978, 0.11592663452299978, 0.11592663452299978, 0.1954862465613936, 0.1954862465613936, 0.1954862465613936, 0.1954862465613936, 0.1954862465613936, 0.29932772300505617, 0.11279015649465884, 0.18653756651039732, 0.08676165884204527, 0.06940932707363622, 0.08676165884204527, 0.0477189123631249, 0.0477189123631249, 0.05205699530522716, 0.008676165884204527, 0.23749141660485457, 0.11214872450784798, 0.20120800573466843, 0.10885023261055833, 0.0923577731241101, 0.08246229743224116, 0.03958190276747576, 0.05607436225392399, 0.05937285415121364, 0.00989547569186894, 0.3499255084774151, 0.1955466076785555, 0.07204348703946782, 0.0823354137593918, 0.06175156031954384, 0.07204348703946782, 0.0823354137593918, 0.05145963359961987, 0.0411677068796959, 0.010291926719923974, 0.1668737412317368, 0.11442770827319095, 0.2336232377244315, 0.09058860238294283, 0.09535642356099246, 0.10012424473904208, 0.08105296002684359, 0.04767821178049623, 0.05244603295854585, 0.019071284712198492, 0.2664010546888125, 0.1305887522984375, 0.11883576459157812, 0.1253652022065, 0.11752987706859375, 0.08618857651696875, 0.06007082605728125, 0.06137671358026563, 0.024811862936703125, 0.010447100183875, 0.178626965512257, 0.13397022413419274, 0.178626965512257, 0.0893134827561285, 0.0893134827561285, 0.0893134827561285, 0.04465674137806425, 0.13397022413419274, 0.04465674137806425, 0.0998537789905585, 0.199707557981117, 0.199707557981117, 0.0998537789905585, 0.199707557981117, 0.0998537789905585, 0.0998537789905585, 0.16535825597510415, 0.17171818889722354, 0.11447879259814903, 0.12083872552026842, 0.10811885967602963, 0.09539899383179086, 0.0699592621433133, 0.0699592621433133, 0.0635993292211939, 0.012719865844238782, 0.1433866510801247, 0.1433866510801247, 0.1433866510801247, 0.1433866510801247, 0.1433866510801247, 0.1433866510801247, 0.18874159052505385, 0.2141491123265034, 0.1070745561632517, 0.11433384810652301, 0.10344491019161606, 0.06533362748944171, 0.08892632630507345, 0.06351880450362389, 0.038111282702174336, 0.01633340687236043, 0.3515421832555582, 0.07030843665111164, 0.14061687330222328, 0.03515421832555582, 0.07030843665111164, 0.1757710916277791, 0.07030843665111164, 0.03515421832555582, 0.03515421832555582, 0.3159730752217022, 0.1579865376108511, 0.1579865376108511, 0.1579865376108511, 0.1579865376108511, 0.21777396969611948, 0.1633304772720896, 0.1633304772720896, 0.10888698484805974, 0.05444349242402987, 0.05444349242402987, 0.05444349242402987, 0.1633304772720896, 0.05444349242402987, 0.19048946457909713, 0.09524473228954856, 0.09524473228954856, 0.19048946457909713, 0.09524473228954856, 0.09524473228954856, 0.09524473228954856, 0.09524473228954856, 0.1579601167001575, 0.10530674446677167, 0.1579601167001575, 0.052653372233385835, 0.1579601167001575, 0.052653372233385835, 0.1579601167001575, 0.052653372233385835, 0.052653372233385835, 0.052653372233385835, 0.1797671105183394, 0.1797671105183394, 0.1797671105183394, 0.1797671105183394, 0.1797671105183394, 0.3650736434042404, 0.14342178848023732, 0.11734509966564871, 0.0651917220364715, 0.0651917220364715, 0.0651917220364715, 0.0782300664437658, 0.052153377629177204, 0.026076688814588602, 0.013038344407294301, 0.11050342363909492, 0.05525171181954746, 0.33151027091728474, 0.11050342363909492, 0.08287756772932119, 0.08287756772932119, 0.05525171181954746, 0.11050342363909492, 0.02762585590977373, 0.21710770538221594, 0.2508800151083384, 0.09649231350320708, 0.06754461945224496, 0.1157907762038485, 0.10131692917836743, 0.07719385080256566, 0.03377230972612248, 0.03377230972612248, 0.009649231350320708, 0.21826288790860174, 0.15174467445074216, 0.12991838565988198, 0.11121013812485897, 0.12160360897764953, 0.0893838493339988, 0.05300670134923185, 0.0800297255664873, 0.03221975964365073, 0.01247216502334867, 0.21724082169407707, 0.17689609766517705, 0.12413761239661548, 0.1179307317767847, 0.096206649607377, 0.06517224650822312, 0.06206880619830774, 0.08689632867763084, 0.03724128371898464, 0.015517201549576936, 0.20773383132429957, 0.15580037349322468, 0.10386691566214978, 0.10386691566214978, 0.05193345783107489, 0.10386691566214978, 0.05193345783107489, 0.15580037349322468, 0.05193345783107489, 0.1950448446501824, 0.17554036018516417, 0.1560358757201459, 0.0975224223250912, 0.0975224223250912, 0.07801793786007295, 0.0975224223250912, 0.05851345339505472, 0.039008968930036476, 0.019504484465018238, 0.20427741508127725, 0.19826925581418087, 0.13818766314321695, 0.11115094644128322, 0.07510199083870488, 0.08111015010580126, 0.08111015010580126, 0.06308567230451209, 0.03004079633548195, 0.01802447780128917, 0.21075488604250692, 0.21075488604250692, 0.21075488604250692, 0.21371992096295625, 0.1511677489737983, 0.1563804299728948, 0.09209069765070473, 0.11120386131405853, 0.10425361998192988, 0.07123997365431875, 0.0538643703239971, 0.029538525661546796, 0.01563804299728948, 0.27744129981096577, 0.12610968173225717, 0.134516993847741, 0.10088774538580574, 0.0798694650970962, 0.0798694650970962, 0.0672584969238705, 0.09248043327032192, 0.02942559240419334, 0.008407312115483812, 0.16864748398668056, 0.25793144609727614, 0.12896572304863807, 0.0892839621105956, 0.07936352187608497, 0.0892839621105956, 0.0892839621105956, 0.04960220117255311, 0.029761320703531866, 0.019840880469021244, 0.20392127248029723, 0.30588190872044585, 0.10196063624014862, 0.10196063624014862, 0.10196063624014862, 0.10196063624014862, 0.20762772410549335, 0.4152554482109867, 0.20762772410549335, 0.3019496777369707, 0.13210298400992465, 0.09435927429280333, 0.20759040344416732, 0.018871854858560667, 0.056615564575681995, 0.056615564575681995, 0.07548741943424267, 0.056615564575681995, 0.018871854858560667, 0.2638534355912422, 0.1451193895751832, 0.1451193895751832, 0.07915603067737265, 0.07915603067737265, 0.07915603067737265, 0.1319267177956211, 0.03957801533868632, 0.03957801533868632, 0.013192671779562109, 0.3050110886783194, 0.378992484110369, 0.24196508920523874, 0.12098254460261937, 0.12098254460261937, 0.12098254460261937, 0.12098254460261937, 0.12098254460261937, 0.12098254460261937, 0.18923982589572583, 0.12615988393048388, 0.12615988393048388, 0.06307994196524194, 0.12615988393048388, 0.06307994196524194, 0.06307994196524194, 0.18923982589572583, 0.20169562060710705, 0.10860533417305764, 0.24824076382413174, 0.08533276256454529, 0.11636285804256176, 0.05430266708652882, 0.0465451432170247, 0.08533276256454529, 0.038787619347520584, 0.007757523869504117, 0.16870236326848675, 0.12652677245136507, 0.12652677245136507, 0.21087795408560844, 0.08435118163424338, 0.08435118163424338, 0.04217559081712169, 0.08435118163424338, 0.04217559081712169, 0.09664353909998388, 0.2899306172999517, 0.09664353909998388, 0.09664353909998388, 0.09664353909998388, 0.09664353909998388, 0.09664353909998388, 0.09664353909998388, 0.192998516005572, 0.192998516005572, 0.192998516005572, 0.192998516005572, 0.25295145935339247, 0.06323786483834812, 0.3161893241917406, 0.06323786483834812, 0.12647572967669624, 0.12647572967669624, 0.1537360473813287, 0.17569833986437566, 0.1537360473813287, 0.043924584966093916, 0.08784916993218783, 0.17569833986437566, 0.08784916993218783, 0.06588687744914087, 0.043924584966093916, 0.021962292483046958, 0.39535109477921165, 0.30820994575471045, 0.30820994575471045, 0.21041172834789876, 0.2945764196870583, 0.168329382678319, 0.0841646913391595, 0.0841646913391595, 0.0841646913391595, 0.04208234566957975, 0.04208234566957975, 0.11508819360841087, 0.18414110977345738, 0.06905291616504652, 0.1611234710517752, 0.13810583233009305, 0.1611234710517752, 0.046035277443364345, 0.06905291616504652, 0.023017638721682172, 0.023017638721682172, 0.22982162486434268, 0.2626532855592488, 0.1313266427796244, 0.1313266427796244, 0.0656633213898122, 0.0656633213898122, 0.0328316606949061, 0.0328316606949061, 0.0328316606949061, 0.1488779124153884, 0.1488779124153884, 0.1488779124153884, 0.1488779124153884, 0.1488779124153884, 0.1488779124153884, 0.2528912732853478, 0.2528912732853478, 0.2528912732853478, 0.4049051680843995, 0.20245258404219976, 0.34853687669876077, 0.34853687669876077, 0.19782770794866908, 0.17951032758305158, 0.14653904292494008, 0.10257733004745805, 0.09158690182808754, 0.08792342575496404, 0.08425994968184054, 0.04029823680435852, 0.047625188950605524, 0.01831738036561751, 0.23401376123562026, 0.24033845748523164, 0.10119513999378174, 0.08854574749455903, 0.10119513999378174, 0.0758963549953363, 0.05059756999689087, 0.05692226624650223, 0.03794817749766815, 0.012649392499222718, 0.19964146637134736, 0.09982073318567368, 0.1497310997785105, 0.04991036659283684, 0.1497310997785105, 0.09982073318567368, 0.1497310997785105, 0.04991036659283684, 0.04991036659283684, 0.04991036659283684, 0.36669393798884775, 0.2323476105108726, 0.11446536694285636, 0.2613910618247317, 0.07346284744093766, 0.09225566887931706, 0.08029660069125745, 0.029043451313859076, 0.04954471106481843, 0.05808690262771815, 0.008542191562899728, 0.19304195179975872, 0.12284487841802827, 0.28078829352692175, 0.06518299671160684, 0.10278857173753386, 0.06518299671160684, 0.027577421685679816, 0.06518299671160684, 0.06769003504666864, 0.010028153340247205, 0.21296765684439967, 0.14197843789626644, 0.14197843789626644, 0.07098921894813322, 0.14197843789626644, 0.07098921894813322, 0.14197843789626644, 0.07098921894813322, 0.07098921894813322, 0.18945580674603468, 0.18945580674603468, 0.18945580674603468, 0.18945580674603468, 0.18945580674603468, 0.20063527681123863, 0.1643758894357136, 0.18613152186102863, 0.09427440717636515, 0.0894398221929618, 0.07493606724275179, 0.07977065222615512, 0.05318043481743675, 0.04592855734233174, 0.012086462458508352, 0.2238111921336459, 0.2238111921336459, 0.2238111921336459, 0.19011858762649564, 0.09505929381324782, 0.14258894071987172, 0.19011858762649564, 0.09505929381324782, 0.09505929381324782, 0.04752964690662391, 0.09505929381324782, 0.04752964690662391, 0.2144998513009828, 0.21104017627999921, 0.13838700083934374, 0.09514106307704882, 0.08995155054557344, 0.06227415037770469, 0.07784268797213086, 0.06054431286721289, 0.032866912699344136, 0.017298375104917968, 0.06733563910492987, 0.2020069173147896, 0.2020069173147896, 0.06733563910492987, 0.2020069173147896, 0.06733563910492987, 0.06733563910492987, 0.2495947129455045, 0.2495947129455045, 0.183765299160982, 0.12251019944065467, 0.12251019944065467, 0.06125509972032733, 0.183765299160982, 0.06125509972032733, 0.183765299160982, 0.06125509972032733, 0.06125509972032733, 0.06125509972032733, 0.15714444534693828, 0.15714444534693828, 0.15714444534693828, 0.15714444534693828, 0.15714444534693828, 0.2442318780449452, 0.16282125202996348, 0.08141062601498174, 0.08141062601498174, 0.08141062601498174, 0.08141062601498174, 0.08141062601498174, 0.08141062601498174, 0.08141062601498174, 0.2425415064869091, 0.2425415064869091, 0.2425415064869091, 0.2425415064869091, 0.1704721987599873, 0.1704721987599873, 0.08523609937999364, 0.08523609937999364, 0.08523609937999364, 0.2557082981399809, 0.08523609937999364, 0.08523609937999364, 0.08523609937999364, 0.20448900507169326, 0.10224450253584663, 0.10224450253584663, 0.10224450253584663, 0.10224450253584663, 0.20448900507169326, 0.10224450253584663, 0.10224450253584663, 0.10224450253584663, 0.17514461502419498, 0.2627169225362925, 0.10946538439012186, 0.10946538439012186, 0.13135846126814624, 0.06567923063407312, 0.043786153756048746, 0.06567923063407312, 0.021893076878024373, 0.36558893701776507, 0.2586852478892639, 0.2802423518800359, 0.12934262394463195, 0.08622841596308797, 0.10778551995385996, 0.06467131197231597, 0.04311420798154399, 0.021557103990771993, 0.021557103990771993, 0.2309839314815752, 0.1154919657407876, 0.1154919657407876, 0.1154919657407876, 0.1154919657407876, 0.1154919657407876, 0.1154919657407876, 0.1154919657407876, 0.30347935619957417, 0.11252605342231402, 0.12616557504926115, 0.08183712976168292, 0.0852470101684197, 0.15344461830315548, 0.05114820610105182, 0.047738325694315034, 0.027279043253894308, 0.013639521626947154, 0.23637667920204625, 0.23637667920204625, 0.23637667920204625, 0.10836037406456381, 0.10836037406456381, 0.10836037406456381, 0.10836037406456381, 0.10836037406456381, 0.21672074812912762, 0.10836037406456381, 0.10836037406456381, 0.2784904080184952, 0.09283013600616506, 0.09283013600616506, 0.09283013600616506, 0.09283013600616506, 0.09283013600616506, 0.09283013600616506, 0.09283013600616506, 0.2868763459291655, 0.09562544864305515, 0.09562544864305515, 0.09562544864305515, 0.09562544864305515, 0.09562544864305515, 0.09562544864305515, 0.09562544864305515, 0.09562544864305515, 0.418024164370135, 0.13934138812337835, 0.13934138812337835, 0.06967069406168917, 0.06967069406168917, 0.06967069406168917, 0.06967069406168917, 0.06967069406168917, 0.06967069406168917, 0.4911549580121082, 0.25596302014824707, 0.25596302014824707, 0.1345849585382454, 0.2691699170764908, 0.1345849585382454, 0.0672924792691227, 0.0672924792691227, 0.0672924792691227, 0.1345849585382454, 0.20572053404602794, 0.3085808010690419, 0.051430133511506985, 0.051430133511506985, 0.10286026702301397, 0.10286026702301397, 0.051430133511506985, 0.051430133511506985, 0.051430133511506985, 0.13204112265068974, 0.33010280662672437, 0.09903084198801732, 0.06602056132534487, 0.16505140331336218, 0.033010280662672434, 0.06602056132534487, 0.033010280662672434, 0.033010280662672434, 0.15812269975577922, 0.15812269975577922, 0.15812269975577922, 0.15812269975577922, 0.15812269975577922, 0.15812269975577922, 0.15812269975577922, 0.2258767814306943, 0.16940758607302073, 0.11293839071534716, 0.11293839071534716, 0.05646919535767358, 0.05646919535767358, 0.05646919535767358, 0.16940758607302073, 0.05646919535767358, 0.17852486362169567, 0.07140994544867826, 0.21422983634603482, 0.07140994544867826, 0.10711491817301741, 0.14281989089735653, 0.03570497272433913, 0.07140994544867826, 0.07140994544867826, 0.2539469905244331, 0.19560781702557684, 0.12182592112996453, 0.08407704416011637, 0.0926563343805364, 0.06777639274131829, 0.06520260567519229, 0.0712081088294863, 0.03774887696984816, 0.011153077286546048, 0.35039829331904304, 0.13869932443878788, 0.09489953777390749, 0.08759957332976076, 0.08029960888561403, 0.06569967999732057, 0.0729996444414673, 0.04379978666488038, 0.03649982222073365, 0.02189989333244019, 0.23722105120500528, 0.23722105120500528, 0.23722105120500528, 0.28034488207276664, 0.1940849183580692, 0.11591182624162466, 0.07278184438427594, 0.08625996371469742, 0.06739059665210737, 0.07817309211644453, 0.05391247732168589, 0.040434357991264415, 0.010782495464337178, 0.15809201314892038, 0.15809201314892038, 0.15809201314892038, 0.15809201314892038, 0.15809201314892038, 0.15809201314892038, 0.25499047765327587, 0.14433423263392975, 0.20206792568750165, 0.07216711631696487, 0.10103396284375082, 0.07216711631696487, 0.0577336930535719, 0.03848912870238127, 0.03848912870238127, 0.014433423263392975, 0.251633973929381, 0.1258169869646905, 0.07549019217881431, 0.17614378175056672, 0.050326794785876205, 0.10065358957175241, 0.10065358957175241, 0.050326794785876205, 0.07549019217881431, 0.025163397392938103, 0.26064217241668264, 0.26064217241668264, 0.19877268192027522, 0.07950907276811009, 0.11926360915216513, 0.19877268192027522, 0.07950907276811009, 0.11926360915216513, 0.07950907276811009, 0.11926360915216513, 0.039754536384055045, 0.20556195007088476, 0.11595802311690936, 0.15812457697760368, 0.15812457697760368, 0.06852065002362825, 0.07379146925621505, 0.10541638465173578, 0.06852065002362825, 0.026354096162933945, 0.01581245769776037, 0.34167606043492077, 0.14865127304636164, 0.13090186730948264, 0.0976217315528345, 0.059904244361966626, 0.07321629866462588, 0.05768556864485675, 0.0443735143421975, 0.03106146003953825, 0.015530730019769125, 0.4506532489027345, 0.0901306497805469, 0.0901306497805469, 0.0901306497805469, 0.0901306497805469, 0.0901306497805469, 0.0901306497805469, 0.12921250043588153, 0.12921250043588153, 0.3876375013076446, 0.12921250043588153, 0.12921250043588153, 0.25878879098517793, 0.25878879098517793, 0.23819527530482149, 0.11909763765241074, 0.14142844471223776, 0.12281943882904858, 0.09676683059258373, 0.10793223412249724, 0.0521052164729297, 0.07071422235611888, 0.04093981294301619, 0.011165403529913508, 0.18945868606037505, 0.22567872898368205, 0.12537707165760115, 0.0696539286986673, 0.1030878144740276, 0.11423244306581437, 0.0696539286986673, 0.05572314295893384, 0.02786157147946692, 0.019503100035626844, 0.2196457830930504, 0.18636611898804278, 0.10649492513602445, 0.08652712667301986, 0.08652712667301986, 0.1198067907780275, 0.06655932821001527, 0.05324746256801222, 0.046591529747010696, 0.02662373128400611, 0.19750972889930285, 0.09875486444965142, 0.09875486444965142, 0.09875486444965142, 0.09875486444965142, 0.19750972889930285, 0.09875486444965142, 0.11706335819392992, 0.05853167909696496, 0.38045591413027224, 0.05853167909696496, 0.1463291977424124, 0.11706335819392992, 0.05853167909696496, 0.08779751864544744, 0.20148485842451028, 0.18133637258205923, 0.1531284924026278, 0.09268303487527473, 0.0805939433698041, 0.08865333770678452, 0.09671273204376493, 0.05641576035886287, 0.036267274516411846, 0.016118788673960823, 0.3716664131660606, 0.2105194699627952, 0.2105194699627952, 0.4210389399255904, 0.35435253445999376, 0.12402338706099782, 0.1417410137839975, 0.053152880168999064, 0.1417410137839975, 0.053152880168999064, 0.035435253445999376, 0.053152880168999064, 0.035435253445999376, 0.20162724083449435, 0.15122043062587076, 0.10081362041724717, 0.05040681020862359, 0.10081362041724717, 0.20162724083449435, 0.05040681020862359, 0.05040681020862359, 0.05040681020862359, 0.23196977224556137, 0.11598488612278068, 0.11598488612278068, 0.11598488612278068, 0.11598488612278068, 0.23196977224556137, 0.11598488612278068, 0.14803542325830016, 0.19033125847495735, 0.31721876412492894, 0.04229583521665719, 0.08459167043331438, 0.06344375282498578, 0.04229583521665719, 0.04229583521665719, 0.06344375282498578, 0.021147917608328595, 0.26815972462581905, 0.19665046472560063, 0.08172486845739246, 0.11747949840750167, 0.05618584706445732, 0.09704828129315356, 0.07661706417880544, 0.04597023850728326, 0.04341633636798975, 0.015323412835761087, 0.2334845552083719, 0.2334845552083719, 0.2334845552083719, 0.2334845552083719, 0.36544650472161044, 0.12181550157387015, 0.12181550157387015, 0.12181550157387015, 0.12181550157387015, 0.12181550157387015, 0.1949643985485258, 0.16496987569490643, 0.16197042340954448, 0.1109797345583916, 0.08998356856085805, 0.09298302084621998, 0.0509906888511529, 0.08098521170477224, 0.03599342742434322, 0.01799671371217161, 0.21945474739053175, 0.09875463632573929, 0.16459106054289882, 0.0877818989562127, 0.14264558580384565, 0.10972737369526588, 0.06583642421715953, 0.04389094947810635, 0.032918212108579765, 0.04389094947810635, 0.19767870415884856, 0.06589290138628286, 0.3129912815848435, 0.06589290138628286, 0.11531257742599499, 0.09883935207942428, 0.03294645069314143, 0.04941967603971214, 0.06589290138628286, 0.016473225346570714, 0.15444567958057268, 0.07722283979028634, 0.30889135916114535, 0.07722283979028634, 0.15444567958057268, 0.07722283979028634, 0.07722283979028634, 0.07722283979028634, 0.22198135895277105, 0.13247274647181498, 0.18259756946115038, 0.16111550246572093, 0.06802654548552661, 0.10382999047790904, 0.0501248229893354, 0.03938378949162067, 0.028642755993905943, 0.014321377996952971, 0.11230012681854083, 0.22460025363708166, 0.07486675121236055, 0.1497335024247211, 0.11230012681854083, 0.1871668780309014, 0.07486675121236055, 0.03743337560618028, 0.03743337560618028, 0.12156589290747719, 0.18234883936121576, 0.06078294645373859, 0.24313178581495437, 0.12156589290747719, 0.12156589290747719, 0.06078294645373859, 0.06078294645373859, 0.06078294645373859, 0.21664447106633916, 0.18217830521487613, 0.11324597351195002, 0.11570784250134024, 0.08124167664987719, 0.12063158048012067, 0.06647046271353588, 0.059084855745365226, 0.029542427872682613, 0.017233082925731523, 0.210176385865419, 0.18528707701293518, 0.14380489559212878, 0.08296436284161277, 0.08019888408022567, 0.11338462921687077, 0.07466792655745148, 0.06084053275051603, 0.03318574513664511, 0.01382739380693546, 0.2744182932374879, 0.12106689407536232, 0.09685351526028986, 0.1129957678036715, 0.08071126271690822, 0.13720914661874395, 0.06456901017352656, 0.06456901017352656, 0.03228450508676328, 0.01614225254338164, 0.36466214974769484, 0.3550677591291847, 0.12911554877424897, 0.08069721798390561, 0.06455777438712448, 0.11297610517746785, 0.09683666158068674, 0.06455777438712448, 0.04841833079034337, 0.01613944359678112, 0.03227888719356224, 0.2805755171464351, 0.14767132481391318, 0.08860279488834791, 0.07383566240695659, 0.07383566240695659, 0.11813705985113056, 0.05906852992556528, 0.05906852992556528, 0.08860279488834791, 0.01476713248139132, 0.2970459644982493, 0.10801671436299976, 0.08101253577224982, 0.08101253577224982, 0.10801671436299976, 0.10801671436299976, 0.08101253577224982, 0.08101253577224982, 0.08101253577224982, 0.23350421157921228, 0.11675210578960614, 0.16345294810544858, 0.05837605289480307, 0.12842731636856675, 0.046700842315842456, 0.15177773752648796, 0.046700842315842456, 0.046700842315842456, 0.023350421157921228, 0.19368430093908787, 0.15781683780221972, 0.22955176407595598, 0.08608191152848349, 0.10760238941060436, 0.057387941018988996, 0.05021444839161537, 0.057387941018988996, 0.03586746313686812, 0.014346985254747249, 0.24579354402295897, 0.16386236268197266, 0.08193118134098633, 0.08193118134098633, 0.08193118134098633, 0.08193118134098633, 0.08193118134098633, 0.08193118134098633, 0.08193118134098633, 0.1418877372041812, 0.2364795620069687, 0.11823978100348435, 0.0709438686020906, 0.09459182480278748, 0.0709438686020906, 0.0709438686020906, 0.1418877372041812, 0.04729591240139374, 0.22249584648493656, 0.17492776896057083, 0.1388680972888742, 0.10817901501508985, 0.10204119856033297, 0.10204119856033297, 0.055240348092811836, 0.055240348092811836, 0.029154628160095137, 0.012275632909513742, 0.1629364628990822, 0.20517924957662204, 0.2092023721173401, 0.09253181843651581, 0.08046245081436158, 0.08046245081436158, 0.04425434794789887, 0.0764393282736435, 0.03419654159610367, 0.016092490162872317, 0.19099046045296783, 0.28648569067945173, 0.09549523022648392, 0.09549523022648392, 0.09549523022648392, 0.09549523022648392, 0.09549523022648392, 0.09549523022648392, 0.09999491793065195, 0.09999491793065195, 0.09999491793065195, 0.09999491793065195, 0.1999898358613039, 0.09999491793065195, 0.1999898358613039, 0.09999491793065195, 0.2503820513490414, 0.1669213675660276, 0.0834606837830138, 0.0834606837830138, 0.0834606837830138, 0.2503820513490414, 0.0834606837830138, 0.13174771746736608, 0.26349543493473215, 0.13174771746736608, 0.13174771746736608, 0.13174771746736608, 0.13174771746736608, 0.18320158815814921, 0.148306047556597, 0.061067196052716405, 0.11341050695504476, 0.11341050695504476, 0.16575381785737311, 0.08723885150388058, 0.061067196052716405, 0.04361942575194029, 0.017447770300776117, 0.24583715875278658, 0.17353211206079053, 0.10122706536879447, 0.10122706536879447, 0.18799312139918975, 0.05784403735359684, 0.07230504669199606, 0.02892201867679842, 0.02892201867679842, 0.01446100933839921, 0.1913318787162972, 0.0956659393581486, 0.2869978180744458, 0.0956659393581486, 0.0956659393581486, 0.0956659393581486, 0.0956659393581486, 0.0956659393581486, 0.1787854685190829, 0.04469636712977072, 0.3128745699083951, 0.08939273425954145, 0.13408910138931218, 0.08939273425954145, 0.04469636712977072, 0.04469636712977072, 0.17299071306399508, 0.10811919566499692, 0.1513668739309957, 0.12974303479799631, 0.17299071306399508, 0.08649535653199754, 0.04324767826599877, 0.04324767826599877, 0.08649535653199754, 0.021623839132999385, 0.358503284096272, 0.134438731536102, 0.1434013136385088, 0.044812910512034, 0.10755098522888161, 0.0627380747168476, 0.044812910512034, 0.0627380747168476, 0.0179251642048136, 0.0179251642048136, 0.28421918726046563, 0.28421918726046563, 0.28421918726046563, 0.3837402232709108, 0.3837402232709108, 0.24746889292823782, 0.16497926195215853, 0.08248963097607927, 0.24746889292823782, 0.08248963097607927, 0.08248963097607927, 0.08248963097607927, 0.08248963097607927, 0.2022136590007775, 0.404427318001555, 0.2022136590007775, 0.19216083909531495, 0.21936945348049228, 0.15134791751754895, 0.12073822633422444, 0.07482368955923768, 0.07992530475645843, 0.06802153596294334, 0.04931561357313392, 0.030609691183324506, 0.010203230394441501, 0.19599600684912158, 0.19599600684912158, 0.11284618576161547, 0.13066400456608107, 0.09799800342456079, 0.089089094022328, 0.0712712752178624, 0.05939272934821867, 0.03266600114152027, 0.011878545869643733, 0.15857059590225822, 0.1964113062880244, 0.15136284154306467, 0.097304683849113, 0.11172019256750011, 0.10271049961850817, 0.07568142077153234, 0.04144458756536295, 0.046850403334758116, 0.019821324487782278, 0.2018568629188365, 0.17302016821614558, 0.17302016821614558, 0.10957943987022555, 0.0807427451675346, 0.09516109251888007, 0.06344072834592004, 0.057673389405381865, 0.034604033643229115, 0.014418347351345466, 0.23178179463804238, 0.17078658552276807, 0.10369185549596632, 0.1585875436997132, 0.11589089731902119, 0.06099520911527431, 0.05489568820374688, 0.07319425093832917, 0.024398083646109722, 0.012199041823054861, 0.18569703341738944, 0.18569703341738944, 0.18569703341738944, 0.18569703341738944, 0.18569703341738944, 0.21519357583734275, 0.19017106701904707, 0.13011704585513748, 0.12010804232781921, 0.06505852292756874, 0.08507652998220527, 0.10009003527318268, 0.05004501763659134, 0.030027010581954804, 0.010009003527318268, 0.17106910189087127, 0.2177243114974725, 0.12441389228427001, 0.08294259485618001, 0.0933104192132025, 0.08294259485618001, 0.11922998010575876, 0.051839121785112505, 0.041471297428090005, 0.015551736535533751, 0.18205664760960927, 0.19163857643116763, 0.1628927899664925, 0.10061025262636301, 0.07186446616168787, 0.09102832380480463, 0.09102832380480463, 0.057491572929350296, 0.028745786464675148, 0.014372893232337574, 0.26417908500740883, 0.14487240145567581, 0.13350986016503458, 0.09374096564779023, 0.09374096564779023, 0.07385651838916807, 0.06533461242118713, 0.0568127064532062, 0.05965334177586651, 0.01704381193596186, 0.24109869910809992, 0.13627317775675213, 0.1467557298918869, 0.10657261337387025, 0.10831970539639271, 0.07337786494594345, 0.0663894968558536, 0.07687204899098837, 0.029700564382881874, 0.01572382820270217, 0.13423754412166064, 0.13423754412166064, 0.2684750882433213, 0.13423754412166064, 0.13423754412166064, 0.13423754412166064, 0.13423754412166064, 0.20302395390567918, 0.13169121334422434, 0.19204968612699383, 0.09328127611882557, 0.12071694556553897, 0.1097426777868536, 0.032922803336056086, 0.04389707111474145, 0.07133274056145485, 0.005487133889342681, 0.1557070982468694, 0.14654785717352414, 0.23814026790697673, 0.07327392858676207, 0.10075165180679785, 0.10991089288014311, 0.036636964293381036, 0.045796205366726295, 0.07327392858676207, 0.009159241073345259, 0.16822458461514841, 0.10093475076908905, 0.15700961230747185, 0.17943955692282498, 0.10093475076908905, 0.10093475076908905, 0.0560748615383828, 0.08971977846141249, 0.044859889230706246, 0.011214972307676561, 0.054543169532907575, 0.10908633906581515, 0.10908633906581515, 0.10908633906581515, 0.16362950859872272, 0.10908633906581515, 0.2181726781316303, 0.054543169532907575, 0.054543169532907575, 0.5109442881094802, 0.15074293671287356, 0.1004952911419157, 0.1004952911419157, 0.05024764557095785, 0.2009905822838314, 0.05024764557095785, 0.2009905822838314, 0.05024764557095785, 0.05024764557095785, 0.1706432017082976, 0.1706432017082976, 0.1706432017082976, 0.1706432017082976, 0.1706432017082976, 0.16569994215992132, 0.16569994215992132, 0.16569994215992132, 0.16569994215992132, 0.16569994215992132, 0.3490225118177858, 0.20363426995046624, 0.20363426995046624, 0.20363426995046624, 0.20363426995046624, 0.20363426995046624, 0.21409051984572883, 0.10704525992286441, 0.32113577976859325, 0.10704525992286441, 0.10704525992286441, 0.10704525992286441, 0.22418181943035123, 0.12316161684753864, 0.16606060698544536, 0.10378787936590335, 0.113474748106721, 0.0816464651011773, 0.07334343475190502, 0.051202020487178986, 0.04705050531254285, 0.016606060698544534, 0.2437072437839946, 0.1675487301014963, 0.1218536218919973, 0.06092681094599865, 0.07615851368249832, 0.10662191915549764, 0.06092681094599865, 0.1218536218919973, 0.030463405472999326, 0.015231702736499663, 0.26554971239247355, 0.09372342790322596, 0.09372342790322596, 0.18744685580645193, 0.12496457053763461, 0.07810285658602163, 0.04686171395161298, 0.06248228526881731, 0.031241142634408654, 0.015620571317204327, 0.37531503130178456, 0.17059774150081117, 0.1023586449004867, 0.06823909660032447, 0.06823909660032447, 0.06823909660032447, 0.06823909660032447, 0.034119548300162235, 0.034119548300162235, 0.22065808972867895, 0.12900011399522768, 0.13239485383720737, 0.17992121162492283, 0.09165797573345125, 0.0848684960494919, 0.061105317155634165, 0.05771057731365449, 0.030552658577817082, 0.013578959367918704, 0.14530287026232372, 0.07265143513116186, 0.14530287026232372, 0.21795430539348556, 0.10897715269674278, 0.10897715269674278, 0.07265143513116186, 0.10897715269674278, 0.03632571756558093, 0.2543218288809694, 0.16326833459025195, 0.13501035360347757, 0.08791371862552028, 0.08477394296032312, 0.08791371862552028, 0.06907506463433737, 0.0533761863083516, 0.04395685931276014, 0.015698878325985762, 0.47723271270588913, 0.15907757090196306, 0.15907757090196306, 0.15907757090196306, 0.396457513530866, 0.49806635786458214, 0.23062682658061728, 0.11531341329030864, 0.11531341329030864, 0.11531341329030864, 0.11531341329030864, 0.11531341329030864, 0.05765670664515432, 0.05765670664515432, 0.05765670664515432, 0.15576035921134834, 0.2347692370721772, 0.15576035921134834, 0.1128698255154698, 0.08803846390206645, 0.0632071022886631, 0.09481065343299463, 0.04289053369587852, 0.036118344164950335, 0.013544379061856376, 0.15925017744190964, 0.15925017744190964, 0.15925017744190964, 0.15925017744190964, 0.15925017744190964, 0.15925017744190964, 0.21853098213693375, 0.21853098213693375, 0.21853098213693375, 0.21853098213693375, 0.2819118731570669, 0.2819118731570669, 0.22169840553762765, 0.1477989370250851, 0.1944722855593225, 0.08362308279050867, 0.0933466970684748, 0.08362308279050867, 0.06028640852338998, 0.0622311313789832, 0.04083917996745772, 0.013613059989152574, 0.39358451393599203, 0.15795233244718762, 0.15795233244718762, 0.1797388610605928, 0.09259274660697205, 0.14161243598713372, 0.07625285014691816, 0.06535958584021556, 0.07625285014691816, 0.043573057226810374, 0.01633989646005389, 0.20097099644308636, 0.14355071174506168, 0.12632462633565428, 0.09187245551683948, 0.11484056939604935, 0.08613042704703701, 0.08613042704703701, 0.09761448398664195, 0.040194199288617276, 0.011484056939604935, 0.2737128108947918, 0.2737128108947918, 0.2737128108947918, 0.2902299029301133, 0.11286718447282183, 0.11017987055680227, 0.10480524272476313, 0.10480524272476313, 0.08599404531262617, 0.061808220068450055, 0.07524478964854789, 0.040309708740293514, 0.01074925566407827, 0.22570782919154198, 0.15047188612769466, 0.3009437722553893, 0.07523594306384733, 0.07523594306384733, 0.07523594306384733, 0.07523594306384733, 0.07523594306384733, 0.3048158657600769, 0.15240793288003845, 0.15240793288003845, 0.07620396644001923, 0.07620396644001923, 0.07620396644001923, 0.15240793288003845, 0.07620396644001923, 0.22408860783421725, 0.1493924052228115, 0.298784810445623, 0.07469620261140575, 0.07469620261140575, 0.07469620261140575, 0.07469620261140575, 0.07469620261140575, 0.1368414711154101, 0.1368414711154101, 0.1368414711154101, 0.1368414711154101, 0.1368414711154101, 0.1368414711154101, 0.1368414711154101, 0.1368414711154101, 0.1368414711154101, 0.3003943041100721, 0.15019715205503606, 0.09011829123302163, 0.06007886082201443, 0.06007886082201443, 0.06007886082201443, 0.18023658246604327, 0.030039430411007213, 0.030039430411007213, 0.16827518733281038, 0.10096511239968622, 0.1346201498662483, 0.12340180404406094, 0.10096511239968622, 0.08974676657749887, 0.10096511239968622, 0.12340180404406094, 0.03365503746656207, 0.02243669164437472, 0.08811997807063095, 0.1762399561412619, 0.1762399561412619, 0.08811997807063095, 0.1762399561412619, 0.08811997807063095, 0.08811997807063095, 0.08811997807063095, 0.17485134554200657, 0.1457094546183388, 0.1457094546183388, 0.11656756369467104, 0.11656756369467104, 0.08742567277100328, 0.05828378184733552, 0.05828378184733552, 0.02914189092366776, 0.02914189092366776, 0.4854098395725727, 0.16180327985752424, 0.16180327985752424, 0.16180327985752424, 0.1298257529922751, 0.1298257529922751, 0.15579090359073014, 0.1038606023938201, 0.07789545179536507, 0.2077212047876402, 0.07789545179536507, 0.05193030119691005, 0.07789545179536507, 0.025965150598455024, 0.20939576480570196, 0.10469788240285098, 0.12563745888342118, 0.12563745888342118, 0.10469788240285098, 0.08375830592228078, 0.06281872944171059, 0.12563745888342118, 0.020939576480570196, 0.020939576480570196, 0.22651543922672332, 0.15926866820628982, 0.13095423830294942, 0.12387563082711431, 0.0884825934479388, 0.09202189718585635, 0.046010948592928175, 0.0884825934479388, 0.031853733641257966, 0.014157214951670207, 0.3226733928025353, 0.1401510696011012, 0.15644770560122925, 0.12385443360097316, 0.0717051984005634, 0.06192721680048658, 0.03911192640030731, 0.04237125360033292, 0.0358525992002817, 0.009777981600076828, 0.4450590074797823, 0.13351770224393467, 0.1780236029919129, 0.04450590074797822, 0.08901180149595644, 0.04450590074797822, 0.04450590074797822, 0.04450590074797822, 0.04450590074797822, 0.16673143305886926, 0.16673143305886926, 0.11909388075633517, 0.14291265690760221, 0.09527510460506815, 0.047637552302534074, 0.047637552302534074, 0.11909388075633517, 0.047637552302534074, 0.023818776151267037, 0.3090167598320859, 0.3090167598320859, 0.3090167598320859, 0.2541196401719624, 0.1386107128210704, 0.0924071418807136, 0.0693053564105352, 0.0924071418807136, 0.0693053564105352, 0.1617124982912488, 0.0462035709403568, 0.0462035709403568, 0.0231017854701784, 0.33027544220770166, 0.16513772110385083, 0.16513772110385083, 0.16513772110385083, 0.1414503629844481, 0.2829007259688962, 0.1414503629844481, 0.1414503629844481, 0.1414503629844481, 0.18764096458387552, 0.13134867520871288, 0.15011277166710044, 0.13134867520871288, 0.13134867520871288, 0.07505638583355022, 0.05629228937516266, 0.05629228937516266, 0.03752819291677511, 0.03752819291677511, 0.23719750980209361, 0.11859875490104681, 0.11859875490104681, 0.23719750980209361, 0.11859875490104681, 0.20258175801039738, 0.09922371820917422, 0.1901787932342506, 0.10749236139327208, 0.11989532616941885, 0.07441778865688067, 0.06614914547278282, 0.0950893966171253, 0.03307457273639141, 0.016537286368195705, 0.2158130854188336, 0.19158916766774003, 0.1387369834835359, 0.12111958875546784, 0.11451306573244231, 0.06386305588924668, 0.05065000984319564, 0.06386305588924668, 0.026426092092102074, 0.013213046046051037, 0.19942304920540077, 0.15510681604864504, 0.18908259480215778, 0.09601850517297075, 0.08124642745405217, 0.09601850517297075, 0.08567805076972775, 0.051702272016215016, 0.029544155437837154, 0.016249285490810433, 0.20100456718585016, 0.17639176304064402, 0.17639176304064402, 0.1394725568228348, 0.06563414438721638, 0.05742987633881433, 0.07794054645981945, 0.053327742314613305, 0.03281707219360819, 0.020510670121005117, 0.23027815716134784, 0.23027815716134784, 0.23027815716134784, 0.19396998291178788, 0.1666929540648177, 0.13032358226885749, 0.12426202030286411, 0.1030465534218873, 0.09395421047289725, 0.06061561965993371, 0.08486186752390719, 0.030307809829966854, 0.018184685897980113, 0.19108501506219175, 0.09969652959766526, 0.10800457373080403, 0.17446892679591422, 0.12462066199708158, 0.1163126178639428, 0.0581563089319714, 0.08308044133138771, 0.024924132399416314, 0.008308044133138771, 0.23288032955071486, 0.23288032955071486, 0.23288032955071486, 0.15565444963577427, 0.15565444963577427, 0.20753926618103236, 0.05188481654525809, 0.20753926618103236, 0.05188481654525809, 0.05188481654525809, 0.05188481654525809, 0.05188481654525809, 0.19705876987443627, 0.19705876987443627, 0.19705876987443627, 0.19705876987443627, 0.19705876987443627, 0.2095243357143837, 0.10476216785719185, 0.10476216785719185, 0.052381083928595924, 0.2095243357143837, 0.052381083928595924, 0.15714325178578775, 0.052381083928595924, 0.052381083928595924, 0.14211593325060362, 0.28423186650120724, 0.07105796662530181, 0.14211593325060362, 0.14211593325060362, 0.10658694993795273, 0.07105796662530181, 0.07105796662530181, 0.035528983312650905, 0.1312812707622418, 0.1312812707622418, 0.3063229651118975, 0.08752084717482786, 0.08752084717482786, 0.08752084717482786, 0.04376042358741393, 0.04376042358741393, 0.08752084717482786, 0.2691238414178616, 0.2691238414178616, 0.1441116343874454, 0.2882232687748908, 0.11208682674579086, 0.0960744229249636, 0.06404961528330906, 0.12809923056661812, 0.0480372114624818, 0.08006201910413634, 0.03202480764165453, 0.016012403820827265, 0.18678912690486618, 0.09339456345243309, 0.09339456345243309, 0.18678912690486618, 0.09339456345243309, 0.09339456345243309, 0.09339456345243309, 0.18904494454872306, 0.18904494454872306, 0.18904494454872306, 0.18904494454872306, 0.18904494454872306, 0.18904494454872306, 0.16253136160041148, 0.1218985212003086, 0.32506272320082297, 0.08126568080020574, 0.08126568080020574, 0.08126568080020574, 0.04063284040010287, 0.04063284040010287, 0.08126568080020574, 0.207496997846424, 0.207496997846424, 0.207496997846424, 0.22990643031104635, 0.1548843319990207, 0.13552379049914312, 0.1403639258741125, 0.07018196293705625, 0.07744216599951036, 0.06292175987460216, 0.08228230137447974, 0.03630101531227048, 0.009680270749938795, 0.24404355074643888, 0.10846380033175061, 0.13557975041468825, 0.13557975041468825, 0.054231900165875306, 0.054231900165875306, 0.054231900165875306, 0.13557975041468825, 0.027115950082937653, 0.3128618539647492, 0.10428728465491639, 0.10428728465491639, 0.10428728465491639, 0.03476242821830546, 0.03476242821830546, 0.13904971287322185, 0.06952485643661092, 0.03476242821830546, 0.03476242821830546, 0.25279201308490556, 0.10111680523396221, 0.08426400436163518, 0.1853808095955974, 0.10111680523396221, 0.08426400436163518, 0.05055840261698111, 0.08426400436163518, 0.033705601744654076, 0.033705601744654076, 0.23364802654636016, 0.10384356735393785, 0.10384356735393785, 0.051921783676968924, 0.23364802654636016, 0.025960891838484462, 0.1298044591924223, 0.025960891838484462, 0.051921783676968924, 0.025960891838484462, 0.14822163791235266, 0.14822163791235266, 0.14822163791235266, 0.14822163791235266, 0.14822163791235266, 0.14822163791235266, 0.20802356458468446, 0.20802356458468446, 0.20802356458468446, 0.20802356458468446, 0.15911779324487585, 0.21215705765983445, 0.10607852882991722, 0.21215705765983445, 0.10607852882991722, 0.05303926441495861, 0.05303926441495861, 0.05303926441495861, 0.1351517311190912, 0.2196215630685232, 0.18583363028875038, 0.0506818991696592, 0.2196215630685232, 0.084469831949432, 0.0168939663898864, 0.0506818991696592, 0.0337879327797728, 0.0168939663898864, 0.17966187470337866, 0.134746406027534, 0.08983093735168933, 0.044915468675844665, 0.22457734337922333, 0.044915468675844665, 0.134746406027534, 0.044915468675844665, 0.044915468675844665, 0.4884421118135569, 0.24422105590677845, 0.13549455970943633, 0.16936819963679542, 0.1185577397457568, 0.08468409981839771, 0.18630501960047496, 0.08468409981839771, 0.10162091978207725, 0.05081045989103863, 0.05081045989103863, 0.01693681996367954, 0.20730978154252064, 0.06910326051417355, 0.1382065210283471, 0.06910326051417355, 0.20730978154252064, 0.06910326051417355, 0.1382065210283471, 0.06910326051417355, 0.2362807164605298, 0.1724676762485619, 0.13452478747387828, 0.09830657546168028, 0.0810598078368241, 0.08795851488676656, 0.06726239373693914, 0.07071174726191037, 0.039667565537169235, 0.013797414099884952, 0.13580207794911683, 0.13580207794911683, 0.13580207794911683, 0.13580207794911683, 0.13580207794911683, 0.27160415589823367, 0.13580207794911683, 0.13580207794911683, 0.2084975553262977, 0.2084975553262977, 0.2084975553262977, 0.2084975553262977, 0.19559511481164568, 0.14669633610873428, 0.09779755740582284, 0.04889877870291142, 0.09779755740582284, 0.19559511481164568, 0.09779755740582284, 0.04889877870291142, 0.04889877870291142, 0.15821188268813807, 0.26368647114689675, 0.18458052980282774, 0.13184323557344838, 0.07910594134406904, 0.05273729422937935, 0.05273729422937935, 0.026368647114689676, 0.026368647114689676, 0.2889611695812807, 0.2889611695812807, 0.42299892460102473, 0.14099964153367492, 0.14099964153367492, 0.07049982076683746, 0.07049982076683746, 0.07049982076683746, 0.07049982076683746, 0.10465931239417699, 0.1902896588985036, 0.18077517595357842, 0.08563034650432663, 0.18077517595357842, 0.0951448294492518, 0.057086897669551086, 0.057086897669551086, 0.038057931779700724, 0.019028965889850362, 0.17106887803463008, 0.17106887803463008, 0.17106887803463008, 0.17106887803463008, 0.17106887803463008, 0.5068869055597012, 0.0909797009978951, 0.06498550071278221, 0.06498550071278221, 0.0909797009978951, 0.06498550071278221, 0.051988400570225766, 0.03899130042766932, 0.025994200285112883, 0.012997100142556442, 0.2250856536226312, 0.2250856536226312, 0.2250856536226312, 0.12497582744604457, 0.08331721829736305, 0.3332688731894522, 0.08331721829736305, 0.08331721829736305, 0.08331721829736305, 0.041658609148681525, 0.08331721829736305, 0.041658609148681525, 0.2291684835998033, 0.11458424179990165, 0.11458424179990165, 0.11458424179990165, 0.11458424179990165, 0.11458424179990165, 0.11458424179990165, 0.3091884043493924, 0.3091884043493924, 0.03864855054367405, 0.05797282581551107, 0.05797282581551107, 0.11594565163102213, 0.03864855054367405, 0.03864855054367405, 0.019324275271837024, 0.019324275271837024, 0.37173904561087034, 0.30304979301810986, 0.11019992473385812, 0.06887495295866132, 0.20662485887598397, 0.02754998118346453, 0.06887495295866132, 0.05509996236692906, 0.11019992473385812, 0.0413249717751968, 0.013774990591732265, 0.19192289220611605, 0.1119550204535677, 0.2718907639586644, 0.06397429740203868, 0.1119550204535677, 0.09596144610305803, 0.01599357435050967, 0.06397429740203868, 0.04798072305152901, 0.01599357435050967, 0.22106266393488375, 0.09474114168637875, 0.34738418618338873, 0.09474114168637875, 0.0631607611242525, 0.0631607611242525, 0.03158038056212625, 0.03158038056212625, 0.0631607611242525, 0.2537269682210824, 0.15613967275143537, 0.11385184471458829, 0.1333693038085177, 0.09758729546964709, 0.06831110682875297, 0.07481692652672944, 0.052046557583811784, 0.03252909848988236, 0.01626454924494118, 0.18362332190813668, 0.1224155479387578, 0.09181166095406834, 0.15301943492344725, 0.1224155479387578, 0.0612077739693789, 0.0612077739693789, 0.1224155479387578, 0.0612077739693789, 0.03060388698468945, 0.2529130442657376, 0.13102723980032185, 0.12493294957705109, 0.10665007890723872, 0.12798009468868649, 0.0822729180141556, 0.0761786277908848, 0.048754321786166274, 0.030471451116353923, 0.021330015781447744, 0.2201773322772075, 0.1437268696809549, 0.11620470314630396, 0.10397262913090355, 0.12232074015400417, 0.07950848110010271, 0.09174055511550312, 0.05504433306930188, 0.05504433306930188, 0.015290092519250521, 0.2229809929408704, 0.2229809929408704, 0.2229809929408704, 0.18197883144212773, 0.17826497773922714, 0.20426195365953112, 0.13369873330442036, 0.06313551294930962, 0.05570780554350849, 0.07427707405801132, 0.06684936665221018, 0.022283122217403393, 0.011141561108701696, 0.28600231673809506, 0.16445133212440466, 0.14776786364801578, 0.0607754923068452, 0.0893757239806547, 0.09533410557936502, 0.0703089028647817, 0.03813364223174601, 0.03336693695277776, 0.01310843951716269, 0.34504713582655205, 0.34504713582655205, 0.4046323125339643, 0.09196188921226461, 0.09196188921226461, 0.12874664489717047, 0.05517713352735877, 0.05517713352735877, 0.07356951136981169, 0.05517713352735877, 0.01839237784245292, 0.01839237784245292, 0.3565769595487009, 0.3565769595487009, 0.26619370503199374, 0.15315254262114708, 0.1276271188509559, 0.10210169508076472, 0.10574818419079203, 0.08386924953062816, 0.05469733665040967, 0.05469733665040967, 0.029171912880218493, 0.018232445550136558, 0.24949151463675578, 0.14067074761434104, 0.16721239810761293, 0.08227911652914287, 0.11147493207174196, 0.061045796134525356, 0.07166245633183412, 0.06900829128250692, 0.03450414564125346, 0.015924990295963136, 0.22052741694683625, 0.11406590531732909, 0.15208787375643879, 0.14828567691252784, 0.09505492109777425, 0.07984613372213037, 0.06463734634648649, 0.07984613372213037, 0.030417574751287757, 0.011406590531732909, 0.2418697034684203, 0.12093485173421015, 0.12093485173421015, 0.12093485173421015, 0.12093485173421015, 0.12093485173421015, 0.30511022666744186, 0.15255511333372093, 0.15255511333372093, 0.15255511333372093, 0.15255511333372093, 0.29456978917878823, 0.14914926034369025, 0.14542052883509798, 0.08948955620621415, 0.07084589866325287, 0.0932182877148064, 0.0596597041374761, 0.05593097262888384, 0.02982985206873805, 0.01118619452577677, 0.18542083178660157, 0.16342175004920817, 0.11628086061193657, 0.13827994234932997, 0.08485360098708886, 0.10370995676199748, 0.08171087502460408, 0.07228269713714976, 0.04085543751230204, 0.009428177887454316, 0.23019967966274218, 0.34529951949411325, 0.11509983983137109, 0.11509983983137109, 0.11509983983137109, 0.11509983983137109, 0.11509983983137109, 0.11509983983137109, 0.19208447794566555, 0.20809151777447102, 0.13605983854484643, 0.08003519914402732, 0.17607743811686008, 0.08803871905843004, 0.032014079657610925, 0.04001759957201366, 0.032014079657610925, 0.016007039828805462, 0.1688089223126203, 0.3376178446252406, 0.1688089223126203, 0.1688089223126203, 0.1768057799374826, 0.3536115598749652, 0.05893525997916087, 0.05893525997916087, 0.11787051995832173, 0.11787051995832173, 0.05893525997916087, 0.05893525997916087, 0.05893525997916087, 0.19883393574019742, 0.21871732931421714, 0.1546486166868202, 0.09941696787009871, 0.09057990405942326, 0.09057990405942326, 0.041976053100708344, 0.06406871262739694, 0.02872045738469518, 0.0110463297633443, 0.20118632396642347, 0.13412421597761565, 0.11177017998134638, 0.11177017998134638, 0.17883228797015419, 0.06706210798880782, 0.08941614398507709, 0.08941614398507709, 0.022354035996269273, 0.022354035996269273, 0.1978859371579032, 0.23746312458948385, 0.11873156229474192, 0.1108161248084258, 0.07915437486316128, 0.08706981234947742, 0.0554080624042129, 0.0554080624042129, 0.04749262491789677, 0.015830874972632258, 0.27815442797160916, 0.27815442797160916, 0.07586029853771159, 0.07586029853771159, 0.05057353235847439, 0.12643383089618598, 0.025286766179237197, 0.025286766179237197, 0.025286766179237197, 0.2252281239174392, 0.16480106628105307, 0.10437400864466695, 0.12085411527277226, 0.09338727089259674, 0.08789390201656164, 0.05493368876035103, 0.09338727089259674, 0.038453582132245714, 0.01648010662810531, 0.3881455980899251, 0.19407279904496255, 0.19407279904496255, 0.19407279904496255, 0.3868673375067659, 0.385749044849889, 0.2305428031355393, 0.2305428031355393, 0.2305428031355393, 0.2305428031355393, 0.23478393132381248, 0.13206596136964452, 0.13206596136964452, 0.11739196566190624, 0.08804397424642968, 0.04402198712321484, 0.04402198712321484, 0.13206596136964452, 0.05869598283095312, 0.02934799141547656, 0.12612019360871987, 0.25224038721743974, 0.12612019360871987, 0.25224038721743974, 0.12612019360871987, 0.12612019360871987, 0.4672195504945104, 0.0584024438118138, 0.0584024438118138, 0.0584024438118138, 0.1168048876236276, 0.0584024438118138, 0.0584024438118138, 0.0584024438118138, 0.1912828743214983, 0.1912828743214983, 0.1912828743214983, 0.1912828743214983, 0.2403815152857961, 0.2403815152857961, 0.2403815152857961, 0.4616096978513681, 0.09232193957027361, 0.09232193957027361, 0.09232193957027361, 0.09232193957027361, 0.09232193957027361, 0.09232193957027361, 0.09232193957027361, 0.2038573650725773, 0.12231441904354637, 0.12231441904354637, 0.2038573650725773, 0.12231441904354637, 0.08154294602903092, 0.04077147301451546, 0.04077147301451546, 0.04077147301451546, 0.21405398624046554, 0.14480122598619727, 0.14480122598619727, 0.09443558216491126, 0.15739263694151878, 0.08184417120958976, 0.044069938343625256, 0.050365643821286006, 0.044069938343625256, 0.018887116432982253, 0.2714862316104095, 0.2714862316104095, 0.2714862316104095, 0.11220455134868704, 0.11220455134868704, 0.22440910269737407, 0.11220455134868704, 0.22440910269737407, 0.22440910269737407, 0.11220455134868704, 0.2700505744471736, 0.19459526688105158, 0.11913995931492954, 0.09134063547477932, 0.09928329942910795, 0.0675126436117934, 0.047655983725971816, 0.0436846517488075, 0.047655983725971816, 0.019856659885821592, 0.15130123981723223, 0.15130123981723223, 0.07565061990861612, 0.22695185972584836, 0.07565061990861612, 0.07565061990861612, 0.07565061990861612, 0.07565061990861612, 0.07565061990861612, 0.17365390610411172, 0.13892312488328937, 0.27784624976657873, 0.06946156244164468, 0.06946156244164468, 0.03473078122082234, 0.06946156244164468, 0.06946156244164468, 0.0921449523624412, 0.2764348570873236, 0.1842899047248824, 0.0921449523624412, 0.0921449523624412, 0.0921449523624412, 0.2045263324159017, 0.1431684326911312, 0.15850790762232383, 0.07669737465596314, 0.13805527438073367, 0.08692369127675822, 0.07669737465596314, 0.061357899724770515, 0.040905266483180346, 0.020452633241590173, 0.26062678128195826, 0.16885678787281802, 0.10278239261823706, 0.1174655915636995, 0.08442839393640901, 0.07341599472731218, 0.09911159288187145, 0.0403787971000217, 0.029366397890924874, 0.022024798418193655, 0.23608083192669194, 0.23608083192669194, 0.23608083192669194, 0.23608083192669194, 0.3336264739676592, 0.13687239957647557, 0.12831787460294583, 0.08126798724853236, 0.08982251222206208, 0.0556044123279432, 0.06415893730147292, 0.0556044123279432, 0.03421809989411889, 0.017109049947059446, 0.15262452308495128, 0.15262452308495128, 0.15262452308495128, 0.15262452308495128, 0.15262452308495128, 0.15262452308495128, 0.23871854525542394, 0.17392351154323746, 0.09548741810216958, 0.09548741810216958, 0.09548741810216958, 0.11253874276327129, 0.08525662330550855, 0.06138476877996616, 0.03069238438998308, 0.013641059728881369, 0.1842106380536518, 0.13615568899617742, 0.13615568899617742, 0.09610989811494876, 0.08810073993870303, 0.0800915817624573, 0.05606410723372011, 0.15217400534866887, 0.04004579088122865, 0.01601831635249146, 0.21474731557378418, 0.18187782849616416, 0.10737365778689209, 0.1172345039101781, 0.1051823586483841, 0.10189540994062209, 0.06026072630897005, 0.06026072630897005, 0.03725208535463603, 0.013147794831048012, 0.14872136183607507, 0.14872136183607507, 0.14872136183607507, 0.14872136183607507, 0.14872136183607507, 0.3129507245958011, 0.3129507245958011, 0.146585863567735, 0.10993939767580126, 0.10993939767580126, 0.146585863567735, 0.10993939767580126, 0.146585863567735, 0.03664646589193375, 0.146585863567735, 0.03664646589193375, 0.3525606361968216, 0.1762803180984108, 0.1762803180984108, 0.1762803180984108, 0.1762803180984108, 0.39562917339139564, 0.1318763911304652, 0.0659381955652326, 0.1318763911304652, 0.0659381955652326, 0.0659381955652326, 0.0659381955652326, 0.0659381955652326, 0.0659381955652326, 0.22233145571867557, 0.16854158739964115, 0.12371669713377914, 0.12550969274441362, 0.10399374541679986, 0.0824777980891861, 0.06992682881474474, 0.055582863929668894, 0.034066916602055124, 0.014343964885075843, 0.16607216028346106, 0.16607216028346106, 0.16607216028346106, 0.05535738676115368, 0.16607216028346106, 0.11071477352230737, 0.05535738676115368, 0.05535738676115368, 0.05535738676115368, 0.18007670077181395, 0.3601534015436279, 0.18007670077181395, 0.18007670077181395, 0.18007670077181395, 0.16207217818067376, 0.14181315590808954, 0.14181315590808954, 0.1012951113629211, 0.1012951113629211, 0.12155413363550532, 0.06077706681775266, 0.04051804454516844, 0.08103608909033688, 0.02025902227258422, 0.18826663486671388, 0.1506133078933711, 0.1506133078933711, 0.13806219890225685, 0.08785776293779982, 0.07530665394668555, 0.0627555449555713, 0.07530665394668555, 0.07530665394668555, 0.012551108991114258, 0.23794241868864968, 0.23794241868864968, 0.23794241868864968, 0.1484564400634461, 0.1484564400634461, 0.1484564400634461, 0.1484564400634461, 0.1484564400634461, 0.2527331213084332, 0.15403261239508056, 0.1256188295260851, 0.07925949958193466, 0.1330961408073997, 0.07327765055688298, 0.07028672604435715, 0.06430487701930547, 0.03439563189404712, 0.013459160306366262, 0.2034850281276608, 0.1017425140638304, 0.29069289732522974, 0.08720786919756891, 0.1017425140638304, 0.07267322433130743, 0.029069289732522974, 0.043603934598784456, 0.043603934598784456, 0.014534644866261487, 0.2774409507155034, 0.08761293180489581, 0.07301077650407985, 0.05840862120326387, 0.21903232951223953, 0.05840862120326387, 0.11681724240652774, 0.029204310601631936, 0.05840862120326387, 0.014602155300815968, 0.13222094581796606, 0.1983314187269491, 0.13222094581796606, 0.1983314187269491, 0.06611047290898303, 0.13222094581796606, 0.06611047290898303, 0.06611047290898303, 0.1437085718715446, 0.0718542859357723, 0.2155628578073169, 0.0718542859357723, 0.1437085718715446, 0.1437085718715446, 0.0718542859357723, 0.0718542859357723, 0.15397462898440806, 0.24423561838906105, 0.14335568905444887, 0.09026098940465299, 0.12211780919453052, 0.07964204947469382, 0.06902310954473465, 0.042475759719836706, 0.03716628975485711, 0.015928409894938763, 0.21263259725069902, 0.12018364192430815, 0.12018364192430815, 0.12942853745694724, 0.09244895532639089, 0.10169385085902997, 0.06471426872847362, 0.11093874639166906, 0.036979582130556354, 0.009244895532639089, 0.39315325565294107, 0.2172803333528925, 0.15611994322393016, 0.1593389111254545, 0.11105439260258951, 0.10783542470106516, 0.077255229636584, 0.061160390128962336, 0.061160390128962336, 0.03540864691676767, 0.012875871606097334, 0.24919244768593818, 0.1012699901554726, 0.14678459247253892, 0.12857875154571238, 0.09899426003961928, 0.1012699901554726, 0.06030684807011289, 0.07054763359145282, 0.030722356564019775, 0.0136543806951199, 0.2652817893393314, 0.14469915782145348, 0.09646610521430231, 0.09646610521430231, 0.12058263151787789, 0.09646610521430231, 0.07234957891072674, 0.048233052607151154, 0.024116526303575577, 0.024116526303575577, 0.2969399606841983, 0.15836797903157243, 0.11029198539698794, 0.09049598801804139, 0.09049598801804139, 0.08201198914135001, 0.06787199101353103, 0.056559992511275865, 0.033935995506765515, 0.016967997753382758, 0.2205730653785928, 0.17777530642453748, 0.14485395338295648, 0.10205619442890115, 0.08888765321226874, 0.06255057077900393, 0.09876405912474305, 0.05267416486652963, 0.03621348834573912, 0.01646067652079051, 0.1851116285248467, 0.1648650441549416, 0.13015661380653284, 0.13883372139363503, 0.11280239863232845, 0.08098633747962043, 0.05495501471831386, 0.07520159908821897, 0.037600799544109484, 0.020246584369905108, 0.19921780729638447, 0.07968712291855379, 0.07968712291855379, 0.07968712291855379, 0.15937424583710758, 0.11953068437783068, 0.15937424583710758, 0.039843561459276894, 0.039843561459276894, 0.039843561459276894, 0.16009869802220897, 0.16009869802220897, 0.16009869802220897, 0.16009869802220897, 0.16009869802220897, 0.14983025476937098, 0.18728781846171372, 0.11237269107702823, 0.11237269107702823, 0.07491512738468549, 0.07491512738468549, 0.07491512738468549, 0.14983025476937098, 0.037457563692342745, 0.23975746182210755, 0.13700426389834716, 0.10275319792376038, 0.18838086286022734, 0.08562766493646698, 0.06850213194917358, 0.05137659896188019, 0.05137659896188019, 0.06850213194917358, 0.017125532987293395, 0.22332319494810188, 0.22332319494810188, 0.22332319494810188, 0.22332319494810188, 0.12757061508242207, 0.12757061508242207, 0.12757061508242207, 0.12757061508242207, 0.12757061508242207, 0.12757061508242207, 0.12757061508242207, 0.26077261055829587, 0.13038630527914793, 0.06519315263957397, 0.19557945791872192, 0.06519315263957397, 0.06519315263957397, 0.06519315263957397, 0.06519315263957397, 0.1304378579408452, 0.1304378579408452, 0.1304378579408452, 0.1304378579408452, 0.1304378579408452, 0.2608757158816904, 0.1304378579408452, 0.20212670380287215, 0.13475113586858142, 0.13475113586858142, 0.06737556793429071, 0.06737556793429071, 0.13475113586858142, 0.06737556793429071, 0.13475113586858142, 0.13552083751825267, 0.06776041875912633, 0.33880209379563164, 0.06776041875912633, 0.13552083751825267, 0.13552083751825267, 0.06776041875912633, 0.06776041875912633, 0.2219117622297357, 0.2219117622297357, 0.2219117622297357, 0.2219117622297357, 0.14779632202394483, 0.14779632202394483, 0.14779632202394483, 0.14779632202394483, 0.14779632202394483, 0.14779632202394483, 0.14779632202394483, 0.14779632202394483, 0.527954612697066, 0.20808586079093516, 0.1248515164745611, 0.1248515164745611, 0.08323434431637407, 0.16646868863274814, 0.08323434431637407, 0.1248515164745611, 0.041617172158187034, 0.041617172158187034, 0.041617172158187034, 0.1749486138577174, 0.11663240923847827, 0.11663240923847827, 0.11663240923847827, 0.058316204619239136, 0.1749486138577174, 0.058316204619239136, 0.11663240923847827, 0.058316204619239136, 0.2348502821680125, 0.2348502821680125, 0.2348502821680125, 0.2348502821680125, 0.2348502821680125, 0.17770310498926606, 0.15603199462472142, 0.14736355047890357, 0.11268977389563213, 0.13436088426017678, 0.07801599731236071, 0.07368177523945178, 0.07368177523945178, 0.03467377658327143, 0.013002666218726784, 0.21276919465212735, 0.20903640176349353, 0.1381133368794511, 0.07465585777267626, 0.1381133368794511, 0.0559918933295072, 0.0858542364385777, 0.04479351466360576, 0.02612955022043669, 0.01119837866590144, 0.22275099574937213, 0.21638668158510435, 0.1559256970245605, 0.10819334079255218, 0.07637176997121331, 0.07318961288907942, 0.060460984560543864, 0.04455019914987443, 0.028639413739204988, 0.01272862832853555, 0.2822415663739548, 0.2822415663739548, 0.283990006257468, 0.283990006257468, 0.283990006257468, 0.31099468150971377, 0.31099468150971377, 0.1328874105311584, 0.1328874105311584, 0.1328874105311584, 0.1328874105311584, 0.1328874105311584, 0.1328874105311584, 0.1328874105311584, 0.16289569476977864, 0.27149282461629776, 0.16289569476977864, 0.054298564923259546, 0.21719425969303818, 0.054298564923259546, 0.054298564923259546, 0.054298564923259546, 0.3827372015317589, 0.13917716419336687, 0.2087657462900503, 0.03479429104834172, 0.10438287314502515, 0.03479429104834172, 0.03479429104834172, 0.03479429104834172, 0.03479429104834172, 0.22570254344443608, 0.22570254344443608, 0.3021457120453607, 0.1781069460477916, 0.10495587892102005, 0.10495587892102005, 0.0826925106650461, 0.06679010476792185, 0.06042914240907215, 0.04134625533252305, 0.04134625533252305, 0.0190828870765491, 0.1649265602983217, 0.24189228843753852, 0.10995104019888115, 0.10995104019888115, 0.09895593617899302, 0.08796083215910491, 0.054975520099440574, 0.08796083215910491, 0.032985312059664346, 0.010995104019888114, 0.12436437664374564, 0.12436437664374564, 0.3730931299312369, 0.12436437664374564, 0.12436437664374564, 0.12436437664374564, 0.12436437664374564, 0.2507917955127565, 0.21003812874193356, 0.13166569264419717, 0.09091202587337423, 0.09718182076119314, 0.06269794887818912, 0.0501583591025513, 0.053293256546460756, 0.04075366677082293, 0.012539589775637825, 0.21307891023010184, 0.10653945511505092, 0.21307891023010184, 0.10653945511505092, 0.10653945511505092, 0.10653945511505092, 0.10653945511505092, 0.10653945511505092, 0.3479256747832582, 0.11804621108717689, 0.14911100347853923, 0.07455550173926961, 0.12425916956544936, 0.05591662630445221, 0.043490709347907275, 0.03727775086963481, 0.043490709347907275, 0.012425916956544936, 0.24360376581011256, 0.12180188290505628, 0.12180188290505628, 0.12180188290505628, 0.24360376581011256, 0.19461718743240652, 0.19461718743240652, 0.09730859371620326, 0.19461718743240652, 0.09730859371620326, 0.04865429685810163, 0.04865429685810163, 0.04865429685810163, 0.04865429685810163, 0.4892532972824192, 0.2446266486412096, 0.3667864054163398, 0.1555647909799618, 0.0777823954899809, 0.23334718646994268, 0.0777823954899809, 0.1555647909799618, 0.1555647909799618, 0.0777823954899809, 0.0777823954899809], \"Term\": [\"abc\", \"abc\", \"abc\", \"abc\", \"abc\", \"abc\", \"abc\", \"abc\", \"abc\", \"abc\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquired\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisition\", \"acquisitions\", \"acquisitions\", \"acquisitions\", \"acquisitions\", \"acquisitions\", \"acquisitions\", \"acquisitions\", \"acquisitions\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adaptive\", \"adds\", \"adds\", \"adds\", \"adds\", \"adds\", \"adds\", \"adds\", \"adds\", \"adds\", \"ade\", \"ade\", \"ade\", \"ade\", \"ade\", \"ade\", \"ade\", \"ade\", \"aff\", \"aff\", \"aff\", \"aff\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"alan\", \"alan\", \"alan\", \"alan\", \"alan\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"alk\", \"alk\", \"alk\", \"alk\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"ann\", \"ann\", \"ann\", \"ann\", \"ann\", \"ann\", \"ann\", \"ann\", \"ann\", \"ann\", \"annotated\", \"annotated\", \"annotated\", \"annotated\", \"annotated\", \"annotated\", \"annotated\", \"annotated\", \"annotations\", \"annotations\", \"annotations\", \"annotations\", \"annotations\", \"annotations\", \"annotations\", \"annotations\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"aoa\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arms\", \"arms\", \"arms\", \"arms\", \"arms\", \"arms\", \"arms\", \"arms\", \"arms\", \"arms\", \"asignificant\", \"asignificant\", \"asignificant\", \"asignificant\", \"asignificant\", \"asignificant\", \"asignificant\", \"asignificant\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"associated\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"attributes\", \"autoencoding\", \"autoencoding\", \"autoencoding\", \"autoencoding\", \"backup\", \"backup\", \"backup\", \"backup\", \"backup\", \"backup\", \"backup\", \"backup\", \"backup\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"basilar\", \"basilar\", \"basilar\", \"basilar\", \"basilar\", \"basilar\", \"basilar\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"battisti\", \"bci\", \"bci\", \"bci\", \"bci\", \"bci\", \"bci\", \"bci\", \"bci\", \"bci\", \"bci\", \"bcr\", \"bcr\", \"bcr\", \"bcr\", \"bcr\", \"bcr\", \"bcr\", \"bcr\", \"bcr\", \"bellman\", \"bellman\", \"bellman\", \"bellman\", \"bellman\", \"bellman\", \"bellman\", \"bellman\", \"bellman\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"bf\", \"bf\", \"bf\", \"bf\", \"bf\", \"bf\", \"bf\", \"bf\", \"bf\", \"bilmes\", \"bilmes\", \"bilmes\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"bin\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"bm\", \"bm\", \"bm\", \"bm\", \"bm\", \"bm\", \"bm\", \"bm\", \"bm\", \"bm\", \"bmb\", \"bmb\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bopp\", \"bopp\", \"bopp\", \"bopp\", \"bopp\", \"bopp\", \"bopp\", \"bopp\", \"bopp\", \"bopp\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"bru\", \"bubeck\", \"bubeck\", \"buffer\", \"buffer\", \"buffer\", \"buffer\", \"buffer\", \"buffer\", \"buffer\", \"buffer\", \"buffer\", \"buffer\", \"bumps\", \"bumps\", \"bumps\", \"bumps\", \"bumps\", \"bvcs\", \"bvcs\", \"bvcs\", \"bvcs\", \"bvcs\", \"bvcs\", \"bvcs\", \"bvcs\", \"bvcs\", \"cachan\", \"cachan\", \"cameras\", \"cameras\", \"cameras\", \"cameras\", \"caption\", \"caption\", \"caption\", \"caption\", \"caption\", \"caption\", \"caption\", \"caption\", \"caption\", \"captions\", \"captions\", \"captions\", \"captions\", \"captions\", \"captions\", \"captions\", \"captions\", \"captions\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"casp\", \"cbe\", \"cholesky\", \"cholesky\", \"cholesky\", \"cholesky\", \"cholesky\", \"cholesky\", \"cholesky\", \"cholesky\", \"cholesky\", \"cholesky\", \"chris\", \"chris\", \"chunk\", \"chunk\", \"chunk\", \"chunk\", \"chunk\", \"chunk\", \"chunk\", \"chunk\", \"chunk\", \"chunks\", \"chunks\", \"chunks\", \"chunks\", \"chunks\", \"chunks\", \"chunks\", \"chunks\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classication\", \"classier\", \"classier\", \"classier\", \"classier\", \"classier\", \"classier\", \"classier\", \"classier\", \"classier\", \"classif\", \"classif\", \"classif\", \"classif\", \"classif\", \"classif\", \"classif\", \"classif\", \"classif\", \"closure\", \"closure\", \"closure\", \"closure\", \"closure\", \"closure\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clutter\", \"clutter\", \"clutter\", \"clutter\", \"clutter\", \"clutter\", \"cma\", \"cma\", \"cma\", \"cma\", \"cma\", \"cma\", \"cma\", \"cma\", \"cma\", \"cma\", \"cml\", \"cml\", \"cml\", \"cml\", \"cml\", \"cml\", \"cml\", \"cml\", \"cochlea\", \"cochlea\", \"cochlea\", \"cochlea\", \"cochlea\", \"cochlea\", \"cochlea\", \"cochlea\", \"cochlea\", \"cochlea\", \"cochlear\", \"cochlear\", \"cochlear\", \"cochlear\", \"cochlear\", \"cochlear\", \"cochlear\", \"cochlear\", \"cochlear\", \"cochlear\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"compartment\", \"complicating\", \"complicating\", \"complicating\", \"complicating\", \"complicating\", \"complicating\", \"complicating\", \"complicating\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"computational\", \"condence\", \"condence\", \"condence\", \"condence\", \"connectionist\", \"connectionist\", \"connectionist\", \"connectionist\", \"connectionist\", \"connectionist\", \"connectionist\", \"connectionist\", \"connectionist\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"constraints\", \"constraints\", \"constraints\", \"constraints\", \"constraints\", \"constraints\", \"constraints\", \"constraints\", \"constraints\", \"constraints\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"copy\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"cursive\", \"cursive\", \"cursive\", \"cursive\", \"cx\", \"cx\", \"cx\", \"cx\", \"cx\", \"cx\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"cycle\", \"dag\", \"dag\", \"dag\", \"dag\", \"dag\", \"dag\", \"dag\", \"dag\", \"dag\", \"dag\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"datapoint\", \"datapoint\", \"datapoint\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"delays\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"depressing\", \"depressing\", \"depressing\", \"depressing\", \"depressing\", \"depressing\", \"depressing\", \"depressing\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"deprivation\", \"deprivation\", \"deprivation\", \"deprivation\", \"deprivation\", \"deprivation\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"descriptions\", \"descriptions\", \"descriptions\", \"descriptions\", \"descriptions\", \"descriptions\", \"descriptions\", \"descriptions\", \"descriptions\", \"dhhubel\", \"dhhubel\", \"dhhubel\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dietrich\", \"dietrich\", \"dietterich\", \"dietterich\", \"dietterich\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"disambiguate\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"disparity\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"dominance\", \"dominance\", \"dominance\", \"dominance\", \"dominance\", \"dominance\", \"dominance\", \"dominance\", \"dominance\", \"dominance\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dp\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dps\", \"dydx\", \"dydx\", \"dydx\", \"dydx\", \"dydx\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"dynamics\", \"eager\", \"eager\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"efferent\", \"efferent\", \"egi\", \"eigenface\", \"eigenface\", \"eigenface\", \"eigenface\", \"eigenface\", \"eigenface\", \"eigenface\", \"eigenface\", \"eigenface\", \"eigenspace\", \"eigenspace\", \"eigenspace\", \"eigenspace\", \"eigenspace\", \"eigenspace\", \"eigenspaces\", \"eigenspaces\", \"eigenspaces\", \"eigenspaces\", \"eigenspaces\", \"eigenspaces\", \"eigenspaces\", \"eigenspaces\", \"eigenspaces\", \"eigenvalues\", \"eigenvalues\", \"eigenvalues\", \"eigenvalues\", \"eigenvalues\", \"eigenvalues\", \"eigenvalues\", \"eigenvalues\", \"eigenvalues\", \"eigenvalues\", \"eki\", \"eki\", \"eki\", \"eki\", \"eki\", \"eki\", \"electron\", \"electron\", \"electron\", \"electron\", \"electron\", \"electron\", \"electron\", \"electron\", \"electron\", \"electron\", \"ellipses\", \"ellipses\", \"ellipses\", \"elvgren\", \"elvgren\", \"elvgren\", \"elvgren\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"empirical\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"engle\", \"engle\", \"engle\", \"engle\", \"err\", \"err\", \"err\", \"err\", \"err\", \"err\", \"err\", \"err\", \"err\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"execution\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"fampi\", \"fampi\", \"fampi\", \"fampi\", \"fampi\", \"fampi\", \"fampi\", \"fcp\", \"fcp\", \"fcp\", \"fcp\", \"fcp\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"fi\", \"fi\", \"fi\", \"fi\", \"fi\", \"fi\", \"fi\", \"fi\", \"fi\", \"fi\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"fires\", \"fires\", \"fires\", \"fires\", \"fires\", \"fires\", \"fires\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firings\", \"firings\", \"firings\", \"firings\", \"firings\", \"firings\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fisherface\", \"fisherface\", \"fisherface\", \"fisherface\", \"fisherface\", \"fisherface\", \"fisherface\", \"fisherface\", \"fisherface\", \"fld\", \"fld\", \"fld\", \"fld\", \"fld\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"flexible\", \"floating\", \"floating\", \"floating\", \"floating\", \"floating\", \"floating\", \"floating\", \"floating\", \"fluid\", \"fluid\", \"fluid\", \"fluid\", \"fluid\", \"fluid\", \"fluid\", \"fluid\", \"fluid\", \"fluid\", \"fohc\", \"fohc\", \"fohc\", \"fohc\", \"fohc\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"formulation\", \"fragments\", \"fragments\", \"fragments\", \"fragments\", \"fragments\", \"fragments\", \"fragments\", \"fragments\", \"fragments\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"garch\", \"garch\", \"garch\", \"garch\", \"garch\", \"garch\", \"garch\", \"garch\", \"garch\", \"gate\", \"gate\", \"gate\", \"gate\", \"gate\", \"gate\", \"gate\", \"gate\", \"gate\", \"gate\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gerhand\", \"gerhand\", \"gerhand\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"graphs\", \"gridded\", \"gridded\", \"gridded\", \"gridded\", \"gridded\", \"gridded\", \"gyrus\", \"gyrus\", \"gyrus\", \"haar\", \"haar\", \"haar\", \"haar\", \"haar\", \"haar\", \"haar\", \"haar\", \"haar\", \"haar\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hdbl\", \"heard\", \"hemmen\", \"hemmen\", \"hemmen\", \"hemmen\", \"hemmen\", \"hemmen\", \"hemmen\", \"herz\", \"herz\", \"herz\", \"herz\", \"herz\", \"herz\", \"herz\", \"herz\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hierarchy\", \"hitting\", \"hitting\", \"hitting\", \"hitting\", \"hitting\", \"hitting\", \"hitting\", \"hitting\", \"hiv\", \"hiv\", \"hiv\", \"hiv\", \"hji\", \"hji\", \"hji\", \"hji\", \"hji\", \"hji\", \"hk\", \"hk\", \"hk\", \"hk\", \"hk\", \"hk\", \"hk\", \"hk\", \"hk\", \"hk\", \"hkh\", \"hkhzl\", \"hkhzl\", \"hop\", \"hop\", \"hop\", \"hop\", \"hop\", \"hop\", \"hop\", \"hop\", \"hopfield\", \"hopfield\", \"hopfield\", \"hopfield\", \"hopfield\", \"hopfield\", \"hopfield\", \"hopfield\", \"hopfield\", \"hopfield\", \"hops\", \"hops\", \"hops\", \"hops\", \"hops\", \"hops\", \"hops\", \"hops\", \"hops\", \"hourglass\", \"hourglass\", \"hourglass\", \"hourglass\", \"hourglass\", \"hourglass\", \"hubel\", \"hubel\", \"hubel\", \"hxi\", \"hxi\", \"iac\", \"iac\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"ii\", \"iin\", \"iin\", \"iin\", \"iin\", \"iin\", \"iin\", \"iin\", \"iin\", \"iin\", \"iin\", \"ilk\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"imem\", \"imem\", \"imem\", \"imem\", \"imem\", \"imem\", \"imem\", \"imem\", \"imem\", \"independences\", \"independences\", \"independences\", \"independences\", \"independences\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"ininin\", \"ininin\", \"ininin\", \"initializing\", \"initializing\", \"initializing\", \"initializing\", \"initializing\", \"initializing\", \"initializing\", \"initializing\", \"initializing\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"interspike\", \"inv\", \"inv\", \"iout\", \"iout\", \"iout\", \"iout\", \"iout\", \"iout\", \"iout\", \"iout\", \"iout\", \"iout\", \"ipsilateral\", \"ipsilateral\", \"ipsilateral\", \"ipsilateral\", \"ipsilateral\", \"ipw\", \"ipw\", \"ipw\", \"ipw\", \"ipw\", \"ipw\", \"ipw\", \"ipw\", \"ipw\", \"isb\", \"isb\", \"isb\", \"isb\", \"isomap\", \"isomap\", \"isomap\", \"isomap\", \"isomap\", \"isomap\", \"isomap\", \"isomap\", \"isomap\", \"isometry\", \"isometry\", \"isometry\", \"isometry\", \"isometry\", \"isometry\", \"isometry\", \"isometry\", \"isometry\", \"ji\", \"ji\", \"ji\", \"ji\", \"ji\", \"ji\", \"ji\", \"ji\", \"ji\", \"jji\", \"kalman\", \"kalman\", \"kalman\", \"kalman\", \"kalman\", \"kalman\", \"kalman\", \"kalman\", \"kalman\", \"kbk\", \"kbk\", \"kbk\", \"kbk\", \"kbk\", \"kbk\", \"kbk\", \"kbk\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kingdom\", \"kingdom\", \"kingdom\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpca\", \"kpf\", \"kpf\", \"kpf\", \"kpf\", \"kpf\", \"kpf\", \"kpf\", \"kpf\", \"kpf\", \"kxk\", \"kxk\", \"kxk\", \"kxk\", \"kxk\", \"kxk\", \"kxk\", \"kxk\", \"kxk\", \"kyk\", \"lambon\", \"lambon\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"latency\", \"lateral\", \"lateral\", \"lateral\", \"lateral\", \"lateral\", \"lateral\", \"lateral\", \"lateral\", \"lateral\", \"lazy\", \"lazy\", \"lazy\", \"lazy\", \"lazy\", \"lazy\", \"lazy\", \"lbcr\", \"lbcr\", \"lbcr\", \"lbcr\", \"lbcr\", \"lbcr\", \"lbcr\", \"lbcr\", \"lbcr\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"least\", \"leigs\", \"leigs\", \"leigs\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"lett\", \"lett\", \"lett\", \"lett\", \"lett\", \"lett\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"leverage\", \"leverage\", \"leverage\", \"leverage\", \"leverage\", \"leverage\", \"leverage\", \"leverage\", \"leverage\", \"leverage\", \"lexical\", \"lexical\", \"lightweight\", \"lightweight\", \"lightweight\", \"lightweight\", \"lightweight\", \"lightweight\", \"lightweight\", \"lightweight\", \"lightweight\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"lipschitz\", \"lipschitz\", \"lipschitz\", \"lipschitz\", \"lipschitz\", \"lipschitz\", \"lipschitz\", \"ljt\", \"ljt\", \"ljt\", \"ljt\", \"ljt\", \"lmc\", \"lmc\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"lpf\", \"lpf\", \"lpf\", \"lpf\", \"lpf\", \"lpf\", \"lpf\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"lsbp\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"magleby\", \"mailxidianeducn\", \"mailxidianeducn\", \"mailxidianeducn\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifolds\", \"manifolds\", \"manifolds\", \"manifolds\", \"manifolds\", \"manifolds\", \"manifolds\", \"matched\", \"matched\", \"matched\", \"matched\", \"matched\", \"matched\", \"matched\", \"matched\", \"matched\", \"matched\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matsuura\", \"matsuura\", \"matsuura\", \"matsuura\", \"maximin\", \"maximin\", \"maximin\", \"maximin\", \"maximin\", \"maximin\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"memorability\", \"memorability\", \"memorability\", \"memorability\", \"memorability\", \"memorability\", \"memorability\", \"memorability\", \"memorability\", \"memorability\", \"memorable\", \"memorable\", \"memorable\", \"memorable\", \"memorable\", \"memorable\", \"memorable\", \"memorable\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"messages\", \"messages\", \"messages\", \"messages\", \"messages\", \"messages\", \"messages\", \"messages\", \"messages\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"microscopic\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"minimax\", \"missing\", \"missing\", \"missing\", \"missing\", \"missing\", \"missing\", \"missing\", \"missing\", \"missing\", \"missing\", \"missingness\", \"missingness\", \"missingness\", \"missingness\", \"missingness\", \"missingness\", \"missingness\", \"missingness\", \"missingness\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mnar\", \"mnar\", \"mnar\", \"mnar\", \"mnar\", \"mnar\", \"mnar\", \"mnar\", \"mnar\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"momentum\", \"momentum\", \"momentum\", \"momentum\", \"momentum\", \"momentum\", \"momentum\", \"momentum\", \"monocular\", \"monocular\", \"monocular\", \"monocular\", \"monocular\", \"monocular\", \"monocular\", \"monocular\", \"moore\", \"moore\", \"moore\", \"moore\", \"moore\", \"moore\", \"moore\", \"morrison\", \"morrison\", \"morrison\", \"morrison\", \"morrison\", \"morrison\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"motor\", \"mra\", \"mra\", \"mra\", \"mra\", \"mra\", \"mra\", \"mra\", \"mra\", \"msrc\", \"msrc\", \"msrc\", \"msrc\", \"msrc\", \"msrc\", \"msrc\", \"msrc\", \"mt\", \"mt\", \"mt\", \"mt\", \"mt\", \"mt\", \"mt\", \"mt\", \"mt\", \"mt\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mtl\", \"mutate\", \"mutate\", \"mutate\", \"nanocrystalline\", \"nanocrystalline\", \"nanostructure\", \"nanostructure\", \"nanostructure\", \"nanostructure\", \"nanostructure\", \"nanostructure\", \"nanostructure\", \"nanostructure\", \"neill\", \"neill\", \"neill\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"next\", \"next\", \"next\", \"next\", \"next\", \"next\", \"next\", \"next\", \"next\", \"next\", \"ngp\", \"ngp\", \"ngp\", \"ngp\", \"ngp\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"nwa\", \"nwa\", \"nwa\", \"nwa\", \"nwa\", \"nwa\", \"nwa\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"observe\", \"observe\", \"observe\", \"observe\", \"observe\", \"observe\", \"observe\", \"observe\", \"observe\", \"observe\", \"ocular\", \"ocular\", \"ocular\", \"ocular\", \"ocular\", \"ocular\", \"ocular\", \"ocular\", \"ocular\", \"offs\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"ohcs\", \"ohcs\", \"ohcs\", \"ohcs\", \"ohcs\", \"ojj\", \"ojj\", \"ojj\", \"ojj\", \"ojj\", \"oldfield\", \"oli\", \"oli\", \"oli\", \"oli\", \"oli\", \"oliva\", \"oliva\", \"oliva\", \"oliva\", \"oliva\", \"oliva\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"opi\", \"opi\", \"opi\", \"opi\", \"opi\", \"opi\", \"opi\", \"opi\", \"opi\", \"opi\", \"opponents\", \"opponents\", \"opponents\", \"opponents\", \"opponents\", \"opponents\", \"opponents\", \"opponents\", \"opponents\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimizer\", \"optimizer\", \"optimizer\", \"optimizer\", \"optimizer\", \"optimizer\", \"optimizer\", \"optimizer\", \"optimizer\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"orlin\", \"orlin\", \"orlin\", \"orlin\", \"ornstein\", \"oscillates\", \"ou\", \"ou\", \"ou\", \"ou\", \"ou\", \"ou\", \"ou\", \"ou\", \"ou\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"overtures\", \"overtures\", \"overtures\", \"overtures\", \"overtures\", \"overtures\", \"padua\", \"padua\", \"padua\", \"padua\", \"paige\", \"paige\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"partners\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"patterns\", \"peretto\", \"peretto\", \"peretto\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"photo\", \"photo\", \"photo\", \"photo\", \"photo\", \"photo\", \"photo\", \"photo\", \"picking\", \"picking\", \"picking\", \"picking\", \"picking\", \"picking\", \"picking\", \"picking\", \"pictures\", \"pictures\", \"pictures\", \"pictures\", \"pictures\", \"pictures\", \"pictures\", \"pictures\", \"pis\", \"pis\", \"pis\", \"pis\", \"pis\", \"pis\", \"pis\", \"pis\", \"pis\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plastic\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"plasticity\", \"players\", \"players\", \"players\", \"players\", \"plds\", \"plds\", \"plds\", \"plds\", \"plds\", \"plds\", \"plds\", \"plds\", \"plds\", \"plds\", \"plots\", \"plots\", \"plots\", \"plots\", \"plots\", \"plots\", \"plots\", \"plots\", \"plots\", \"plots\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"polytope\", \"polytope\", \"polytope\", \"polytope\", \"polytope\", \"polytope\", \"polytope\", \"polytope\", \"polytope\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"potts\", \"potts\", \"potts\", \"pq\", \"pq\", \"pq\", \"pq\", \"pq\", \"pq\", \"pq\", \"pq\", \"pq\", \"pq\", \"practically\", \"practically\", \"practically\", \"practically\", \"prec\", \"prec\", \"prec\", \"prec\", \"prec\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"prij\", \"prij\", \"prij\", \"prij\", \"prij\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"problems\", \"probs\", \"probs\", \"probs\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"projector\", \"projector\", \"projector\", \"ps\", \"ps\", \"ps\", \"ps\", \"ps\", \"ps\", \"ps\", \"ps\", \"ps\", \"psbd\", \"psbd\", \"psbd\", \"psbd\", \"psbd\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"pulls\", \"push\", \"push\", \"push\", \"push\", \"push\", \"push\", \"push\", \"push\", \"push\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"q_\", \"q_\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"qi\", \"quantum\", \"quantum\", \"quantum\", \"quantum\", \"quantum\", \"quantum\", \"quantum\", \"quoted\", \"quoted\", \"quoted\", \"quoted\", \"quoted\", \"quoted\", \"qv\", \"qv\", \"qv\", \"qv\", \"qv\", \"qv\", \"qv\", \"qv\", \"qv\", \"ralph\", \"ralph\", \"ralph\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"randomness\", \"randomness\", \"randomness\", \"randomness\", \"randomness\", \"randomness\", \"randomness\", \"randomness\", \"randomness\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"reichardt\", \"reichardt\", \"reichardt\", \"reichardt\", \"reichardt\", \"reichardt\", \"reinf\", \"reinf\", \"reinf\", \"reinf\", \"reinforce\", \"reinforce\", \"reinforce\", \"reinforce\", \"reinforce\", \"reinforce\", \"reinforce\", \"reinforce\", \"release\", \"release\", \"release\", \"release\", \"release\", \"release\", \"release\", \"release\", \"release\", \"release\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"remix\", \"renumber\", \"renumber\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"restless\", \"restless\", \"restless\", \"restless\", \"restless\", \"restless\", \"restless\", \"restless\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"rfs\", \"rfs\", \"rfs\", \"rfs\", \"rfs\", \"rfs\", \"rfs\", \"rfs\", \"rgcs\", \"rgcs\", \"rgcs\", \"rgcs\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"riemannian\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rostamizadeh\", \"rostamizadeh\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"sabes\", \"sabes\", \"sabes\", \"sabes\", \"sabes\", \"saga\", \"saga\", \"saga\", \"saga\", \"saga\", \"saga\", \"saga\", \"saga\", \"saga\", \"saga\", \"sailing\", \"sailing\", \"sailing\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"saliency\", \"sbd\", \"sbd\", \"sbd\", \"sbd\", \"sbd\", \"sbd\", \"sbd\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scan\", \"scarpa\", \"scattering\", \"scattering\", \"scattering\", \"scattering\", \"scattering\", \"scattering\", \"scattering\", \"scattering\", \"scattering\", \"scattering\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scg\", \"scg\", \"scg\", \"scg\", \"scg\", \"scg\", \"scg\", \"scg\", \"scg\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"secretary\", \"secretary\", \"secretary\", \"secretary\", \"secretary\", \"secretary\", \"secretary\", \"secretary\", \"secretary\", \"secretary\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seg\", \"seg\", \"seg\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sethuraman\", \"sethuraman\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sharpen\", \"sharpen\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shrinkage\", \"shrinkage\", \"shrinkage\", \"shrinkage\", \"shrinkage\", \"shrinkage\", \"sia\", \"sia\", \"sia\", \"sia\", \"sia\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"spelling\", \"spelling\", \"spelling\", \"spelling\", \"spelling\", \"spelling\", \"spelling\", \"spelling\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spoken\", \"spoken\", \"spoken\", \"spoken\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"sse\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistic\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"stefanie\", \"stefanie\", \"stefanie\", \"stefanie\", \"sticki\", \"stickk\", \"stp\", \"stp\", \"stp\", \"stp\", \"stream\", \"stream\", \"stream\", \"stream\", \"stream\", \"stream\", \"stream\", \"stream\", \"stream\", \"stream\", \"stripe\", \"stripe\", \"stripe\", \"stripe\", \"stripe\", \"stripe\", \"suboptimality\", \"suboptimality\", \"suboptimality\", \"suboptimality\", \"suboptimality\", \"suboptimality\", \"suboptimality\", \"suboptimality\", \"summable\", \"summable\", \"summable\", \"summable\", \"supu\", \"supu\", \"supu\", \"svrg\", \"svrg\", \"svrg\", \"svrg\", \"svrg\", \"svrg\", \"svrg\", \"svrg\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"talks\", \"talks\", \"talks\", \"tanaka\", \"tanaka\", \"tanaka\", \"tanaka\", \"tanaka\", \"tanaka\", \"tanaka\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"temperature\", \"template\", \"template\", \"template\", \"template\", \"template\", \"template\", \"template\", \"template\", \"templates\", \"templates\", \"templates\", \"templates\", \"templates\", \"templates\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"testable\", \"testable\", \"testable\", \"testable\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"thermal\", \"thermal\", \"thermal\", \"thermal\", \"thermal\", \"thermal\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"thus\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"ti\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timepoints\", \"timepoints\", \"timepoints\", \"timepoints\", \"timepoints\", \"tnwiesel\", \"tnwiesel\", \"tpe\", \"tpe\", \"tpe\", \"tpe\", \"tpe\", \"tpe\", \"tpe\", \"tpe\", \"tpe\", \"tps\", \"tps\", \"tps\", \"tps\", \"tps\", \"tracked\", \"tracked\", \"tracked\", \"tracked\", \"tracked\", \"tracked\", \"tracked\", \"tracked\", \"tracked\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"trains\", \"trains\", \"trains\", \"trains\", \"trains\", \"trains\", \"trains\", \"trains\", \"trains\", \"trer\", \"trer\", \"trer\", \"trer\", \"trer\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trials\", \"trials\", \"trials\", \"trials\", \"trials\", \"trials\", \"trials\", \"trials\", \"trials\", \"trials\", \"tuneable\", \"tuneable\", \"tuneable\", \"tunneling\", \"tunneling\", \"tunneling\", \"tunneling\", \"tunneling\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"types\", \"types\", \"types\", \"types\", \"types\", \"types\", \"types\", \"types\", \"types\", \"types\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"uij\", \"uij\", \"uij\", \"uij\", \"uij\", \"uij\", \"uij\", \"uij\", \"uiuc\", \"uiuc\", \"uiuc\", \"uiuc\", \"uiuc\", \"uiuc\", \"uiuc\", \"uiuc\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"ups\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"vesicle\", \"vesicle\", \"vesicle\", \"vesicle\", \"vesicle\", \"vgpds\", \"vgpds\", \"vgpds\", \"vgpds\", \"vgpds\", \"vgpds\", \"vgpds\", \"vgpds\", \"vgpds\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"violated\", \"violated\", \"violated\", \"violated\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"vjn\", \"vjn\", \"vjn\", \"vjn\", \"vjn\", \"vjn\", \"vjn\", \"vjn\", \"vl\", \"vl\", \"vl\", \"vl\", \"vl\", \"vl\", \"vl\", \"vmf\", \"vmf\", \"vmf\", \"vmf\", \"vmf\", \"vmf\", \"vmf\", \"vmf\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"vmt\", \"volatility\", \"volatility\", \"volatility\", \"volatility\", \"vp\", \"vp\", \"vp\", \"vp\", \"vp\", \"vp\", \"vp\", \"vp\", \"vsconsistency\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"war\", \"wavelength\", \"wavelength\", \"wavelength\", \"wavelength\", \"wavelength\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wettschereck\", \"wettschereck\", \"wheeler\", \"wheeler\", \"wheeler\", \"wiesel\", \"wiesel\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"wji\", \"wji\", \"wji\", \"wji\", \"wji\", \"wji\", \"wji\", \"wji\", \"wolfes\", \"wolfes\", \"wolfes\", \"wolfes\", \"wolfes\", \"wolfes\", \"wolfes\", \"wolfes\", \"wolfes\", \"women\", \"women\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"wzm\", \"wzm\", \"wzm\", \"wzm\", \"wzm\", \"wzm\", \"wzm\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xml\", \"xml\", \"xml\", \"xml\", \"xml\", \"xml\", \"xml\", \"xml\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"yale\", \"yale\", \"yale\", \"yale\", \"yale\", \"ylx\", \"ylx\", \"ylx\", \"ylx\", \"ylx\", \"ylx\", \"ylx\", \"ylx\", \"ylx\", \"yxi\", \"yxi\", \"zippelius\", \"zm\", \"zm\", \"zm\", \"zm\", \"zm\", \"zm\", \"zm\", \"zm\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [9, 10, 2, 7, 1, 4, 3, 8, 6, 5]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el272140274952825520569640130\", ldavis_el272140274952825520569640130_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el272140274952825520569640130\", ldavis_el272140274952825520569640130_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el272140274952825520569640130\", ldavis_el272140274952825520569640130_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4oq1FD25Jsc"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "Machine learning has become increasingly popular over the past decade, and recent advances in computational availability have led to exponential growth to people looking for ways how new methods can be incorporated to advance the field of Natural Language Processing.\n",
        "\n",
        "Often, we treat topic models as black-box algorithms, but hopefully, this article addressed to shed light on the underlying math, and intuitions behind it, and high-level code to get you started with any textual data.\n",
        "\n",
        "In the next article, we’ll go one step deeper into understanding how you can evaluate the performance of topic models, tune its hyper-parameters to get more intuitive and reliable results.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. Topic model — Wikipedia. https://en.wikipedia.org/wiki/Topic_model\n",
        "2. Distributed Strategies for Topic Modeling. https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&isAllowed=y\n",
        "3. Topic Mapping — Software — Resources — Amaral Lab. https://amaral.northwestern.edu/resources/software/topic-mapping\n",
        "4. A Survey of Topic Modeling in Text Mining. https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}