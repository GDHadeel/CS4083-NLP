{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5QhPxTx2d6Y"
      },
      "source": [
        "### Evaluate Topic Model in Python: Latent Dirichlet Allocation (LDA)\\\n",
        "##### A step-by-step guide to building interpretable topic models\n",
        "\n",
        "** **\n",
        "*Preface: This article aims to provide consolidated information on the underlying topic and is not to be considered as the original work. The information and the code are repurposed through several online articles, research papers, books, and open-source code*\n",
        "** **\n",
        "\n",
        "In the previous [article](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0), I introduced the concept of topic modeling and walked through the code for developing your first topic model using Latent Dirichlet Allocation (LDA) method in the python using Gensim implementation.\n",
        "\n",
        "Pursuing on that understanding, in this article, we’ll go a few steps deeper by outlining the framework to quantitatively evaluate topic models through the measure of topic coherence and share the code template in python using Gensim implementation to allow for end-to-end model development.\n",
        "\n",
        "### Why evaluate topic models?\n",
        "\n",
        "![img](https://tinyurl.com/y3xznjwq)\n",
        "\n",
        "We know probabilistic topic models, such as LDA, are popular tools for text analysis, providing both a predictive and latent topic representation of the corpus. However, there is a longstanding assumption that the latent space discovered by these models is generally meaningful and useful, and that evaluating such assumptions is challenging due to its unsupervised training process. Besides, there is a no-gold standard list of topics to compare against every corpus.\n",
        "\n",
        "Nevertheless, it is equally important to identify if a trained model is objectively good or bad, as well have an ability to compare different models/methods. To do so, one would require an objective measure for the quality. Traditionally, and still for many practical applications, to evaluate if “the correct thing” has been learned about the corpus, an implicit knowledge and “eyeballing” approaches are used. Ideally, we’d like to capture this information in a single metric that can be maximized, and compared.\n",
        "\n",
        "Let’s take a look at roughly what approaches are commonly used for the evaluation:\n",
        "\n",
        "**Eye Balling Models**\n",
        "- Top N words\n",
        "- Topics / Documents\n",
        "\n",
        "**Intrinsic Evaluation Metrics**\n",
        "- Capturing model semantics\n",
        "- Topics interpretability\n",
        "\n",
        "**Human Judgements**\n",
        "- What is a topic\n",
        "\n",
        "**Extrinsic Evaluation Metrics/Evaluation at task**\n",
        "- Is model good at performing predefined tasks, such as classification\n",
        "\n",
        "Natural language is messy, ambiguous and full of subjective interpretation, and sometimes trying to cleanse ambiguity reduces the language to an unnatural form. In this article, we’ll explore more about topic coherence, an intrinsic evaluation metric, and how you can use it to quantitatively justify the model selection.\n",
        "\n",
        "### What is Topic Coherence?\n",
        "\n",
        "Before we understand topic coherence, let’s briefly look at the perplexity measure. Perplexity as well is one of the intrinsic evaluation metric, and is widely used for language model evaluation. It captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set.\n",
        "\n",
        "Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data.\n",
        "\n",
        "However, recent studies have shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated.\n",
        "\n",
        "*Optimizing for perplexity may not yield human interpretable topics*\n",
        "\n",
        "This limitation of perplexity measure served as a motivation for more work trying to model the human judgment, and thus *Topic Coherence*.\n",
        "\n",
        "The concept of topic coherence combines a number of measures into a framework to evaluate the coherence between topics inferred by a model. But before that…\n",
        "\n",
        "#### What is topic coherence?\n",
        "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But,\n",
        "\n",
        "#### What is coherence?\n",
        "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But …\n",
        "\n",
        "### Coherence Measures\n",
        "Let’s take quick look at different coherence measures, and how they are calculated:\n",
        "\n",
        "1. `C_v` measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
        "2. `C_p` is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson's coherence\n",
        "3. `C_uci` measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words\n",
        "4. `C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure\n",
        "5. `C_npmi` is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI)\n",
        "6. `C_a` is based on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
        "\n",
        "There is, of course, a lot more to the concept of topic model evaluation, and the coherence measure. However, keeping in mind the length, and purpose of this article, let’s apply these concepts into developing a model that is at least better than with the default parameters. Also, we’ll be re-purposing already available online pieces of code to support this exercise instead of re-inventing the wheel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4EXTnob2d6c"
      },
      "source": [
        "### Model Implementation\n",
        "1. Loading Data\n",
        "2. Data Cleaning\n",
        "3. Phrase Modeling: Bi-grams and Tri-grams\n",
        "4. Data Transformation: Corpus and Dictionary\n",
        "5. Base Model\n",
        "6. Hyper-parameter Tuning\n",
        "7. Final model\n",
        "8. Visualize Results\n",
        "\n",
        "** **\n",
        "\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file\n",
        "\n",
        "** **\n",
        "#### Step 1: Loading Data\n",
        "** **\n",
        "\n",
        "For this tutorial, we’ll use the dataset of papers published in NIPS conference. The NIPS conference (Neural Information Processing Systems) is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "CX36HSbj2d6d",
        "outputId": "df3a41db-5707-4b9d-df4b-dfd6056d6c23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9a6c2f2-ac67-4344-9a84-329464c8c37b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9a6c2f2-ac67-4344-9a84-329464c8c37b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9a6c2f2-ac67-4344-9a84-329464c8c37b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9a6c2f2-ac67-4344-9a84-329464c8c37b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ceff10ad-d51d-40f0-b64d-141e58252d4c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ceff10ad-d51d-40f0-b64d-141e58252d4c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ceff10ad-d51d-40f0-b64d-141e58252d4c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h-j0s5c2d6e"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VzNHfrZE2d6f",
        "outputId": "fd44851b-5d14-4e31-87fb-7bab622df103"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             paper_text\n",
              "5742  Optimal Cluster Recovery\\nin the Labeled Stoch...\n",
              "2948  Replacing supervised classification learning b...\n",
              "3444  b-Bit Minwise Hashing for Estimating Three-Way...\n",
              "3719  Facial Expression Transfer with Input-Output\\n...\n",
              "2990  Associative Memory in a Network of 'biological..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f4079ac-92b0-4935-aeb8-fc792b3c8717\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5742</th>\n",
              "      <td>Optimal Cluster Recovery\\nin the Labeled Stoch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2948</th>\n",
              "      <td>Replacing supervised classification learning b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3444</th>\n",
              "      <td>b-Bit Minwise Hashing for Estimating Three-Way...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3719</th>\n",
              "      <td>Facial Expression Transfer with Input-Output\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2990</th>\n",
              "      <td>Associative Memory in a Network of 'biological...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f4079ac-92b0-4935-aeb8-fc792b3c8717')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0f4079ac-92b0-4935-aeb8-fc792b3c8717 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0f4079ac-92b0-4935-aeb8-fc792b3c8717');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dece1728-d91d-49a4-84b9-13cec6cbb3af\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dece1728-d91d-49a4-84b9-13cec6cbb3af')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dece1728-d91d-49a4-84b9-13cec6cbb3af button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Convolution Kernels for Natural Language\\n\\nMichael Collins\\nAT&T Labs?Research\\n180 Park Avenue, New Jersey, NJ 07932\\nmcollins@research.att.com\\n\\nNigel Duffy\\nDepartment of Computer Science\\nUniversity of California at Santa Cruz\\nnigeduff@cse.ucsc.edu\\n\\nAbstract\\nWe describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled\\nare strings, trees, graphs or other discrete structures which require some\\nmechanism to convert them into feature vectors. We describe kernels for\\nvarious natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can\\nbe applied to parsing using the voted perceptron algorithm, and we give\\nexperimental results on the ATIS corpus of parse trees.\\n\\n1 Introduction\\nKernel methods have been widely used to extend the applicability of many well-known algorithms, such as the Perceptron [1], Support Vector Machines [6], or Principal Component\\nAnalysis [15]. A key property of these algorithms is that the only operation they require\\nis the evaluation of dot products between pairs of examples. One may therefore replace\\n\\u0002\\u0001 into a new\\nthe dot\\u0004\\u0003product with a Mercer kernel, implicitly mapping feature vectors in\\nspace\\n, and applying the original algorithm in this new feature space. Kernels provide\\nan efficient way to carry out these calculations when \\u0005 is large or even infinite.\\nThis paper describes the application of kernel methods to Natural Language Processing\\n(NLP) problems. In many NLP tasks the input domain cannot be neatly formulated as a sub\\u0001\\nset of . Instead, the objects being modeled are strings, trees or other discrete structures\\nwhich require some mechanism to convert them into feature vectors. We describe kernels\\nfor various NLP structures, and show that they allow computationally feasible representations in very high dimensional feature spaces, for example a parse tree representation that\\ntracks all subtrees. We show how a tree kernel can be applied to parsing using the perceptron algorithm, giving experimental results on the ATIS corpus of parses. The kernels we\\ndescribe are instances of ?Convolution Kernels?, which were introduced by Haussler [10]\\nand Watkins [16], and which involve a recursive calculation over the ?parts? of a discrete\\nstructure. Although we concentrate on NLP tasks in this paper, the kernels should also be\\nuseful in computational biology, which shares similar problems and structures.\\n1.1 Natural Language Tasks\\nFigure 1 shows some typical structures from NLP tasks. Each structure involves an ?observed? string (a sentence), and some hidden structure (an underlying state sequence or\\ntree). We assume that there is some training set of structures, and that the task is to learn\\n\\n\\fa) Lou Gerstner is chairman of IBM\\n[S [NP Lou Gerstner ] [VP is [NP chairman [PP of [NP IBM ] ] ] ] ]\\nb) Lou Gerstner is chairman of IBM Lou/SP Gerstner/CP is/N chairman/N of/N IBM/SC\\nc) Lou/N Gerstner/N is/V chairman/N of/P IBM/N\\nFigure 1: Three NLP tasks where a function is learned from a string to some hidden structure. In (a), the hidden structure is a parse tree. In (b), the hidden structure is an underlying sequence of states representing named entity boundaries (SP = Start person, CP =\\nContinue person, SC = Start company, N= No entity). In (c), the hidden states represent\\npart-of-speech tags (N = noun, V = verb, P = preposition,).\\nthe mapping from an input string to its hidden structure. We refer to tasks that involve trees\\nas parsing problems, and tasks that involve hidden state sequences as tagging problems.\\nIn many of these problems ambiguity is the key issue: although only one analysis is plausible, there may be very many possible analyses. A common way to deal with ambiguity\\nis to use a stochastic grammar, for example a Probabilistic Context Free Grammar (PCFG)\\nfor parsing, or a Hidden Markov Model (HMM) for tagging. Probabilities are attached to\\nrules in the grammar ? context-free rules in the case of PCFGs, state transition probabilities and state emission probabilities for HMMs. Rule probabilities are typically estimated\\nusing maximum likelihood estimation, which gives simple relative frequency estimates.\\nCompeting analyses for the same sentence are ranked using these probabilities. See [3] for\\nan introduction to these methods.\\nThis paper proposes an alternative to generative models such as PCFGs and HMMs. Instead\\nof identifying parameters with rules of the grammar, we show how kernels can be used to\\nform representations that are sensitive to larger sub-structures of trees or state sequences.\\nThe parameter estimation methods we describe are discriminative, optimizing a criterion\\nthat is directly related to error rate.\\nWhile we use the parsing problem as a running example in this paper, kernels over NLP\\nstructures could be used in many ways: for example, in PCA over discrete structures, or\\nin classification and regression problems. Structured objects such as parse trees are so\\nprevalent in NLP that convolution kernels should have many applications.\\n\\n2 A Tree Kernel\\nThe previous section introduced PCFGs as a parsing method. This approach essentially\\ncounts the relative number of occurences of a given rule in the training data and uses these\\ncounts to represent its learned knowledge. PCFGs make some fairly strong independence\\nassumptions, disregarding substantial amounts of structural information. In particular, it\\ndoes not appear reasonable to assume that the rules applied at level \\u0001 in the parse tree are\\nunrelated to those applied at level \\u0001\\u0003\\u0002\\u0005\\u0004 .\\nAs an alternative we attempt to capture considerably more structural information by considering all tree fragments that occur in a parse tree. This allows us to capture higher order\\ndependencies between grammar rules. See figure 2 for an example. As in a PCFG the new\\nrepresentation tracks the counts of single rules, but it is also sensitive to larger sub-trees.\\nConceptually we begin by enumerating all tree fragments that occur in the training data\\n\\u0004\\u0007\\u0006\\t\\b\\n\\b\\t\\b\\u000b\\u0006 \\u0005 . Note that this is done only implicitly. Each tree is represented by an \\u0005 dimensional vector where the \\u0001 ?th component counts the number of occurences of the \\u0001 ?th tree\\nfragment. Let us define the function \\f\\u000e\\n\\u0010\\u000f\\u0012\\u0011\\u0014\\u0013 to be the number of occurences of the \\u0001 ?th tree\\nfragment in tree \\u0011 , so that \\u0011 is now represented as \\u0015\\u0016\\u000f\\u0012\\u0011\\u0014\\u0013\\u0018\\u0017\\u0019\\u000f\\u001a\\f\\u001c\\u001b\\u0007\\u000f\\u0012\\u0011\\u0014\\u0013\\u000b\\u0006\\u0010\\f\\u001e\\u001d\\u001f\\u000f\\u0012\\u0011 \\u0013!\\u0006\\n\\b\\t\\b\\n\\b\\u000b\\u0006\\\"\\f \\u0003 \\u000f#\\u0011\\u0014\\u0013$\\u0013 .\\n\\n\\fa)\\n\\nS\\n\\nNP\\n\\nb)\\nVP\\n\\nN\\n\\nV\\n\\nJeff\\n\\nate\\n\\nNP\\nD\\n\\nN\\n\\nthe\\n\\napple\\n\\nNP\\n\\nNP\\n\\nD\\n\\nN\\n\\nthe\\n\\napple\\n\\nD\\n\\nN\\n\\nD\\n\\nN\\n\\nNP\\n\\nthe\\n\\napple\\n\\nD\\n\\nNP\\nN\\n\\nD\\n\\nthe\\n\\nN\\napple\\n\\nFigure 2: a) An example tree. b) The sub-trees of the NP covering the apple. The tree in\\n(a) contains all of these sub-trees, and many others. We define a sub-tree to be any subgraph which includes more than one node, with the restriction that entire (not partial) rule\\nproductions must be included. For example, the fragment [NP [D the ]] is excluded\\nbecause it contains only part of the production NP\\nD N.\\nNote that \\u0005 will be huge (a given tree will have a number of subtrees that is exponential in\\nits size). Because of this we would like design algorithms whose computational complexity\\ndoes not depend on \\u0005 .\\nRepresentations of this kind have been studied extensively by Bod [2]. However, the work\\nin [2] involves training and decoding algorithms that depend computationally on the num\\u001b\\nber of subtrees involved. The parameter estimation techniques described in [2] do not\\ncorrespond to maximum-likelihood estimation or a discriminative criterion: see [11] for\\ndiscussion. The methods we propose show that the score for a parse can be calculated in\\npolynomial time in spite of an exponentially large number of subtrees, and that efficient parameter estimation techniques exist which optimize discriminative criteria that have been\\nwell-studied theoretically.\\nGoodman [9] gives an ingenious conversion of the model in [2] to an equivalent PCFG\\nwhose number of rules is linear in the size of the training data, thus solving many of the\\ncomputational issues. An exact implementation of Bod?s parsing method is still infeasible,\\nbut Goodman gives an approximation that can be implemented efficiently. However, the\\nmethod still suffers from the lack of justification of the parameter estimation techniques.\\nThe key to our efficient use of this high dimensional representation is the definition of an\\nappropriate kernel. We begin\\u0001 by examining the inner product between \\u0001 two trees \\u0011 \\u001b and\\n\\u0011 \\u001d under this representation, \\u000f\\u0012\\u0011 \\u001b \\u0006 \\u0011 \\u001d\\t\\u0013\\u0018\\u0017 \\u0015 \\u000f#\\u0011 \\u001b\\u000b\\u0013\\u0003\\u0002\\u001f\\u0015 \\u000f#\\u0011 \\u001d\\t\\u0013 . To compute we first define\\nthe set of nodes in trees \\u0011 \\u001b and \\u0011 \\u001d as \\u0004 \\u001b and \\u0004 \\u001d respectively. We define the indicator\\nfunction \\u0005\\u000b\\n\\u0010\\u000f \\u0005 \\u0013 to be \\u0004 if sub-tree \\u0001 is seen rooted at node \\u0005 and 0 otherwise. It follows\\nthat \\f \\n \\u000f#\\u0011 \\u001b \\u0013 \\u0017\\u0007\\u0006 \\u0003\\t\\b\\u000b\\n\\n\\f\\u000e\\b \\u0005 \\n \\u000f \\u0005 \\u001b \\u0013 and \\f \\n \\u000f\\u0012\\u0011 \\u001d \\u0013 \\u0017\\u0007\\u0006 \\u0003\\u0010\\u000f\\u0011\\n\\n\\f\\u0012\\u000f \\u0005 \\n \\u000f \\u0005 \\u001d \\u0013 . The first step to efficient\\ncomputation of the inner product is the following property (which can be proved with some\\nsimple algebra):\\n#\\n#\\n#\\n#\\n\\n\\u0013\\u0012\\u0014\\u0016\\u0015\\u0018\\u0017\\u001a\\u0019\\u0018\\u001b\\u000b\\u0013\\u001c\\u0014\\u0016\\u0015\\u001e\\u001d\\u000b\\u0019 \\u001f\\\"!$#&% \\u0014\\u0016\\u0015\\u0018\\u0017'\\u0019(% \\u0014\\u0016\\u0015\\u001e\\u001d\\u000b\\u0019)\\u001f*!\\n! !2#43 \\u001465\\u0018\\u0017\\u001a\\u001973 \\u0014658\\u001d9\\u0019\\u0012\\u001f:!\\n!\\n\\u001465\\u0018\\u0017-=>5\\u001e\\u001d-\\u0019\\n+ \\b-,/.0\\b + \\u000f1,/.\\u001e\\u000f\\n+ \\b9,/.0\\b + \\u0011\\u000f ,;.2\\u000f$<\\nwhere we define ? \\u000f \\u0005 \\u001b \\u0006 \\u0005 \\u001d \\u0013 \\u0017@\\u0006 \\n \\u0005 \\n \\u000f \\u0005 \\u001b \\u0013A\\u0005 \\n \\u000f \\u0005 \\u001d \\u0013 . Next, we note that ? \\u000f \\u0005 \\u001b \\u0006 \\u0005 \\u001d \\u0013 can be\\n\\ncomputed in polynomial time, due to the following recursive definition:\\n\\nB\\nB\\n\\nIf the productions at \\u0005 \\u001b and \\u0005\\u001c\\u001d are different ? \\u000f \\u0005 \\u001b \\u0006 \\u001c\\n\\u0005 \\u001d\\t\\u0013 \\u0017DC .\\n\\nIf the productions at \\u0005 \\u001b\\n? \\u000f \\u0005 \\u0017 \\u001b \\u0006 \\u0005 \\u001d \\u0013 \\u0017 \\u0004 .\\u001d\\n\\nand \\u001c\\n\\u0005 \\u001d are the same, and\\n\\n\\u0005\\n\\n\\u001b and \\u0005 \\u001d are pre-terminals, then\\n\\nIn training, a parameter is explicitly estimated for each sub-tree. In searching for the best parse,\\ncalculating the score for a parse in principle requires summing over an exponential number of deriva\\u001d underlying a tree, and in practice is approximated using Monte-Carlo techniques.\\ntions\\nPre-terminals are nodes directly above words in the surface string, for example the N, V, and D\\n\\n\\fB\\n\\nElse if the productions at \\u0005 \\u001b and \\u0005\\u001c\\u001d are the same and \\u0005 \\u001b and \\u0005\\u001c\\u001d are not pre-terminals,\\n\\u0003\\u0001\\u0003\\u0002\\u0006 \\u0003 \\b\\u0005\\u0004\\n? \\u000f \\u0005 \\u001b \\u0006 \\u0005\\u001c\\u001d \\u0013 \\u0017\\n\\u000f$\\u0004\\u0016\\u0002\\\"? \\u000f\\n\\n\\t \\f \\u000f \\u0005 \\u001b \\u0006\\f\\u000b \\u0013!\\u0006\\n\\t\\n\\f \\u000f \\u0005\\u001c\\u001d\\u001f\\u0006\\u000e\\u000b \\u0013 \\u0013$\\u0013 \\u0006\\n\\u0007\\u0003\\b \\u001b\\n\\nwhere \\u0005\\u000f\\t\\u001f\\u000f \\u0005 \\u001b\\n\\u0013 is the number of children of \\u0005 \\u001b in the tree; because the productions at \\u0005 \\u001b /\\u0005\\u001c\\u001d\\nare the same, we have \\u0005\\u000f\\t\\u001f\\u000f \\u0005 \\u001b\\n\\u0013 \\u0017 \\u0005\\u000f\\t\\u0007\\u000f \\u0005\\u001c\\u001d\\t\\u0013 . The \\u0001 ?th child-node of \\u0005 \\u001b is \\t\\n\\f \\u000f \\u0005 \\u001b \\u0006$\\u0001 \\u0013 .\\n\\nTo see that this recursive definition is correct, note that ? \\u000f \\u0005 \\u001b \\u0006 \\u0005 \\u001d \\u0013 simply counts the number\\nof common subtrees that are found rooted at both \\u0005 \\u001b and \\u0005 \\u001d . The first two cases are trivially\\ncorrect. The last, recursive, definition follows because a common subtree for \\u0005 \\u001b and \\u0005\\u001c\\u001d can\\nbe formed by taking the production at \\u0005 \\u001b /\\u0005\\u001c\\u001d , together with a choice at each child of simply\\ntaking the non-terminal at that child, or any one of the common sub-trees at that child.\\n\\u0011 \\u000f \\u0005 \\u001b \\u0006 \\u0001 \\u0013\\u000b\\u0006\\u0003\\n\\t \\f \\u0001\\u0005\\u0010\\n\\u000e\\n\\u0011 \\u000f \\u0005 \\u001d \\u0006 \\u0001 \\u0013$\\u0013$\\u0013 possible choices at the \\u0001 ?th child. (Note\\nThus there are \\u000f \\u0004\\u0016\\u0002\\\"? \\u000f\\n\\n\\t \\f \\u0001\\u0005\\u0010\\n\\u000e\\nthat a similar recursion is described by Goodman [9], Goodman?s application being the\\nconversion of Bod?s model [2] to an equivalent PCFG.)\\nIt is clear from the identity \\u0015 \\u000f#\\u0011 \\u001b \\u0013 \\u0002\\u001a\\u0015\\u0016\\u000f\\u0012\\u0011 \\u001d \\u0013 \\u0017 \\u0006 \\u0003\\t\\b\\u0013\\u0012 \\u0003\\u0010\\u000f ? \\u000f \\u0005 \\u001b \\u0006 \\u0005 \\u001d \\u0013 , and the recursive definition\\nof ? \\u000f \\u0005 \\u001b\\u001f\\u0006 \\u0005\\u001c\\u001d \\u0013 , that \\u0015 \\u000f#\\u0011 \\u001b\\u000b\\u0013 \\u0002 \\u0015\\u0016\\u000f\\u0012\\u0011 \\u001d\\t\\u0013 can be calculated in \\u0014 \\u000f\\n\\u0015 \\u0004 \\u001b\\u0001\\u0015\\u0016\\u0015 \\u0004 \\u001d\\u0017\\u0015 \\u0013 time: the matrix of\\n? \\u000f \\u0005 \\u001b\\u001f\\u0006 \\u0005\\u001c\\u001d \\u0013 values can be filled in, then summed. This can be a pessimistic estimate of\\nthe runtime. A more useful characterization is that it runs in time linear in the number of\\nmembers \\u000f \\u0005 \\u001b \\u0006 \\u0005 \\u001d \\u0013\\u0019\\u0018 \\u0004 \\u001b\\u001b\\u001a \\u0004 \\u001d such that the productions at \\u0005 \\u001b and \\u0005 \\u001d are the same. In our\\ndata we have found a typically linear number of nodes with identical productions, so that\\nmost values of ? are 0, and the running time is close to linear in the size of the trees.\\nThis recursive kernel structure, where a kernel between two objects is defined in terms\\nof kernels between its parts is quite a general idea. Haussler [10] goes into some detail\\ndescribing which construction operations are valid in this context, i.e. which operations\\nmaintain the essential Mercer conditions. This paper and previous work by Lodhi et al. [12]\\nexamining the application of convolution kernels to strings provide some evidence that\\nconvolution kernels may provide an extremely useful tool for applying modern machine\\nlearning techniques to highly structured objects. The key idea here is that one may take\\na structured object and split it up into parts. If one can construct kernels over the parts\\nthen one can combine these into a kernel over the whole object. Clearly, this idea can be\\nextended recursively so that one only needs to construct kernels over the ?atomic? parts of\\na structured object. The recursive combination of the kernels over parts of an object retains\\ninformation regarding the structure of that object.\\nSeveral issues remain with \\u0001 the kernel we describe over trees and convolution kernels in\\ngeneral. First, the value of \\u000f\\u0012\\u0011 \\u001b \\u0006$\\u0011 \\u001d\\n\\u0013 will depend greatly on the size of the trees \\u0011 \\u001b \\u0006 \\u0011 \\u001d .\\n\\u0001\\u001d\\u001c \\u000f\\u0012\\u0011 \\u001b \\u0006 \\u0011 \\u001d\\n\\u0013\\u0016\\u0017 \\u0001 \\u000f\\u0012\\u0011 \\u001b \\u0006 \\u0011 \\u001d\\n\\u0013\\u0003\\u001e \\u001f \\u0001 \\u000f\\u0012\\u0011 \\u001b \\u0006 \\u0011\\u001c\\u001b\\u000b\\u0013 \\u0001 \\u000f#\\u0011 \\u001d\\u001f\\u0006$\\u0011 \\u001d \\u0013\\nOne may normalize the kernel by using\\nwhich also satisfies the essential Mercer conditions. Second, the value of the kernel when\\napplied to two copies of the same tree can be extremely large (in our experiments on the\\norder of \\u0004 C\\u0017! ) while the value of the kernel between two different trees is typically much\\nsmaller (in our experiments the typical pairwise comparison is of order 100). By analogy\\nwith a Gaussian kernel we say that the kernel is very peaked. If one constructs a model\\nwhich is a linear combination of trees, as one would with an SVM [6] or the perceptron,\\nthe output will be dominated by the most similar tree and so the model will behave like\\na nearest neighbor rule. There are several possible solutions to this problem. Following\\nHaussler [10] we may radialize the kernel, however, it is not always clear that the result is\\nstill a valid kernel. Radializing did not appear to help in our experiments.\\nThese problems motivate two simple modifications to the tree kernel. Since there will\\nbe many more tree fragments of larger size ? say depth four versus depth three ? and\\nsymbols in Figure 2.\\n\\n\\fconsequently less training data, it makes sense to downweight the contribution of larger\\ntree fragments to the kernel. The first method for doing this is to simply restrict the depth\\nof the tree fragments we consider. The second method is to scale the relative importance of\\n\\u0004,\\ntree fragments with their size. This can be achieved by introducing a parameter C\\nand modifying the base case and recursive case of the definitions of ? to be respectively\\n\\u0003\\u0017\\u0003\\u0002\\u0006 \\u0003\\t\\b \\u0004\\nand ? \\u000f \\u0005 \\u001b\\u001f\\u0006 \\u0005\\u001c\\u001d\\t\\u0013 \\u0017\\n? \\u000f \\u0005 \\u001b \\u0006 \\u0005\\u001c\\u001d\\t\\u0013 \\u0017\\n\\u000f \\u0004\\u0016\\u0002\\\"? \\u000f\\n\\n\\t \\f \\u000f \\u0005 \\u001b \\u0006\\f\\u000b \\u0013!\\u0006\\n\\t \\f \\u000f \\u0005\\u001c\\u001d\\u001f\\u0006\\u000e\\u000b \\u0013$\\u0013$\\u0013 \\b\\n\\u0007\\u0003\\b \\u001b\\n\\n\\u0002\\u0001\\u0004\\u0003\\u0006\\u0005\\n\\n\\u0007\\u0003\\n\\n\\b\\u0003\\n\\n\\u0003\\n\\t \\n\\f\\u000b\\u000e\\n\\u0010\\u000f\\n\\n\\f \\n \\u000f#\\u0011 \\u001b \\u0013$\\f \\n \\u000f\\u0012\\u0011 \\u001d \\u0013 , where\\nThis corresponds to a modified kernel \\u0015\\u0016\\u000f\\u0012\\u0011 \\u001b \\u0013 \\u0002 \\u0015 \\u000f#\\u0011 \\u001d \\u0013 \\u0017 \\u0006 \\n\\u0001 \\n is the number of rules in the \\u0001 ?th fragment. This kernel downweights the contribution\\nof tree fragments exponentially with their size.\\n\\n\\u0011 \\u0010\\u0012\\u0014\\u0013\\n\\nIt is straightforward to design similar kernels for tagging problems (see figure 1) and for\\nanother common structure found in NLP, dependency structures. See [5] for details. In the\\ntagging kernel, the implicit feature representation tracks all features consisting of a subsequence of state labels, each with or without an underlying word. For example, the paired sequence Lou/SP Gerstner/CP is/N chairman/N of/N IBM/SC would include features such as SP CP , SP Gerstner/CP N , SP CP is/N N of/N\\nand so on.\\n\\n\\u0015\\n\\n\\u0015\\n\\n\\u0016 \\u0015\\n\\n\\u0016\\n\\n\\u0016 \\u0015\\n\\n\\u0016\\n\\n3 Linear Models for Parsing and Tagging\\nThis section formalizes the use of kernels for parsing and tagging problems. The method\\nis derived by the transformation from ranking problems to a margin-based classification\\nproblem in [8]. It is also related to the Markov Random Field methods for parsing suggested\\nin [13], and the boosting methods for parsing in [4]. We consider the following set-up:\\n\\nB\\n\\n\\u0015 \\u0011 \\u0018\\u0017 \\u0016\\n\\n\\u0011\\n\\nTraining data is a set of example input/output pairs. In parsing we would have training\\nexamples \\n \\u0006 \\n where each \\n is a sentence and each \\n is the correct tree for that sentence.\\n\\nB\\n\\n\\u0017\\n\\n\\u001a \\u0011\\n\\nWe assume some way of enumerating a set of candidates for a particular sentence. We\\nuse \\u001c\\n \\u0007 to denote the \\u000b ?th candidate for the \\u0001 ?th sentence in training data, and \\u000f \\n \\u0013 \\u0017\\n\\u001c\\n \\u001b\\u001f\\u0006 \\u001c\\n \\u001d \\b\\n\\b\\t\\b to denote the set of candidates for \\n .\\n\\nB\\n\\n\\u0019\\n\\n\\u0015\\u001b\\u0019 \\u001c\\u0019\\n\\n\\u001d\\u0016\\n\\n\\u0019\\n\\n\\u0011 \\u001e\\n\\n\\u0011\\n\\n\\u0019 \\u0004\\u0017\\n\\nWithout loss of generality we take \\n \\u001b to be the correct parse for \\n (i.e., \\u001c\\n \\u001b \\u0017 \\n ).\\nB Each candidate \\n \\u0007 is represented by a feature vector \\u0015\\u0016\\u000f \\n \\u0007 \\u0013 in the space \\u0003 . The param\\u0003 . We then define the ?ranking score? of each\\neters of the model are also a vector \\u0018\\n\\u0007\\nexample as \\u0002 \\u0015\\u0016\\u000f \\n \\u0013 . This score is interpreted as an indication of the plausibility of the\\n\\n \\u0017\\u0002 \\u0004 \\u0002 \\u0015\\u0016\\u000f \\u0013 .\\ncandidate. The output of the model on a training or test example is\\n\\n\\u0019\\n\\n!\\n\\n\\u001f\\u0019\\n\\n!\\n\\n\\u001f\\u0019\\n\\n\\u0011 \\\"$#&%(')\\\"$*\\n+ $, !\\n\\t\\n!\\n\\n\\u001f\\u0019\\n\\nWhen considering approaches to training the parameter vector , note that a ranking function that correctly ranked the correct parse above all competing candidates would satisfy\\n\\u0015\\u0016\\u000f \\n \\u0007 \\u0013 \\u0013 4C \\u0003\\u0001\\u0010\\u0006 \\u000b\\n. It is simple to modify the Perceptron\\nthe conditions \\u0002$\\u000f \\u0015 \\u000f \\n \\u001b \\u0013\\nand Support Vector Machine algorithms to treat this problem. For example, the SVM optimization problem (hard margin version) is to find the\\nwhich minimizes \\u0015 \\u0015 \\u0015\\u0016\\u0015 \\u001d subject to\\n\\u0007\\nthe constraints \\u0002 \\u000f\\u001a\\u0015 \\u000f \\u001c\\n \\u001b \\u0013\\n\\u0015\\u0016\\u000f \\u001c\\n \\u0013 \\u0013\\n\\u0004 \\u0003\\u0001\\u0010\\u0006 \\u000b\\n. Rather than explicitly calculating\\n, the perceptron algorithm and Support Vector Machines can be formulated as a search\\n\\n!\\n\\n!\\n\\n!\\n\\n-\\u0019 /. \\u001f\\u0019\\n\\n-\\u0019 :. \\u001f\\u0019\\n\\n10 32 42 \\u0002576\\n\\n9! 8\\n;5 <2 =2 \\u00065>6\\n\\n\\u001b!\\n\\n? This can be achieved using a modified dynamic programming table where \\u001465 \\u0017 =>5 \\u001d =\\u0010@\\n\\u0019 stores\\n5)\\u0017-=(58\\u001d of depth @ or less. The recursive <definition of can\\nthe number of common subtrees at nodes\\n<\\nbe modified\\nappropriately.\\nA A context-free\\ngrammar ? perhaps taken straight from the training examples ? is one way of\\nenumerating candidates.\\n5 Another choice is to use a hand-crafted grammar (such as the LFG grammar\\nin [13]) or to take the most probable parses from an existing probabilistic parser (as in [4]).\\n\\n\\f\\u001f\\u0019\\n\\n-\\u0019\\n\\n-\\u0019 :. \\u001f\\u0019\\n\\n-\\u0019\\n\\nDefine: \\u000f \\u0013 \\u0017 \\u0006 \\u0002 \\n \\u0012 \\u0007 \\u0004\\u0002\\u0001 \\n \\u0012 \\u0007 \\u000f\\u001a\\u0015 \\u000f \\u001c\\n \\u001b\\t\\u0013\\u000e\\u0002 \\u0015 \\u000f \\u0013\\n\\u0015 \\u000f \\u001c\\n \\u0007 \\u0013\\u000e\\u0002 \\u0015 \\u000f \\u0013 \\u0013\\n\\u0016\\n\\u0007\\nInitialization: Set dual parameters \\u0001 \\n \\u0012 \\u0017DC\\nFor \\u0001 \\u0017 \\u0004 \\b\\t\\b\\n\\b \\u0005 \\u0006 \\u000b \\u0017\\n\\b\\t\\b\\n\\b \\u0005\\u001c\\n\\u0002 \\u0004\\nIf \\u000f \\n \\u001b \\u0013 \\u0003 \\u000f \\n \\u0007 \\u0013 do nothing, Else \\u0001 \\n \\u0007 \\u0017 \\u0001 \\n \\u0007 \\u0005\\n\\n-\\u0019 10\\n\\n-\\u0019\\n\\n\\b6\\n\\nFigure 3: The perceptron algorithm for ranking problems.\\nDepth\\nScore\\nImprovement\\n\\n1\\n\\n2\\n\\n\\u0006\\u0004 \\u0005\\b\\u0007\\n\\t\\n\\u0016\\u0017\\t\\u0018\\u0007\\u001a\\u0019\\n\\n3\\n\\n\\u0004\\u0006\\u000b\\b\\u0007\\f\\t\\n\\u001b\\u0006\\u000e\\u0010\\u0007\\u001d\\u001c\\n\\n\\n\\u000f\\u000e\\u0010\\u0007\\u0011\\t\\n\\u001b\\u0012\\u0005\\u0010\\u0007\\u001d\\u0005\\n\\n4\\n\\n\\u0004\\u0012\\u000b\\u0010\\u0007\\n\\t\\n\\u001b\\u0015\\t\\u001e\\u0007\\u001a\\u0019\\n\\n5\\n\\n6\\n\\n\\u0004\\u0012\\u000b\\u0010\\u0007\\f\\t\\n\\t\\u001f\\u000b\\b\\u0007 \\u0019\\n\\n\\u0004\\u0006\\n\\u0010\\u0007\\u0013\\u000e\\u0015\\u0014 \\u000e\\u0015\\t\\n\\t!\\n\\u0010\\u0007\\u001d\\u0005\\n\\nTable 1: Score shows how the parse score varies with the maximum depth of sub-tree\\nconsidered by the perceptron. Improvement is the relative reduction in error in comparison\\nto the PCFG, which scored 74%. The numbers reported are the mean and standard deviation\\nover the 10 development sets.\\nfor ?dual parameters? \\u0001 \\n \\u0007 which determine the optimal weights\\n\\n! 8\\n\\n\\u0017\\n\\n!\\n\\n\\u0002 \\n \\u0012\\u0007 \\u0004\\n\\n! 8\\n\\n\\u0001 \\n \\u0012 \\u0007 \\u001a\\u000f \\u0015 \\u000f-\\u0019\\u001c\\n \\u001b \\u0013:. \\u0015\\u0016\\u000f\\u001f\\u0019\\u001c\\n \\u0007 \\u0013$\\u0013\\n\\n(1)\\n\\n\\u0003\\u000f\\n\\n(we use \\u0006 \\u0002 \\n \\u0012 \\u0007 \\u0004 as shorthand for \\u0006 \\n \\u0006 \\u0007\\u0003\\b \\u001d ). It follows that the score of a parse can be\\ncalculated using the dual parameters, and inner products between feature\\u0003 vectors, without\\nhaving to explicitly deal with feature or parameter vectors in the space\\n:\\n\\n! 8 \\u0002\\u0019\\n\\n\\u0017\\n\\n!\\n\\n\\u0002 \\n \\u0012\\u0007 \\u0004\\n\\n\\u0001 \\n \\u0012 \\u0007 \\u000f\\u001a\\u0015\\u0016\\u000f\\u001f\\u0019\\u001c\\n \\u001b\\t\\u0013\\u000e\\u0002\\n\\u0015\\u0016\\u000f\\u001f\\u0019 \\u0013\\n\\n. \\u0015\\u0016\\u000f\\u001f\\u0019\\u001c\\n \\u0007 \\u0013\\u000e\\u0002\\n\\u0015\\u0016\\u000f\\u001f\\u0019\\n\\n\\u0013\\u0010\\u0013\\n\\nFor example, see figure 3 for the perceptron algorithm applied to this problem.\\n\\n4 Experimental Results\\nTo demonstrate the utility of convolution kernels for natural language we applied our tree\\nkernel to the problem of parsing the Penn treebank ATIS corpus [14]. We split the treebank\\nrandomly into a training set of size 800, a development set of size 200 and a test set of size\\n336. This was done 10 different ways to obtain statistically significant results. A PCFG\\nwas trained on the training set, and a beam search was used to give a set of parses, with\\nPCFG probabilities, for each of the sentences. We applied a variant of the voted perceptron\\nalgorithm [7], which is a more robust version of the original perceptron algorithm with\\nperformance similar to that of SVMs. The voted perceptron can be kernelized in the same\\nway that SVMs can but it can be considerably more computationally efficient.\\nWe generated a ranking problem by having the PCFG generate its top 100 candidate parse\\ntrees for each sentence. The voted perceptron was applied, using the tree kernel described\\npreviously, to this re-ranking problem. It was trained on 20 trees selected randomly from\\nthe top 100 for each sentence and had to choose the best candidate from the top 100 on the\\ntest set. We tested the sensitivity to two parameter settings: first, the maximum depth of\\nsub-tree examined, and second, the scaling factor used to down-weight deeper trees. For\\neach value of the parameters we trained on the training set and tested on the development\\nset. We report the results averaged over the development sets in Tables 1 and 2.\\nWe report a parse score which combines precision and recall. Define \\t \\n to be the number\\nof correctly placed constituents in the \\u0001 ?th test tree, \\\"\\u0003\\n to be the number of constituents\\n\\n\\fScale\\nScore\\nImp.\\n\\n0.1\\n\\n\\u0004\\u0012\\u0004 \\u0007\\u0011\\t\\n\\t\\u0012\\t\\u0018\\u0007\\u0013\\u001c\\n\\n0.2\\n\\n\\u0004\\u0012\\n\\u0010\\u0007\\u0011\\u0001\\t\\n\\t \\u0004 \\u0007\\n\\n0.3\\n\\n\\u0004\\u0012\\u000b\\u0010\\u0007\\n\\t\\n\\u001b\\u0012\\u000e\\u0010\\u0007\\u001a\\u0019\\n\\n0.4\\n\\n\\u0004\\u0012\\u000b \\u0007\\n\\t\\n\\u0015\\u001b \\t\\u0018\\u0007\\u001d\\u0005\\n\\n0.5\\n\\n\\u0004\\u0006\\u000b\\u0010\\u0007\\u0011\\t\\n\\u001b \\t\\u001e\\u0007\\u001d\\u0019\\n\\n0.6\\n\\n\\u0004\\u0012\\u000b\\u0010\\u0007\\u0011\\t\\n\\u001b\\u000f\\u001b \\u0007\\u001d\\u0019\\n\\n0.7\\n\\n\\u0004\\u0012\\u000b\\u0010\\u0007\\f\\t\\n\\u001b\\u0015\\t\\u001e\\u0007 \\u0019\\n\\n0.8\\n\\n\\u0004\\u0006\\u000b\\b\\u0007\\n\\t\\n\\t\\u001f\\u000b\\b\\u0007\\u001a\\u0019\\n\\n0.9\\n\\n\\u0004\\u0006\\n\\u0010\\u0007\\u0011\\u0001\\t\\n\\t!\\u0004 \\u0007\\n\\nTable 2: Score shows how the parse score varies with the scaling factor for deeper sub-trees\\nis varied. Imp. is the relative reduction in error in comparison to the PCFG, which scored\\n74%. The numbers reported are the mean and standard deviation over the 10 development\\nsets.\\nproposed, and \\u0002 \\n to be the number of constistuents in the true parse tree. A constituent is\\ndefined by a non-terminal label and its span. The score is then\\n\\u0007\\n\\u0004 \\u000b\\t \\n\\u000b\\t \\n\\u0004 !\\n\\u0004 C\\tC\\u0004\\u0003\\u0006\\u0005\\n\\u0002 \\n \\u001a\\n\\u0002\\n\\u0006 \\n \\u0007\\u0002 \\n \\n\\\"\\u000e\\n\\u0007\\u0002 \\n\\t\\b\\n\\n6\\n\\nThe precision and recall on the \\u0001 ?th parse are \\t\\t\\n /\\\"\\u000e\\n and \\t\\n\\n /\\u0002\\u0007\\n respectively. The score is then\\nthe average precision recall, weighted by the size of the trees \\u0002 \\n . We also give relative\\nimprovements over the PCFG scores. If the PCFG score is \\n and the perceptron score is \\u000b ,\\n\\u0005 \\u000f\\f\\u000b \\n\\u0003\\n\\n \\u0013\\u0003\\u001e \\u000f$\\u0004 C\\tC \\u000e\\u000e\\n\\n \\u0013 , i.e., the relative reduction in error.\\nthe relative improvement is \\u0004 C\\tC\\u0004\\u0003\\u0006\\u0014\\n\\n.\\n\\n.\\n\\nWe finally used the development set for cross-validation to choose the best parameter settings for each split. We used the best parameter settings (on the development sets) for each\\nsplit to train on both the training and development sets, then tested on the test set. This gave\\n\\u000f C\\u0010\\u0003\\u0012\\u0011 \\u0004 with the best choice of maximum depth and a score\\na relative goodness score of \\nof \\n\\u000f C\\u0010\\u0003\\u0013\\u0011 \\u0004 with the best choice of scaling factor. The PCFG scored \\u0014\\u0016\\u0015\\u0017\\u0003 on the test data.\\nAll of these results were obtained by running the perceptron through the training data only\\nonce. As has been noted previously by Freund and Schapire [7], the voted perceptron often\\nobtains better results when run multiple times through the training data. Running through\\nthe data twice with a maximum depth of 3 yielded a relative goodness score of \\u000f \\u0004\\u0016\\u0003\\u0018\\u0011 \\u0004 ,\\nwhile using a larger number of iterations did not improve the results significantly.\\nIn summary we observe that in these simple experiments the voted perceptron and an appropriate convolution kernel can obtain promising results. However there are other methods\\nwhich perform considerably better than a PCFG for NLP parsing ? see [3] for an overview\\n? future work will investigate whether the kernels in this paper give performance gains over\\nthese methods.\\n\\n5 A Compressed Representation\\nWhen used with algorithms such as the perceptron, convolution kernels may be even more\\ncomputationally attractive than the traditional radial basis or polynomial kernels. The linear\\ncombination of parse trees constructed by the perceptron algorithm can be viewed as a\\nweighted forest. One may then search for subtrees in this weighted forest that occur more\\nthan once. Given a linear combination of two trees \\u0019 \\u0011 \\u001b \\u0002\\u0006\\\"\\u001a \\u0011 \\u001d which contain a common\\nsubtree, we may construct a smaller weighted acyclic graph, in which the common subtree\\noccurs only once and has weight \\u0019 \\u0002\\u001b\\u001a . This process may be repeated until an arbitrary linear\\ncombination of trees is collapsed into a weighted acyclic graph in which no subtree occurs\\nmore than once. The perceptron may now be evaluated on a new tree by a straightforward\\ngeneralization of the tree kernel to weighted acyclic graphs of the form produced by this\\nprocedure.\\nGiven the nature of our data ? the parse trees have a high branching factor, the words are\\nchosen from a dictionary that is relatively small in comparison to the size of the training\\ndata, and are drawn from a very skewed distribution, and the ancestors of leaves are part\\n\\n\\fof speech tags ? there are a relatively small number of subtrees in the lower levels of the\\nparse trees that occur frequently and make up the majority of the data. It appears that the\\napproach we have described above should save a considerable amount of computation. This\\nis something we intend to explore further in future work.\\n\\n6 Conclusions\\nIn this paper we described how convolution kernels can be used to apply standard kernel\\nbased algorithms to problems in natural language. Tree structures are ubiquitous in natural language problems and we illustrated the approach by constructing a convolution kernel\\nover tree structures. The problem of parsing English sentences provides an appealing example domain and our experiments demonstrate the effectiveness of kernel-based approaches\\nto these problems. Convolution kernels combined with such techniques as kernel PCA and\\nspectral clustering may provide a computationally attractive approach to many other problems in natural language processing. Unfortunately, we are unable to expand on the many\\npotential applications in this short note, however, many of these issues are spelled out in a\\nlonger Technical Report [5].\\n\\nReferences\\n[1] Aizerman, M., Braverman, E., and Rozonoer, L. (1964). Theoretical Foundations of the Potential\\nFunction Method in Pattern Recognition Learning. Automation and Remote Control, 25:821?837.\\n[2] Bod, R. (1998). Beyond Grammar: An Experience-Based Theory of Language. CSLI Publications/Cambridge University Press.\\n[3] Charniak, E. (1997). Statistical techniques for natural language parsing. In AI Magazine, Vol. 18,\\nNo. 4.\\n[4] Collins, M. (2000). Discriminative Reranking for Natural Language Parsing. Proceedings of the\\nSeventeenth International Conference on Machine Learning. San Francisco: Morgan Kaufmann.\\n[5] Collins, M. and Duffy, N. (2001). Parsing with a Single Neuron: Convolution Kernels for Natural\\nLanguage Problems. Technical report UCSC-CRL-01-01, University of California at Santa Cruz.\\n[6] Cortes, C. and Vapnik, V. (1995). Support?Vector Networks. Machine Learning, 20(3):273?297.\\n[7] Freund, Y. and Schapire, R. (1999). Large Margin Classification using the Perceptron Algorithm.\\nIn Machine Learning, 37(3):277?296.\\n[8] Freund, Y., Iyer, R.,Schapire, R.E., & Singer, Y. (1998). An efficient boosting algorithm for combining preferences. In Machine Learning: Proceedings of the Fifteenth International Conference.\\nSan Francisco: Morgan Kaufmann.\\n[9] Goodman, J. (1996). Efficient algorithms for parsing the DOP model. In Proceedings of the\\nConference on Empirical Methods in Natural Language Processing (EMNLP 96), pages 143-152.\\n[10] Haussler, D. (1999). Convolution Kernels on Discrete Structures. Technical report, University\\nof Santa Cruz.\\n[11] Johnson, M. The DOP estimation method is biased and inconsistent. To appear in Computational Linguistics.\\n[12] Lodhi, H., Christianini, N., Shawe-Taylor, J., and Watkins, C. (2001). Text Classification using\\nString Kernels. To appear in Advances in Neural Information Processing Systems 13, MIT Press.\\n[13] Johnson, M., Geman, S., Canon, S., Chi, S., & Riezler, S. (1999). Estimators for stochastic\\n?unification-based? grammars. In Proceedings of the 37th Annual Meeting of the Association for\\nComputational Linguistics. San Francisco: Morgan Kaufmann.\\n[14] Marcus, M., Santorini, B., & Marcinkiewicz, M. (1993). Building a large annotated corpus of\\nenglish: The Penn treebank. Computational Linguistics, 19, 313-330.\\n[15] Scholkopf, B., Smola, A.,and Muller, K.-R. (1999). Kernel principal component analysis. In B.\\nScholkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods ? SV Learning,\\npages 327-352. MIT Press, Cambridge, MA.\\n[16] Watkins, C. (2000). Dynamic alignment kernels. In A.J. Smola, P.L. Bartlett, B. Schlkopf, and\\nD. Schuurmans, editors, Advances in Large Margin Classifiers, pages 39-50, MIT Press.\\n\\n\\f\",\n          \"LTD Facilitates Learning In a Noisy\\nEnvironment\\n\\nPaul Munro\\nSchool of Information Sciences\\nUniversity of Pittsburgh\\nPittsburgh PA 15260\\npwm+@pitt.edu\\n\\nGerardina Hernandez\\nIntelligent Systems Program\\nUniversity of Pittsburgh\\nPittsburgh PA 15260\\ngehst5+@pitt.edu\\n\\nAbstract\\n\\nLong-term potentiation (LTP) has long been held as a biological\\nsubstrate for associative learning. Recently, evidence has emerged\\nthat long-term depression (LTD) results when the presynaptic cell\\nfires after the postsynaptic cell. The computational utility of LTD\\nis explored here. Synaptic modification kernels for both LTP and\\nLTD have been proposed by other laboratories based studies of one\\npostsynaptic unit. Here, the interaction between time-dependent\\nLTP and LTD is studied in small networks.\\n1\\n\\nIntroduction\\n\\nLong term potentiation (LTP) is a neurophysiological phenomenon observed under\\nlaboratory conditions in which two neurons or neural populations are stimulated at a\\nhigh frequency with a resulting measurable increase in synaptic efficacy between\\nthem that lasts for several hours or days [1]-[2] LTP thus provides direct evidence\\nsupporting the neurophysiological hypothesis articulated by Hebb [3].\\nThis increase in synaptic strength must be countered by a mechanism for weakening\\nthe synapse [4]. The biological correlate, long-term depression (LTD) has also been\\nobserved in the laboratory; that is, synapses are observed to weaken when low\\npresynaptic activity coincides with high postsynaptic activity [5]-[6].\\nMathematical formulations of Hebbian learning produce weights, Wi}, (where i is the\\npresynaptic unit and j is the postsynaptic unit), that capture the covariance [Eq. 1]\\nbetween the instantaneous activities of pairs of units, ai and aj [7].\\nwij(t)\\n\\n= (ai (t) -ai)(a j (t)-aj)\\n\\n[1]\\n\\nThis idea has been generalized to capture covariance between acUvlUes that are\\nshifted in time [8]-[9], resulting in a framework that can model systems with\\ntemporal delays and dependencies [Eq. 2].\\nWij(t)\\n\\n= ffK(t\\\"-t')ai (tA')aj (t')dt'dt'\\n\\n[2]\\n\\n\\f151\\n\\nLTD Facilitates Learning in a Noisy Environment\\n\\nAs will be shown in the following sections, depending on the choice of the function\\nK(L1t), this formulation encompasses a broad range of learning rules [10]-[12] and\\ncan support a comparably broad range of biological evidence.\\n\\nFigure 1. Synaptic change as a function of the time difference between spikes from\\nthe presynaptic neuron and the postsynaptic neuron. Note that for tpre < tpos t , LTP\\nresults (L1w > 0), and for tpre > tp ost , the result is LTD.\\nRecent biological data from [13]-[15], indicates an increase in synaptic strength\\n(LTP) when presynaptic activity precedes postsynaptic activity, and LTD in the\\nreverse case (postsynaptic precedes presynaptic). These ideas have started to appear\\nin some theoretical models of neural computation [10]-[12], [16]-[18]. Thus, Figure\\n1 shows the form of the dependence of synaptic change, Liw on the difference in\\nspike arrival times.\\n2\\n\\nA General Framework\\n\\nGiven specific assumptions, the integral in Eq. 2 can be separated into two integrals,\\none representing LTP and one representing LTD [Eq. 3].\\nWij(t) =\\n\\nt\\n\\nf\\n\\n{' =\\n\\nt\\n\\nf\\n\\nKp(t-t')ai(t')a/)dt' +\\n\\n-00\\n\\nv\\n\\n'\\n\\nKD(t-t')ai(t)a/t')dt'\\n\\nr~'_=_-_oo__~v~\\n\\n____\\n\\n[3]\\n\\n~\\n\\nLID\\n\\nL~\\n\\nThe activities that do not depend on t' can be factored out of the integrals, giving\\ntwo Hebb-like products, between the instantaneous activity in one cell and a\\nweighted time average of the activity in the other [Eq. 4]:\\nwij (t)\\n\\n= (ai (t)) p a j (t) -\\n\\nai\\n\\n(t)( a j (t)) D\\n[4]\\n\\nt\\n\\nwhere (f(t)) X :; I\\n\\n,f\\n\\nK X (t - t')f(t')dt' I for X\\n\\nE\\n\\n{P, D}\\n\\nt =-00\\n\\nThe kernel functions Kp and KD can be chosen to select precise times out of the\\nconvoluted function fit), or to average across the functions for an arbitrary range. The\\nalpha function is useful here [Eq. 5]. A high value of a selects an immediate time, while\\na small value approximates a longer time-average.\\n-a 'r\\nKx (r) = fixu X for X E {P,D}\\n\\nwith ap > O,aD > O,fip > O,fiD <\\n\\n?\\n\\n[5]\\n\\n\\fP. W. Munro and G. Hernandez\\n\\n152\\n\\nFor high values of ap and QD, only pre- and post- synaptic activities that are very close\\ntemporally will interact to modify the synapse. In a simulation with discrete step sizes,\\nthis can be reasonably approximated by only considering just a single time step [Eq. 6].\\ndWjj (t)\\n\\n=a j (t-l)a j(t)-aj (t)a j (t-1)\\n\\n[6]\\n\\nSumming LiWi,{t) and Liwiit+l) gives a net change in the weights Li(2)Wij = wiiH1)-Wi/t-l)\\nover the two time steps:\\n\\n[7)\\nThe first tenn is predictive in that it has the fonn of the delta rule where a/HI) acts as a\\ntraining signal for aj (t-1), as in a temporal Hopfield network [9].\\n\\n3\\n\\nTemporal Contrast Enhancement\\n\\nThe computational role of the LTP term in Eq. 3 is well established, but how does the\\nsecond term contribute? A possibility is that the term is analogous to lateral inhibition in\\nthe temporal domain; that is, that by suppressing associations in the \\\"wrong\\\" temporal\\ndirection, the system may be more robust against noise in the input. The resulting system\\nmay be able to detect the onset and offset of a signal more reliably than a system not\\nusing an anti-Hebbian LTD tenn.\\nThe extent to which the LTD term is able to enhance temporal contrast is likely to depend\\nidiosyncratically on the statistical qualities of a particular system. If so, the parameters of\\nthe system might only be valid for signals with specific statistical properties, or the\\nparameters might be adaptive. Either of these possibilities lies beyond the scope of\\nanalysis for this paper.\\n\\n4\\n\\nSimulations\\n\\nTwo preliminary simulation studies illustrate the use of the learning rule for predictive\\nbehavior and for temporal contrast enhancement. For every simulation, kernel functions\\nwere specified by the parameters a and p, and the number of time steps, np and nD, that\\nwere sampled for the approximation of each integral.\\n\\n4.1\\n\\nTask 1. A Sequential Shifter\\n\\nThe first task is a simple shifter over a set of 7 to 20 units. The system is trained on these\\nstimuli and then tested to see if it can reconstruct the sequence given the initial input.\\nThe task is given with no noise and with temporal noise (see Figure 2). Task 1 is\\ndesigned to examine the utility of LTD as an approach to learning a sequence with\\ntemporal noise. The ability of the network to reconstruct the noise-free sequence after\\ntraining on the noisy sequence was tested for different LTD kernel functions.\\nNote that the same patterns are presented (for each time slice, just one of the n units is\\nactive), but the shifts either skip or repeat in time. Experiments were run with k = 1, 2, or\\n3 of the units active.\\n\\n4.2\\n\\nTask 2. Time series reconstruction.\\n\\nIn this task, a set of units was trained on external sinusoidal signals that varied according\\nto frequency and phase. The purpose of this task is to examine the role of LTD in\\nproviding temporal context. The network was then tested under a condition in which the\\n\\n\\f153\\n\\nLTD Facilitates Learning in a Noisy Environment\\n\\nexternal signals were provided to all but one of the units. The activity of the deprived\\nunit was then compared with its training signal\\nSequence\\nClean\\n\\nm\\ne\\n\\nNoisy\\n\\naaaaaaa\\n\\n7\\n\\naaaaaaa\\n12.'14~67\\n\\n_ClClClaaa\\nCI_aaaaa\\nCla_aaaa\\nClCla_aaa\\naClaa_aa\\naClaaa_a\\naaaaaa_\\n_ aaaaaa\\na_aaaaa\\naa_aaaa\\naaa_aaa\\naaaa_aa\\naaaaa_a\\naaaaaa_\\n\\n_aaaaaa\\na_aaaaa\\naa_aaaa\\nCla_aaaa\\naaa_aaa\\naaaa_aa\\nClaaaaa_\\naaaaaa _\\n_aaaaaa\\na_aaaaa\\naaa_aaa\\naaaa_aa\\naaaa_aa\\naaaaa_a\\n\\n12::1 4\\n\\nT\\n\\nReconstruction\\n\\n~\\n\\n6\\n\\nLTP alone\\n\\na12::14~67\\na a a a a a\\n_CICICICICICI\\n_ __\\n__\\n__\\n_ CI\\nCI\\nCICICI\\n\\n-------------------------------------------------\\n\\nLTP&LTD\\n\\naaaaaaa\\n1 2 ::I 4 ~ 6 7\\n_aaaaaa\\nCI_aaaaa\\naa_aaaa\\naaa_aal:l\\naaaa_al:l\\naaaaa_1:I\\naaaaaa_\\n_aal:laaa\\na_aaaaa\\naa_aaaa\\naaa_al:la\\naaaa_aa\\naaaaa_a\\nl:Iaaaaa_\\n\\nFigure 2. Reconstruction of clean shifter sequence using as input the noisy stimulus\\nshifter sequence. For each time slice, just one of the 7 units is active. In the clean\\nsequence, activity shifts cyclically around the 7 units. The noisy sequence has a random\\njitter of ?1.\\n5\\n\\nResultsSequential Shifter Results\\n\\nAll networks trained on the clean sequence can learn the task with LTP alone, but\\nno networks could learn the shifter task based on a noisy training sequence unless\\nthere was also an LTD term. Without an LTD term, most units would saturate to\\nmaximum values. For a range of LTD parameters, the network would converge\\nwithout saturating. Reconstruction performance was found to be sensitive to the\\nLTD parameters. The parameters a and f3 shown in Table.l needed to be chosen\\nvery specifically to get perfect reconstruction (this was done by trial and error). For\\na narrow range of parameters near the optimal values, the reconstructed sequence\\nwas close to the noise-free target. However, the parameters a and f3 shown in\\nTable 2 are estimated from the experimental result of Zhang,et al [15].\\n. I sh?f\\nT able 1 Resu ts 0 fth e sequentla\\nIter task .\\nk n\\n\\nnD\\n\\nTime\\n\\n0.4\\n\\n5\\n\\n208\\n\\n0 .1\\n0.2\\n\\n0.4\\n0.1\\n\\n4\\n7\\n\\n40\\n192\\n\\n1\\n\\n0.2\\n\\n0.1\\n\\n6\\n\\n168\\n\\n2.72\\n\\n1\\n\\n0.1\\n\\n0.4\\n\\n8\\n\\n682\\n\\n1\\n\\n2.72\\n\\n1\\n\\n0.1\\n\\n0.4\\n\\n7\\n\\n99\\n\\n1\\n\\n2.72\\n\\n1\\n\\n0.1\\n\\n0.4\\n\\n13\\n\\n1136\\n\\n1\\n\\n2.72\\n\\n1\\n\\n0.1\\n\\n0.4\\n\\n18\\n\\n4000\\n\\nnr\\n\\nap f3p\\n\\n1 7\\n\\n1\\n\\n1\\n\\n2 7\\n\\nnp\\n\\naD\\n\\n2.72\\n\\n1\\n\\n0.1\\n\\n1\\n2\\n\\n1\\n2.72\\n0.5 0.4\\n\\n1\\n3\\n\\n~ 7\\n\\n1\\n\\n0.5 0.4\\n\\n1 10\\n\\n1\\n\\n1\\n\\n12. 10\\n\\n1\\n\\n1 15\\n\\n1\\n\\n1 20\\n\\n1\\n\\nf3D\\n\\nThe task was to shift a pattern 1 unit with each time step. A block of k of n units was\\nactive. The parameters of the kernel functions (aand /3), the number of values sampled\\nfrom the kernel (the number of time slices used to estimate the integral), np and nD, and\\nthe number of steps used to begin the reconstruction, nr (usually n, = 1) are given in the\\ntable. The last column of the table (Time) reports the number of iterations required for\\nperfect reconstruction.\\n\\n\\f154\\n\\nP. W. Munro and G. Hernandez\\n\\nTable 2 .. Results of the sequential shifter task using as parameters:\\n\\nnr =1; np =1;\\n\\naD =0.125; a p=O.5; fJI =-aD *e*O.35; fJp=ap *e*O.8.\\n\\n~ n\\n\\nnD\\n\\nTime\\n\\n1 7\\n\\n6\\n\\n288\\n\\n~ 7\\n~ 7\\n\\n5\\n\\n96\\n\\n4\\n\\n64\\n\\nFor the above results, the k active units were always adjacent with respect to the shifting\\ndirection. For cases with noncontiguous active units, reconstruction was never exact.\\nNetworks trained with LTP alone would saturate, but would converge to a sequence\\n\\\"close\\\" to the target (Fig. 3) if an LTD term was added.\\nSequence\\n\\nReconstruction\\n\\nClean\\n\\na\\n\\nG\\n\\nNoisy\\n\\naaaaa\\n\\nLTP alone\\n\\na1a2 1\\na 4G!G' ia6 7\\na\\n\\n1 2 1 4 !'i 6 7\\n\\n? a.aaaa\\n? a.aaaa\\naa.a.aa\\na.a.aaa\\naaaa.a.\\naaaa.a.\\n? aaaa.a\\na.aaaa.\\n\\nLTP&LTD\\n\\na1a2a.0 a4!'i67\\naaa\\n\\na1G2 :G>a4 !a' ia6 a7\\na.aaaa ?\\n\\na.aaaa ?\\n.a.aaaa\\n? ? ? ? aaa\\naa ? ? ? ? a\\naaa ? ? ? ?\\n.aaa ???\\na.aaa ??\\n? ?? aaa.\\n\\n???????\\n???????\\n???????\\n???????\\n???????\\n???????\\n? ??????\\n\\nFigure 3. This base pattern (k=2, n=7) with noncontiguous active units was\\npresented as a shifted sequence with noise. The target sequence is partially\\nreconstructed only when LTP and LTD are used together.\\n5.1\\n\\nTime Series Reconstruction Results\\n\\nA network of just four units was trained for hundreds of iterations, the units were each\\nexternally driven by a sinusoidally varying input. Networks trained with LTP alone fail\\nto reconstruct the time series on units deprived of external input during testing. In these\\nsimulations, there is no noise in the patterns, but LTO is shown to be necessary for\\nreconstruction of the patterns (Fig. 4).\\nI'~.\\n\\nf:\\n\\n!',\\nI I\\n\\n?\\n\\n= .\\n\\n.\\n\\n:.\\n\\n:\\n::\\nII\\n\\n:\\n\\n.:1\\n? I\\n\\n~.\\n\\nl:\\n\\n..... '\\n\\n,:\\n\\n:'I ~\\n\\n:\\n\\n.' \\\",\\n\\n??:\\n\\n~\\n\\n?\\n\\nII\\n\\n.\\n\\nI\\n\\nI.\\nI.:': I\\n~I\\n\\n:\\n\\n~\\nI\\n\\n'I\\n\\n~.\\n\\n~,\\n\\n~\\n\\n'\\\"I\\n\\nII\\n\\n.'~ ........ ~!.........'ll ..........,,:........:.;. ...\\n\\nIJ\\n\\nI .\\n\\nu\\n\\n? ? ?\\\\ . : ? ? ? ? ? ? ? ?\\n\\nI\\n\\n~,:\\n\\n?? ? ?? ?\\n\\n????? ? ?,?y ?? ?????rl:????? ? ? ?.~?????? ??::.? ? ? ????\\\"!I? ???? ? ?? ~~??? ?? ?? ?~I\\nI\\n\\nI\\n\\nI\\n\\n:\\n\\n\\\\\\n\\n? \\\"\\n\\nI\\n\\nI\\n\\n:\\n\\n~J\\n\\n.;\\n\\n??\\n!\\ni\\n\\n~\\n\\nFigure 4. Reconstruction of sinusoids. Target signals , from training (dashed)\\nplotted with reconstructed signals (solid). Left: The best reconstruction using LTP\\nalone. Right: A typical result with LTP and LTD together.\\nFor high values of ap and aD, the reconstruction of sinusoids is very sensitive to the\\nvalues of fJD and fJP. Figure 5 shows the results when IfJD I and fJp values are close. In\\nthe first case (top), when IfJD I is slightly smaller than f3p , the first two neurons\\n(from left to right) saturate. And, in the contrary case (bottom) the first two neurons\\n\\n\\fLTD Facilitates Learning in a Noisy Environment\\n\\n155\\n\\nshow almost null activation. However, the third and fourth neurons (from left to\\nright) in both cases (top and bottom) show predictive behavior.\\n:~\\n\\n~\\n.'\\nI\\n\\n~\\nII\\nI:\\n\\\"\\n\\n\\\"\\n\\n\\\"\\n\\nI.\\n\\nI\\n\\n? '\\\\\\n\\n::\\n\\n::.\\n\\nI\\n\\n~\\n\\nI'\\n\\nI'\\n\\n.:\\n\\n\\\" I\\n\\n::\\n\\n,\\n\\nI\\n\\n\\\"\\n\\n,i\\n\\n;: ;: ; ~\\n.'\\n\\nI:\\n\\nI:\\n\\n?\\n\\n~:\\n\\n'\\n\\nl'\\n\\nII'!\\n\\nI\\n\\n?\\n\\n.\\\"\\n\\nIt.\\n\\nI\\n\\nI\\nI\\n\\n::\\n\\n::\\n\\n?\\n\\nI,\\n\\n'.\\n\\n.:\\n\\nI,\\n\\n:\\n\\nI\\n\\n:.\\n\\n? ? I : : ;: ? ? ? ? ?\\n\\nI\\n\\nI\\n\\n. :~::;!;~~~\\n:: I: ~ : :: ~: .' : .!\\n\\n~!::\\n\\n' .\\n\\nI I\\n\\n\\\\ .\\n\\n::\\n\\n::\\n\\n::\\n\\n::\\n\\n~:\\n\\n~.I\\n\\n~; ~I?~\\n\\n:. :.\\n\\n.'\\n\\n0'\\n\\n.;\\n\\n? :.:\\n\\n.'\\n\\n\\\"\\n\\n'.\\n\\n\\\\\\n\\nI. .:\\n\\n~:\\n\\n~:\\n\\n~:\\n\\n??\\n~\\nI,'\\n~I\\n.. ~\\n~r ...?.????\\n\\\\ .??.??.?...'J. .?.??...?.?.....!C,\\n\\n.\\n\\n. ..\\n\\n? I\\n\\n..\\n\\n::\\n\\nt ?;\\n\\n,\\n\\nI\\n\\n:'\\n\\nI\\n\\n?\\n\\nI\\n\\nI\\n\\nI\\n\\nI\\n\\n?\\n\\nI\\n\\n?\\n\\nI\\n\\n?\\n\\nI.\\n\\nI\\n\\nI\\n\\nI\\n\\nI.\\n\\n\\\"\\n\\nI:\\n\\nI I\\n\\nI:\\n\\n: :\\n\\n~I\\n? I\\n\\n~I\\n? I\\n\\n~I\\n? I\\n\\n~I\\n? I\\n\\n., I :\\n\\n~!\\n\\n.~!\\n\\n.~\\n\\n.~!\\n\\n't! j\\n\\n:\\n\\n,I ;\\n\\n,\\n\\n{??~??l? ? ??\\n\\n?\\n\\nIt\\n\\n'\\n\\nI\\n\\nI\\n\\n...\\n\\n:.\\n\\nI\\n\\nI\\n\\n'\\\\\\n\\n'.\\n\\n'\\n\\nI,\\n\\nI\\n\\nI\\n\\nI\\n\\n,\\n\\nI\\n\\nI\\n\\nI\\n\\nI\\n\\n?\\n\\nI\\n\\n?\\n\\n,-\\n\\n:\\n\\n\\\"\\n\\n?\\n\\n.. t .\\n\\nI\\n\\nc\\n\\n,, : ct\\n,\\n\\n' .~\\n\\nI\\n\\n':\\n\\nI\\n\\n'I :\\n\\nI\\n\\n,;\\n\\n,\\n\\n., ,:\\n\\n. ?... ... ...... . . . . . ........ .\\n,\\n\\nj\\n~\\n\\n~\\n\\n, ,,\\n\\n.. , .. .\\n\\n,1\\n. ~:\\n\\n. :\\n\\n~~'~\\n\\n:\\n\\np\\n\\n\\\"\\n\\n'I:':\\n?\\n\\n?\\n?\\n\\n....\\n\\nI\\n\\n\\\"\\n:\\n\\n.:\\n\\n. . . ,.\\n\\ni\\n\\n,?\\n;\\n\\n::,:J\\n\\nI\\n\\nI\\n\\n_.\\n\\n:\\n\\n?\\n\\n:,=\\n\\n?\\n\\n\\\"; ?\\n\\n: \\\" };'\\n\\nI\\n\\nI'\\n\\n~:;\\n\\n~\\n\\n\\\"\\n\\nI:\\n\\n. . .. . . ..... 0\\\"<\\\".\\n\\n:: .' I::\\n\\n~\\n\\nI:\\n\\n??\\n\\n?\\n\\n_:\\n\\nI :\\n?\\n\\n....\\n\\n' 1\\\"'~\\\" '\\\"\\n\\n.~\\n\\nI:\\n\\n?\\n\\n.\\\"+ ??_.-.\\n\\n~\\n\\nI{\\n\\n_~-. :.-.-.~\\n\\n\\\\ ~\\nI ._._~.......?......j\\n\\n:. . ::\\n\\nI!\\n\\n..\\n\\nI ,\\n\\n\\\"\\n\\n,\\n\\nII\\n\\n,\\n\\n, ,\\n\\n..\\n\\n..\\n\\n\\\"\\n\\nr:\\n\\n:~\\n\\nI\\n\\nI\\n\\n:'\\\"\\n\\n\\\" ,\\n\\n, \\\"\\n\\nI,\\n\\n~\\n\\nI\\n\\nI\\n\\nI\\n\\n,\\n\\n,\\n\\n.\\n\\n, ,!\\n,? 'I\\n\\n.,\\n\\n\\\\\\n\\n\\\"\\n\\\"\\n\\n,'\\nI:.\\n, \\\"\\n, ,\\\"\\n\\n::\\n\\nI\\n\\n:.\\n\\n:\\n\\n; I\\n\\n.. ......\\n\\nI.\\n\\n?\\n\\n,\\n\\n\\\"\\\"\\n\\n.,\\n,,\\n\\n..\\n\\nI\\n\\nII\\n\\n,, ,,\\n\\n,\\n\\n~\\n\\nI. :\\n\\n..\\n\\n:, :,\\n\\nI\\n\\n?\\n\\nIt\\n\\nIt\\n\\n:\\n\\n:\\n\\n:'\\\"\\n\\n,\\n\\n, ,\\n\\n~\\nI,.\\n.. i\\n\\n?\\n\\nII\\n\\n? ?? ???~??????:t???? ??ft??u\\n\\nI\\n\\nI\\n\\n(\\n\\nI\\n\\n.??. I1 .......... A: ? ? ?????\\\\ .??? -....4: .... . .. .l ...?\\n\\n?\\n\\n:\\n\\n?\\n\\n,\\n\\n.'\\n\\nII\\n\\n'\\\\\\n\\n:\\n\\n?\\n\\n.\\n\\nI: !I =- '\\\\\\n:: ;~ :: ,::, :'~ ~ 1\\n\\n:~\\nI'\\nI:\\n\\nI.\\n\\nI.\\n\\n;\\n\\n~\\n.'\\nI:\\n\\nI,:\\n\\n~\\n\\nI\\nI\\n\\n:\\n\\nt.. .. .\\n\\nI\\n\\n:\\n\\nt\\n\\nFigure 5. Reconstruction of sinusoids . Examples of target signals from training\\n(dashed) plotted with reconstructed signals (solid). Top: When IfJD kfJp. Bottom:\\nWhen IfJDI> fJp .\\n6\\n\\nDiscussion\\n\\nIn the half century that has elapsed since Hebb articulated his neurophysiological\\npostulate, the neuroscience community has corne to recognize its fundamental role\\nin plasticity. Hebb's hypothesis clearly transcends its original motivation to give a\\nneurophysiologically based account of associative memory.\\nThe phenomenon of LTP provides direct biological support for Hebb's postulate,\\nand hence has clear cognitive implications. Initially after its discovery in the\\nlaboratory, the computational role of LTD was thought to be the flip side of LTP.\\nThis interpretation would have synapses strengthen when activities are correlated\\nand have them weaken when they are anti-correlated. Such a theory is appealing for\\nits elegance, and has formed the basis many network models [19]-[20]. However,\\nthe dependence of synaptic change on the relative timing of pre- and post- synaptic\\nactivity that has recently been shown in the laboratory is inconsistent with this story\\nand calls for a computational interpretation. A network trained with such a learning\\nrule cannot converge to a state where the weights are symmetric, for example, since\\nLiWij 7:- LiWji.\\n\\nWhile the simulations reported here are simple and preliminary, they illustrate two\\ntasks that benefit from the inclusion of time-dependent LTD. In the case of the\\nsequential shifter, an examination of more complex predictive tasks is planned in\\nthe near future. It is expected that this will require architectures with unclamped\\n(hidden) units. The role of LTD here is to temporally enhance contrast, in a way\\nanalogous to the role of lateral inhibition for computing spatial contrast\\nenhancement in the retina. The time-series example illustrates the possible role of\\nLTD for providing temporal context.\\n\\n\\fP. W. Munro and G. Hernandez\\n\\nJ56\\n\\n7\\n\\nReferences\\n\\n[1] Bliss TVP & Lcllmo T (1973) Long-lasting potentiation of synaptic in the dentate area of\\nthe un anaesthetized rabbit following stimulation of the perforant path.J Physiol 232:331-356\\n[2] Malenka RC (1995) LTP and LTD: dynamic and interactive processes of synaptic\\nplasticity. The Neuroscientist 1:35-42.\\n[3] Hebb DO (1949) The Organization of Behavior. Wiley: NY.\\n[4] Stent G (1973) A physiological, mechanism for Hebb's postulate of learning. Proc. Natl.\\nAcad. Sci. USA 70: 997-1001\\n[5] Barrionuevo G, Schottler F & Lynch G (1980) The effects of repetitive low frequency\\nstimulation on control and \\\"pontentiated\\\" synaptic responses in the hippocampus. Life Sci\\n27:2385-2391.\\n[6] Thiels E, Xie X, Yeckel MF, Barrionuevo G & Berger TW (1996) NMDA Receptordependent LTD in different subfields of hippocampus in vivo and in vitro. Hippocampus\\n6:43-51.\\n[7] Sejnowski T J (1977) Storing covariance '!Vith nonlinearly interacting neurons. 1. Math.\\nBioL 4:303-321.\\n[8] Sutton RS (1988) Learning to predict by the methods of temporal difference. Machine\\nLearning. 3:9-44\\n[9] Sompolinsky H and Kanter I (1986) Temporal association in asymmetric neural networks.\\nPhys.Rev.Letter. 57:2861-2864.\\n[10] Gerstner W, Kempter R, van Hemmen JL & Wagner H (1996) A neuronal learning rule\\nfor sub-millisecond temporal coding. Nature 383:76-78 .\\n[11] Kempter R, Gerstner W & van Hemmen JL (1999) Spike-based compared to rate-based\\nhebbian learnin g. Kearns , Ms. , Solla, S.A and Cohn, D.A. Eds. Advances in Neural\\nInformation Processing Systems J J. MIT Press, Cambridge MA.\\n[12] Kempter R, Gerstner W, van Hemmen JL & Wagner H (1996) Temporal coding in the\\nsub-millisecond range: Model of barn owl auditory pathway. Touretzky, D.S, Mozer, M.C,\\nHasselmo, M.E, Eds. Advances in Neural Information Processing Systems 8. MIT Press,\\nCambridge MA pp.124-130.\\n[13] Markram H, Lubke J, Frotscher M & Sakmann B (1997) Regulation of synaptic efficacy\\nby coincidence of postsynaptic Aps and EPPSPs. Science 275 :213-215 .\\n[14] Markram H & Tsodyks MV (1996) Redistribution of synaptic efficacy between\\nneocortical pyramidal neurons. Nature 382:807-810.\\n[15] Zhang L, Tao HW, Holt CE & Poo M (1998) A critical window for cooperation and\\ncompetition among developing retinotectal synapses. Nature 35:37-44\\n[16] Abbott LF, & Blum KI (1996) Functional significance of long-term potentiation for\\nsequence learning and prediction. Cerebral Cortex 6: 406-416.\\n[17] Abbott LF, & Song S (1999) Temporally asymmetric hebbian learning, spike timing and\\nneuronal response variability. Kearns, Ms., Solla, S.A and Cohn, D.A. Eds. Advances in\\nNeural Information Processing Systems J J. MIT Press, Cambridge MA.\\n[18] Goldman MS, Nelson SB & Abbott LF (1998) Decorrelation of spike trains by synaptic\\ndepression. Neurocomputing (in press).\\n[19] Hopfield J (1982) Neural networks and physical systems with emergent collective\\n.\\ncomputational properties. Proc. Natl. Acad. Sci. USA. 79:2554-2558.\\n[20] Ackley DH, Hinton GE, Sejnowski TJ (1985) A learning algorithm for Boltzmann\\nmachines. Cognitive Science 9:147-169.\\n\\n\\f\",\n          \"A computational model of hippocampal function in\\ntrace conditioning\\n\\nElliot A. Ludvig, Richard S. Sutton, Eric Verbeek\\nDepartment of Computing Science\\nUniversity of Alberta\\nEdmonton, AB, Canada T6G 2E8\\n{elliot,sutton,everbeek}@cs.ualberta.ca\\n\\nE. James Kehoe\\nSchool of Psychology\\nUniversity of New South Wales\\nSydney, NSW, Australia 2052\\nj.kehoe@unsw.edu.au\\n\\nAbstract\\nWe introduce a new reinforcement-learning model for the role of the hippocampus in classical conditioning, focusing on the differences between trace and delay conditioning. In the model, all stimuli are represented both as unindividuated wholes and as a series of temporal elements with varying delays. These two\\nstimulus representations interact, producing different patterns of learning in trace\\nand delay conditioning. The model proposes that hippocampal lesions eliminate\\nlong-latency temporal elements, but preserve short-latency temporal elements. For\\ntrace conditioning, with no contiguity between cue and reward, these long-latency\\ntemporal elements are necessary for learning adaptively timed responses. For delay conditioning, the continued presence of the cue supports conditioned responding, and the short-latency elements suppress responding early in the cue. In accord\\nwith the empirical data, simulated hippocampal damage impairs trace conditioning, but not delay conditioning, at medium-length intervals. With longer intervals,\\nlearning is impaired in both procedures, and, with shorter intervals, in neither. In\\naddition, the model makes novel predictions about the response topography with\\nextended cues or post-training lesions. These results demonstrate how temporal\\ncontiguity, as in delay conditioning, changes the timing problem faced by animals,\\nrendering it both easier and less susceptible to disruption by hippocampal lesions.\\nThe hippocampus is an important structure in many types of learning and memory, with prominent\\ninvolvement in spatial navigation, episodic and working memories, stimulus configuration, and contextual conditioning. One empirical phenomenon that has eluded many theories of the hippocampus\\nis the dependence of aversive trace conditioning on an intact hippocampus (but see Rodriguez &\\nLevy, 2001; Schmajuk & DiCarlo, 1992; Yamazaki & Tanaka, 2005). For example, trace eyeblink\\nconditioning disappears following hippocampal lesions (Solomon et al., 1986; Moyer, Jr. et al.,\\n1990), induces hippocampal neurogenesis (Gould et al., 1999), and produces unique activity patterns in hippocampal neurons (McEchron & Disterhoft, 1997). In this paper, we present a new abstract computational model of hippocampal function during trace conditioning. We build on a recent\\nextension of the temporal-difference (TD) model of conditioning (Ludvig, Sutton & Kehoe, 2008;\\nSutton & Barto, 1990) to demonstrate how the details of stimulus representation can qualitatively\\nalter learning during trace and delay conditioning. By gently tweaking this stimulus representation\\nand reducing long-latency temporal elements, trace conditioning is severely impaired, whereas delay conditioning is hardly affected. In the model, the hippocampus is responsible for maintaining\\nthese long-latency elements, thus explaining the selective importance of this brain structure in trace\\nconditioning.\\nThe difference between trace and delay conditioning is one of the most basic operational distinctions\\nin classical conditioning (e.g., Pavlov, 1927). Figure 1 is a schematic of the two training procedures.\\nIn trace conditioning, a conditioned stimulus (CS) is followed some time later by a reward or uncon1\\n\\n\\fTrace\\n\\nDelay\\n\\nStimulus\\nReward\\n\\nFigure 1: Event timelines in trace and delay conditioning. Time flows from left-to-right in the diagram. A vertical bar represents a punctate (short) event, and the extended box is a continuously\\navailable stimulus. In delay conditioning, the stimulus and reward overlap, whereas, in trace conditioning, there is a stimulus-free gap between the two punctate events.\\n\\nditioned stimulus (US); the two stimuli are separated by a stimulus-free gap. In contrast, in delay\\nconditioning, the CS remains on until presentation of the US. Trace conditioning is learned more\\nslowly than delay conditioning, with poorer performance often observed even at asymptote.\\nIn both eyeblink conditioning (Moyer, Jr. et al., 1990; Solomon et al., 1986; Tseng et al., 2004)\\nand fear conditioning (e.g., McEchron et al., 1998), hippocampal damage severely impairs the acquisition of conditioned responding during trace conditioning, but not delay conditioning. These\\nselective hippocampal deficits with trace conditioning are modulated by the inter-stimulus interval\\n(ISI) between CS onset and US onset. With very short ISIs (?300 ms in eyeblink conditioning in\\nrabbits), there is little deficit in the acquisition of responding during trace conditioning (Moyer, Jr.\\net al., 1990). Furthermore, with very long ISIs (>1000 ms), delay conditioning is also impaired\\nby hippocampal lesions (Beylin et al., 2001). These interactions between ISI and the hippocampaldependency of conditioning are the primary data that motivate the new model.\\n\\n1\\n\\nTD Model of Conditioning\\n\\nOur full model of conditioning consists of three separate modules: the stimulus representation,\\nlearning algorithm, and response rule. The explanation of hippocampal function relies mostly on\\nthe details of the stimulus representation. To illustrate the implications of these representational\\nissues, we have chosen the temporal-difference (TD) learning algorithm from reinforcement learning\\n(Sutton & Barto, 1990, 1998) that has become the sine qua non for modeling reward learning in\\ndopamine neurons (e.g., Ludvig et al., 2008; Schultz, Dayan, & Montague, 1997), and a simple,\\nleaky-integrator response rule described below. We use these for simplicity and consistency with\\nprior work; other learning algorithms and response rules might also yield similar conclusions.\\n1.1\\n\\nStimulus Representation\\n\\nIn the model, stimuli are not coherent wholes, but are represented as a series of elements or internal\\nmicrostimuli. There are two types of elements in the stimulus representation: the first is the presence\\nmicrostimulus, which is exactly equivalent to the external stimulus (Sutton & Barto, 1990). This microstimulus is available whenever the corresponding stimulus is on (see Fig. 3). The second type\\nof elements are the temporal microstimuli or spectral traces, which are a series of successively later\\nand gradually broadening elements (see Grossberg & Schmajuk, 1989; Machado, 1997; Ludvig et\\nal., 2008). Below, we show how the interaction between these two types of representational elements produces different styles of learning in delay and trace conditioning, resulting in differential\\nsensitivity of these procedures to hippocampal manipulation.\\nThe temporal microstimuli are created in the model through coarse coding of a decaying memory\\ntrace triggered by stimulus onset. Figure 2 illustrates how this memory trace (left panel) is encoded\\nby a series of basis functions evenly spaced across the height of the trace (middle panel). Each basis\\nfunction effectively acts as a receptive field for trace height: As the memory trace fades, different\\nbasis functions become more or less active, each with a particular temporal profile (right panel).\\nThese activity profiles for the temporal microstimuli are then used to generate predictions of the US.\\nFor the basis functions, we chose simple Gaussians:\\n1\\n(y ? ?)2\\nf (y, ?, ?) = ? exp(?\\n).\\n2? 2\\n2?\\n2\\n\\n(1)\\n\\n\\f0.4\\n\\nMicrostimulus Level\\n\\nTrace Height\\n\\n1\\n0.75\\n\\n+\\n\\n0.5\\n0.25\\n0\\n\\n0\\n\\n20\\n\\n40\\n\\n60\\n\\nTime Step\\n\\n0.3\\n0.2\\n0.1\\n0\\n\\nTemporal Basis\\nFunctions\\n\\n0\\n\\n20\\n\\n40\\n\\n60\\n\\nTime Step\\n\\nFigure 2: Creating Microstimuli. The memory traces for a stimulus (left) are coarsely coded by\\na series of temporal basis functions (middle). The resultant time courses (right) of the temporal\\nmicrostimuli are used to predict future occurrence of the US. A single basis function (middle) and\\napproximately corresponding microstimulus (right) have been darkened. The inset in the right panel\\nshows the levels of several microstimuli at the time indicated by the dashed line.\\n\\nGiven these basis functions, the microstimulus levels xt (i) at time t are determined by the corresponding memory trace height:\\nxt (i) = f (yt , i/m, ?)yt ,\\n(2)\\nwhere f is the basis function defined above and m is the number of temporal microstimuli per\\nstimulus. The trace level yt was set to 1 at stimulus onset and decreased exponentially, controlled\\nby a single decay parameter, which was allowed to vary to simulate the effects of hippocampal\\nlesions. Every stimulus, including the US, was represented by a single memory trace and resultant\\nmicrostimuli.\\n1.2\\n\\nHippocampal Damage\\n\\nWe propose that hippocampal damage results in the selective loss of the long-latency temporal elements of the stimulus representation. This idea is implemented in the model through a decrease in\\nthe memory decay constant from .985 to .97, approximately doubling the decay rate of the memory\\ntrace that determines the microstimuli. In effect, we assume that hippocampal damage results in a\\nmemory trace that decays more quickly, or, equivalently, is more susceptible to interference. Figure\\n3 shows the effects of this parameter manipulation on the time course of the elements in the stimulus\\nrepresentation. The presence microstimulus is not affected by this manipulation, but the temporal\\nmicrostimuli are compressed for both the CS and the US. Each microstimulus has a briefer time\\ncourse, and, as a group, they cover a shorter time span. Other means for eliminating or reducing\\nthe long-latency temporal microstimuli are certainly possible and would likely be compatible with\\nour theory. For example, if one assumes that the stimulus representation contains multiple memory\\ntraces with different time constants, each with a separate set of microstimuli, then eliminating the\\nslower memory traces would also remove the long-latency elements, and many of the results below\\nhold (simulations not shown). The key point is that hippocampal damage reduces the number and\\nmagnitude of long-latency microstimuli.\\n1.3\\n\\nLearning and Responding\\n\\nThe model approaches conditioning as a reinforcement-learning prediction problem, wherein the\\nagent tries to predict the upcoming rewards or USs. The model learns through linear TD(?) (Ludvig\\net al., 2008; Schultz et al., 1997; Sutton, 1988; Sutton & Barto, 1990, 1998). At each time step, the\\nUS prediction (Vt ) is determined by:\\n\\nVt (x) =\\n\\nbwtT xc0\\n\\n=\\n\\n$ n\\nX\\ni=1\\n\\n3\\n\\n%\\nwt (i)x(i)\\n\\n,\\n0\\n\\n(3)\\n\\n\\fMicrostimulus Level\\n\\nNormal\\n\\nHippocampal\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n\\n0.6\\n\\n0.4\\n\\n0.4\\n\\n0.2\\n\\n0.2\\n\\n0\\n\\n0\\n0\\n\\n500\\n\\n1000\\n\\n0\\n\\n500\\n\\n1000\\n\\nTime (ms)\\nFigure 3: Hippocampal effects on the stimulus representation. The left panel presents the stimulus\\nrepresentation in delay conditioning with the normal parameter settings, and the right panel presents\\nthe altered stimulus representation following simulated hippocampal damage. In the hippocampal\\nrepresentation, the temporal microstimuli for both CS (red, solid lines) and US (green, dashed lines)\\nare all briefer and shallower. The presence microstimuli (blue square wave and black spike) are not\\naffected by the hippocampal manipulation.\\n\\nwhere x is a vector of the activation levels x(i) for the various microstimuli, wt is a corresponding\\nvector of adjustable weights wt (i) at time step t, and n is the total number of all microstimuli. The\\nUS prediction is constrained to be non-negative, with negative values rectified to 0. As is standard\\nin TD models, this US prediction is compared to the reward received and the previous US prediction\\nto generate a TD error (?t ):\\n?t = rt + ?Vt (xt ) ? Vt (xt?1 ),\\n\\n(4)\\n\\nwhere ? is a discount factor that determines the temporal horizon of the US prediction. This TD\\nerror is then used to update the weight vector based on the following update rule:\\nwt+1 = wt + ??t et ,\\n\\n(5)\\n\\nwhere ? is a step-size parameter and et is a vector of eligibility trace levels (see Sutton & Barto,\\n1998), which together help determine the speed of learning. Each microstimulus has its own corresponding eligibility trace which continuously decays, but accumulates whenever that microstimulus\\nis present:\\net+1 = ??et + xt ,\\n(6)\\nwhere ? is the discount factor as above and ? is a decay parameter that determines the plasticity\\nwindow. These US predictions are translated into responses through a simple, thresholded leakyintegrator response rule:\\nat+1 = ?at + bVt+1 (xt )c? ,\\n\\n(7)\\n\\nwhere ? is a decay constant, and ? is a threshold on the value function V .\\nOur model is defined by Equations 1-7 and 7 additional parameters, which were fixed at the following values for the simulations below: ? = .95, ? = .005, ? = .97, n = 50, ? = .08, ? = .93, ? = .25. In\\nthe simulated experiments, one time step was interpreted as 10 ms.\\n4\\n\\n\\fCR Magnitude\\n\\nISI250\\n\\n5\\n\\n4\\n3\\n\\n3\\nDelay!Normal\\n2\\nDelay!HPC\\nTrace!Normal 1\\nTrace!HPC\\n0\\n250\\n500\\n50\\n\\n3\\n2\\n1\\n50\\n\\nISI1000\\n\\n5\\n\\n4\\n\\n4\\n\\n0\\n\\nISI500\\n\\n5\\n\\n2\\n1\\n250\\n\\n500\\n\\n0\\n\\n50\\n\\n250\\n\\n500\\n\\nTrials\\nFigure 4: Learning in the model for trace and delay conditioning with and without hippocampal\\n(HPC) damage. The three panels present training with different interstimulus intervals (ISI).\\n\\n2\\n\\nResults\\n\\nWe simulated 12 total conditions with the model: trace and delay conditioning, both with and without hippocampal damage, for short (250 ms), medium (500 ms), and long (1000 ms) ISIs. Each\\nsimulated experiment was run for 500 trials, with every 5th trial an unreinforced probe trial, during\\nwhich no US was presented. For delay conditioning, the CS lasted the same duration as the ISI and\\nterminated with US presentation. For trace conditioning, the CS was present for 5 time steps (50\\nms). The US always lasted for a single time step, and an inter-trial interval of 5000 ms separated\\nall trials (onset to onset). Conditioned responding (CR magnitude) was measured as the maximum\\nheight of the response curve on a given trial.\\n\\n0.8\\n\\nCR Magnitude\\n\\nUS Prediction\\n\\nFigure 4 summarizes our results. The figure depicts how the CR magnitude changed across the 500\\ntrials of acquisition training. In general, trace conditioning produced lower levels of responding\\nthan delay conditioning, but this effect was most pronounced with the longest ISI. The effects of\\nsimulated hippocampal damage varied with the ISI. With the shortest ISI (250 ms; left panel), there\\nwas little effect on responding in either trace or delay conditioning. There was a small deficit early in\\ntraining with trace conditioning, but this difference disappeared quickly with further training. With\\nthe longest ISI (1000 ms; right panel), there was a profound effect on responding in both trace and\\ndelay conditioning, with trace conditioning completely eliminated. The intermediate ISI (500 ms;\\nmiddle panel) produced the most complex and interesting results. With this interval, there was only\\na minor deficit in delay conditioning, but a substantial drop in trace conditioning, especially early in\\ntraining. This pattern of results roughly matches the empirical data, capturing the selective deficit in\\ntrace conditioning caused by hippocampal lesions (Solomon et al., 1986) as well as the modulation\\nof this deficit by ISI (Beylin et al., 2001; Moyer, Jr. et al., 1990).\\n\\nDelay\\nTrace\\n\\n0.6\\n0.4\\n0.2\\n0\\n\\n0\\n\\n250\\n\\n500\\n\\n750\\n\\nTime (ms)\\n\\n5\\n4\\n3\\n2\\n1\\n0\\n\\n0\\n\\n250\\n\\n500\\n\\n750\\n\\nTime (ms)\\n\\nFigure 5: Time course of US prediction and CR magnitude for both trace (red, dashed line) and\\ndelay conditioning (blue, solid line) with a 500-ms ISI.\\n\\n5\\n\\n\\fThese differences in sensitivity to simulated hippocampal damage arose despite similar model performance during normal trace and delay conditioning. Figure 5 shows the time course of the US\\nprediction (left panel) and CR magnitude (right panel) after trace and delay conditioning on a probe\\ntrial with a 500-ms ISI. In both instances, the US prediction grew throughout the trial as the usual\\ntime of the US became imminent. Note the sharp drop off in US prediction for delay conditioning\\nexactly as the CS terminates. This change reflects the disappearance of the presence microstimulus,\\nwhich supports much of the responding in delay conditioning (see Fig. 6). In both procedures, even\\nafter the usual time of the US (and CS termination in the case of delay conditioning), there was still\\nsome residual US prediction. These US predictions were caused by the long-latency microstimuli,\\nwhich did not disappear exactly at CS offset, and were ordinarily (on non-probe trials) countered by\\nnegative weights on the US microstimuli. The CR magnitude tracked the US prediction curve quite\\nclosely, peaking around the time the US would have occurred for both trace and delay conditioning.\\nThere was little difference in either curve between trace and delay conditioning, yet altering the\\nstimulus representation (see Fig. 3) had a more pronounced effect on trace conditioning.\\nAn examination of the weight distribution for trace and delay conditioning explains why hippocampal damage had a more pronounced effect on trace than delay conditioning. Figure 6 depicts some\\nrepresentative microstimuli (left column) as well as their corresponding weights (right columns) following trace or delay conditioning with or without simulated hippocampal damage. For clarity in\\nthe figure, we have grouped the weights into four categories: positive (+), large positive (+++), negative (-), and large negative (--). The left column also depicts how the model poses the computational\\nproblem faced by an animal during conditioning; the goal is to sum together weighted versions of the\\navailable microstimuli to produce the ideal US prediction curve in the bottom row. In normal delay\\nconditioning, the model placed a high positive weight on the presence microstimulus, but balanced\\nthat with large negative weights on the early CS microstimuli, producing a prediction topography\\nthat roughly matched the ideal prediction (see Fig. 5, left panel). In normal trace conditioning, the\\nmodel only placed a small positive weight on the presence microstimulus, but supplemented that\\nwith large positive weights on both the early and late CS microstimuli, also producing a prediction\\ntopography that roughly matched the ideal prediction.\\n\\nWeights\\nNormal\\n\\nCS Presence Stimulus\\n\\nCS Early Microstimuli\\n\\nCS Late Microstimuli\\n\\nUS Early Microstimuli\\n\\nHPC Lesion\\n\\nDelay\\n\\nTrace\\n\\nDelay\\n\\nTrace\\n\\n+++\\n\\n+\\n\\n+++\\n\\n+\\n\\n--\\n\\n+\\n\\n--\\n\\n+\\n\\n+\\n\\n+++\\n\\nN/A\\n\\nN/A\\n\\n-\\n\\n--\\n\\n-\\n\\n-\\n\\nIdeal Summed Prediction\\n\\nFigure 6: Schematic of the weights (right columns) on various microstimuli following trace and\\ndelay conditioning. The left column illustrates four representative microstimuli: the presence microstimulus, an early CS microstimulus, a late CS microstimulus, and a US microstimulus. The\\nideal prediction is the expectation of the sum of future discounted rewards.\\n\\n6\\n\\n\\fFollowing hippocampal lesions, the late CS microstimuli were no longer available (N/A), and the\\nsystem could only use the other microstimuli to generate the best possible prediction profile. In\\ndelay conditioning, the loss of these long-latency microstimuli had a small effect, notable only with\\nthe longest ISI (1000 ms) with these parameter settings. With trace conditioning, the loss of the\\nlong-latency microstimuli was catastrophic, as these microstimuli were usually the major basis for\\nthe prediction of the upcoming US. As a result, trace conditioning became much more difficult (or\\nimpossible in the case of the 1000-ms ISI), even though delay conditioning was less affected.\\nThe most notable (and defining) difference between trace and delay conditioning is that the CS and\\nUS overlap in delay conditioning, but not trace conditioning. In our model, this overlap is necessary,\\nbut not sufficient, for the the unique interaction between the presence microstimulus and temporal\\nmicrostimuli in delay conditioning. For example, if the CS were extended to stay on beyond the\\ntime of US occurrence, this contiguity would be maintained, but negative weights on the early CS\\nmicrostimuli would not suffice to suppress responding throughout this extended CS. In this case, the\\nbest solution to predicting the US for the model might be to put high weights on the long-latency\\ntemporal microstimuli (as in trace conditioning; see Fig 6), which would not persist as long as the\\nnow extended presence microstimulus. Indeed, with a CS that was three times as long as the ISI, we\\nfound that the US prediction, CR magnitude, and underlying weights were completely indistinguishable from trace conditioning (simulations not shown). Thus, the model predicts that this extended\\ndelay conditioning should be equally sensitive to hippocampal damage as trace conditioning for\\nthe same ISIs. This empirical prediction is a fundamental test of the representational assumptions\\nunderlying the model.\\nThe particular mechanism that we chose for simulating the loss of the long-latency microstimuli\\n(increasing the decay rate of the memory trace) also leads to a testable model prediction. If one\\nwere to pre-train an animal with trace conditioning and then perform hippocampal lesions, there\\nshould be some loss of responding, but, more importantly, those CRs that do occur should appear\\nearlier in the interval because the temporal microstimuli now follow a shorter time course (see Fig.\\n3). There is some evidence for additional short-latency CRs during trace conditioning in lesioned\\nanimals (e.g., Port et al., 1986; Solomon et al., 1986), but, to our knowledge, this precise model\\nprediction has not been rigorously evaluated.\\n\\n3\\n\\nDiscussion and Conclusion\\n\\nWe evaluated a novel computational model for the role of the hippocampus in trace conditioning,\\nbased on a reinforcement-learning framework. We extended the microstimulus TD model presented\\nby Ludvig et al. (2008) by suggesting a role for the hippocampus in maintaining long-latency elements of the temporal stimulus representation. The current model also introduced an additional\\nelement to the stimulus representation (the presence microstimulus) and a simple response rule for\\ntranslating prediction into actions; we showed how these subtle innovations yield interesting interactions when comparing trace and delay conditioning. In addition, we adduced a pair of testable\\nmodel predictions about the effects of extended stimuli and post-training lesions.\\nThere are several existing theories for the role of the hippocampus in trace conditioning, including\\nthe modulation of timing (Solomon et al., 1986), establishment of contiguity (e.g., Wallenstein et\\nal., 1998), and overcoming of task difficulty (Beylin et al., 2001). Our new model provides a computational mechanism that links these three proposed explanations. In our model, for similar ISIs,\\ndelay conditioning requires learning to suppress responding early in the CS, whereas trace conditioning requires learning to create responding later in the trial, near the time of the US (see Fig. 6).\\nAs a result, for the same ISI, delay conditioning requires changing weights associated with earlier\\nmicrostimuli than trace conditioning, though in opposite directions. These early microstimuli reach\\nhigher activation levels (see Fig. 2), producing higher eligibility traces, and are therefore learned\\nabout more quickly. This differential speed of learning for short-latency temporal microstimuli corresponds with much behavioural data that shorter ISIs tend to improve both the speed and asymptote\\nof learning in eyeblink conditioning (e.g., Schneiderman & Gormerzano, 1964). Thus, the contiguity between the CS and US in delay conditioning alters the timing problem that the animal faces,\\neffectively making the time interval to be learned shorter, and rendering the task easier for most ISIs.\\nIn future work, it will be important to characterize the exact mathematical properties that constrain\\nthe temporal microstimuli. Our simple Gaussian basis function approach suffices for the datasets\\n7\\n\\n\\fexamined here (cf. Ludvig et al., 2008), but other related mathematical functions are certainly\\npossible. For example, replacing the temporal microstimuli in our model with the spectral traces\\nof Grossberg & Schmajuk (1989) produces results that are similar to ours, but using sequences of\\nGamma-shaped functions tends to fail, with longer intervals learned too slowly relative to shorter\\nintervals. One important characteristic of the microstimulus series seems to be that the heights of\\nindividual elements should not decay too quickly. Another key challenge for future modeling is\\nreconciling this abstract account of hippocampal function in trace conditioning with approaches that\\nconsider greater physiological detail (e.g., Rodriguez & Levy, 2001; Yamazaki & Tanaka, 2005).\\nThe current model also contributes to our understanding of the TD models of dopamine (e.g., Schultz\\net al., 1997) and classical conditioning (Sutton & Barto, 1990). These models have often given short\\nshrift to issues of stimulus representation, focusing more closely on the properties of the learning\\nalgorithm (but see Ludvig et al., 2008). Here, we reveal how the interaction of various stimulus\\nrepresentations in conjunction with the TD learning rule produces a viable model of some of the\\ndifferences between trace and delay conditioning.\\n\\nReferences\\nBeylin, A. V., Gandhi, C. C, Wood, G. E., Talk, A. C., Matzel, L. D., & Shors, T. J. (2001). The role of the\\nhippocampus in trace conditioning: Temporal discontinuity or task difficulty? Neurobiology of Learning &\\nMemory, 76, 447-61.\\nGould, E., Beylin, A., Tanapat, P., Reeves, A., & Shors, T. J. (1999). Learning enhances adult neurogenesis in\\nthe hippocampal formation. Nature Neuroscience, 2, 260-5.\\nGrossberg, S., & Schmajuk, N. A. (1989). Neural dynamics of adaptive timing and temporal discrimination\\nduring associative learning. Neural Networks, 2, 79-102.\\nLudvig, E. A., Sutton, R. S., & Kehoe, E. J. (2008). Stimulus representation and the timing of reward-prediction\\nerrors in models of the dopamine system. Neural Computation, 20, 3034-54.\\nMachado, A. (1997). Learning the temporal dynamics of behavior. Psychological Review, 104, 241-265.\\nMcEchron, M. D., Bouwmeester, H., Tseng, W., Weiss, C., & Disterhoft, J. F. (1998). Hippocampectomy\\ndisrupts auditory trace fear conditioning and contextual fear conditioning in the rat. Hippocampus, 8, 63846.\\nMcEchron, M. D., Disterhoft, J. F. (1997). Sequence of single neuron changes in CA1 hippocampus of rabbits\\nduring acquisition of trace eyeblink conditioned responses. Journal of Neurophysiology, 78, 1030-44.\\nMoyer, J. R., Jr., Deyo, R. A., & Disterhoft, J. F. (1990). Hippocampectomy disrupts trace eye-blink conditioning in rabbits. Behavioral Neuroscience, 104, 243-52.\\nPavlov, I. P. (1927). Conditioned Reflexes. London: Oxford University Press.\\nPort, R. L., Romano, A. G., Steinmetz, J. E., Mikhail, A. A., & Patterson, M. M. (1986). Retention and acquisition of classical trace conditioned responses by rabbits with hippocampal lesions. Behavioral Neuroscience,\\n100, 745-752.\\nRodriguez, P., & Levy, W. B. (2001). A model of hippocampal activity in trace conditioning: Where?s the\\ntrace? Behavioral Neuroscience, 115, 1224-1238.\\nSchmajuk, N. A., & DiCarlo, J. J. (1992). Stimulus configuration, classical conditioning, and hippocampal\\nfunction. Psychological Review, 99, 268-305.\\nSchneiderman, N., & Gormezano, I. (1964). Conditioning of the nictitating membrane of the rabbit as a function of CS-US interval. Journal of Comparative and Physiological Psychology, 57, 188-195.\\nSchultz, W., Dayan, P., & Montague, P. R. (1997). A neural substrate of prediction and reward. Science, 275,\\n1593-9.\\nSolomon, P. R., Vander Schaaf, E. R., Thompson, R. F., & Weisz, D. J. (1986). Hippocampus and trace conditioning of the rabbit?s classically conditioned nictitating membrane response. Behavioral Neuroscience,\\n100, 729-744.\\nSutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning, 3, 9-44.\\nSutton, R. S., & Barto, A. G. (1990). Time-derivative models of Pavlovian reinforcement. In M. Gabriel\\n& J. Moore (Eds.), Learning and Computational Neuroscience: Foundations of Adaptive Networks (pp.\\n497-537). Cambridge, MA: MIT Press.\\nSutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. Cambridge, MA: MIT Press.\\nTseng, W., Guan, R., Disterhoft, J. F., & Weiss, C. (2004). Trace eyeblink conditioning is hippocampally\\ndependent in mice. Hippocampus, 14, 58-65.\\nWallenstein, G., Eichenbaum, H., & Hasselmo, M. (1998). The hippocampus as an associator of discontiguous\\nevents. Trends in Neuroscience, 21, 317-323.\\nYamazaki, T., & Tanaka, S. (2005). A neural network model for trace conditioning. International Journal of\\nNeural Systems, 15, 23-30.\\n\\n8\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'title', 'abstract',\n",
        "                              'event_type', 'pdf_name', 'year'], axis=1)\n",
        "\n",
        "# sample only 100 papers\n",
        "papers = papers.sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rGsIReb2d6f"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "ZkPeMN812d6g",
        "outputId": "11304d52-11c3-4ad9-caa8-77b10b1c1c86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5742    optimal cluster recovery\\nin the labeled stoch...\n",
              "2948    replacing supervised classification learning b...\n",
              "3444    b-bit minwise hashing for estimating three-way...\n",
              "3719    facial expression transfer with input-output\\n...\n",
              "2990    associative memory in a network of 'biological...\n",
              "Name: paper_text_processed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5742</th>\n",
              "      <td>optimal cluster recovery\\nin the labeled stoch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2948</th>\n",
              "      <td>replacing supervised classification learning b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3444</th>\n",
              "      <td>b-bit minwise hashing for estimating three-way...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3719</th>\n",
              "      <td>facial expression transfer with input-output\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2990</th>\n",
              "      <td>associative memory in a network of 'biological...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPJOHpBM2d6g"
      },
      "source": [
        "##### Tokenize words and further clean-up text\n",
        "\n",
        "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzEgCoPx2d6g",
        "outputId": "9f8b0883-239e-4346-e0ea-875dfdf3545f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['optimal', 'cluster', 'recovery', 'in', 'the', 'labeled', 'stochastic', 'block', 'model', 'se', 'young', 'yun', 'cnls', 'los', 'alamos', 'national', 'lab', 'los', 'alamos', 'nm', 'syun', 'lanlgov', 'alexandre', 'proutiere', 'automatic', 'control', 'dept', 'kth', 'stockholm', 'sweden']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSHD8xsq2d6h"
      },
      "source": [
        "** **\n",
        "#### Step 3: Phrase Modeling: Bigram and Trigram Models\n",
        "** **\n",
        "\n",
        "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
        "\n",
        "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold.\n",
        "\n",
        "*The higher the values of these param, the harder it is for words to be combined.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pdWvXGs72d6h"
      },
      "outputs": [],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4k4Foj-2d6h"
      },
      "source": [
        "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
        "\n",
        "The phrase models are ready. Let’s define the functions to remove the stopwords, make trigrams and lemmatization and call them sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBEOpU2B2d6i",
        "outputId": "17db53f7-11ab-47ea-e3ff-485d85703969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# NLTK Stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EcoEmX4e2d6i"
      },
      "outputs": [],
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye8KevCO2d6i"
      },
      "source": [
        "Let's call the functions in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J-dClU42d6l",
        "outputId": "2e8bc39a-3b3f-48a4-c48e-43009eddfc10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYyl2tQh2d6l",
        "outputId": "a56edb07-a5e2-4ba6-ccc8-c2fedd742767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['optimal', 'cluster', 'recovery', 'label', 'stochastic', 'block', 'model', 'alexandre', 'proutiere', 'automatic', 'control', 'sweden', 'alepro', 'kthse', 'abstract', 'consider', 'problem', 'community_detection', 'clustering', 'label', 'stochastic', 'block', 'model', 'number', 'cluster', 'size', 'linearly', 'grow', 'global', 'population']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhinEZxw2d6l"
      },
      "source": [
        "** **\n",
        "#### Step 4: Data transformation: Corpus and Dictionary\n",
        "** **\n",
        "\n",
        "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NaIZcam2d6l",
        "outputId": "62acbd4e-1a68-47a3-84de-dd2ae06bad1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 4), (1, 1), (2, 3), (3, 1), (4, 1), (5, 8), (6, 2), (7, 1), (8, 6), (9, 1), (10, 1), (11, 1), (12, 3), (13, 3), (14, 1), (15, 1), (16, 1), (17, 1), (18, 6), (19, 1), (20, 1), (21, 5), (22, 1), (23, 1), (24, 1), (25, 1), (26, 7), (27, 1), (28, 5), (29, 4)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI0fV3e32d6m"
      },
      "source": [
        "** **\n",
        "#### Step 5: Base Model\n",
        "** **\n",
        "\n",
        "We have everything required to train the base LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well. Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior (we'll use default for the base model).\n",
        "\n",
        "chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
        "\n",
        "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SIpOe0lB2d6m"
      },
      "outputs": [],
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=10,\n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEBrc1SB2d6n"
      },
      "source": [
        "** **\n",
        "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
        "\n",
        "You can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ppdNR2E2d6n",
        "outputId": "c95d64d9-ace6-4a48-a227-f52cc6cf2cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.025*\"model\" + 0.013*\"graph\" + 0.009*\"edge\" + 0.009*\"use\" + 0.008*\"memory\" '\n",
            "  '+ 0.008*\"set\" + 0.007*\"sequence\" + 0.006*\"time\" + 0.006*\"random\" + '\n",
            "  '0.006*\"estimate\"'),\n",
            " (1,\n",
            "  '0.014*\"model\" + 0.013*\"function\" + 0.011*\"regression\" + '\n",
            "  '0.008*\"distribution\" + 0.008*\"error\" + 0.008*\"neural\" + 0.007*\"result\" + '\n",
            "  '0.007*\"probability\" + 0.007*\"image\" + 0.006*\"optimal\"'),\n",
            " (2,\n",
            "  '0.013*\"matrix\" + 0.013*\"point\" + 0.009*\"set\" + 0.008*\"use\" + 0.007*\"datum\" '\n",
            "  '+ 0.007*\"problem\" + 0.007*\"time\" + 0.006*\"result\" + 0.006*\"function\" + '\n",
            "  '0.006*\"method\"'),\n",
            " (3,\n",
            "  '0.019*\"model\" + 0.012*\"use\" + 0.009*\"feature\" + 0.007*\"time\" + 0.006*\"show\" '\n",
            "  '+ 0.006*\"datum\" + 0.006*\"object\" + 0.005*\"figure\" + 0.005*\"learn\" + '\n",
            "  '0.005*\"parameter\"'),\n",
            " (4,\n",
            "  '0.026*\"model\" + 0.010*\"use\" + 0.009*\"time\" + 0.007*\"function\" + '\n",
            "  '0.007*\"show\" + 0.006*\"datum\" + 0.006*\"value\" + 0.006*\"figure\" + '\n",
            "  '0.006*\"state\" + 0.006*\"learn\"'),\n",
            " (5,\n",
            "  '0.014*\"model\" + 0.012*\"set\" + 0.011*\"mixture\" + 0.011*\"use\" + 0.011*\"rule\" '\n",
            "  '+ 0.009*\"test\" + 0.008*\"datum\" + 0.008*\"weight\" + 0.007*\"training\" + '\n",
            "  '0.007*\"input\"'),\n",
            " (6,\n",
            "  '0.019*\"function\" + 0.015*\"use\" + 0.012*\"sample\" + 0.011*\"problem\" + '\n",
            "  '0.009*\"show\" + 0.007*\"base\" + 0.007*\"optimization\" + 0.007*\"set\" + '\n",
            "  '0.007*\"figure\" + 0.007*\"prior\"'),\n",
            " (7,\n",
            "  '0.011*\"label\" + 0.010*\"model\" + 0.009*\"set\" + 0.009*\"log\" + 0.008*\"datum\" + '\n",
            "  '0.008*\"use\" + 0.008*\"result\" + 0.007*\"problem\" + 0.007*\"function\" + '\n",
            "  '0.007*\"learn\"'),\n",
            " (8,\n",
            "  '0.016*\"network\" + 0.015*\"model\" + 0.008*\"learn\" + 0.007*\"input\" + '\n",
            "  '0.007*\"use\" + 0.006*\"set\" + 0.006*\"datum\" + 0.006*\"function\" + 0.006*\"time\" '\n",
            "  '+ 0.006*\"output\"'),\n",
            " (9,\n",
            "  '0.013*\"model\" + 0.009*\"value\" + 0.009*\"result\" + 0.008*\"use\" + '\n",
            "  '0.008*\"learn\" + 0.007*\"set\" + 0.007*\"give\" + 0.006*\"algorithm\" + '\n",
            "  '0.006*\"distribution\" + 0.006*\"function\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Kv0jjq2d6n"
      },
      "source": [
        "#### Compute Model Perplexity and Coherence Score\n",
        "\n",
        "Let's calculate the baseline coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51zr0wpT2d6o",
        "outputId": "bc901c12-dc14-419b-f1f5-8731cfa4c7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score:  0.27812811163625606\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibb4WcCt2d6o"
      },
      "source": [
        "** **\n",
        "#### Step 6: Hyperparameter tuning\n",
        "** **\n",
        "First, let's differentiate between model hyperparameters and model parameters :\n",
        "\n",
        "- `Model hyperparameters` can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
        "\n",
        "- `Model parameters` can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
        "\n",
        "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters:\n",
        "- Number of Topics (K)\n",
        "- Dirichlet hyperparameter alpha: Document-Topic Density\n",
        "- Dirichlet hyperparameter beta: Word-Topic Density\n",
        "\n",
        "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use `C_v` as our choice of metric for performance comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aQrD_9Ek2d6o"
      },
      "outputs": [],
      "source": [
        "# supporting function\n",
        "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
        "\n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=k,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=a,\n",
        "                                           eta=b)\n",
        "\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "\n",
        "    return coherence_model_lda.get_coherence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTdIswOu2d6p"
      },
      "source": [
        "Let's call the function, and iterate it over the range of topics, alpha, and beta parameter values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOlyjPQa2d6p",
        "outputId": "0a2fbec7-2aa5-4f88-ea15-e603c9b5f88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 93%|█████████▎| 501/540 [1:27:40<08:50, 13.61s/it]"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "\n",
        "# Topics range\n",
        "min_topics = 2\n",
        "max_topics = 11\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\n",
        "alpha.append('symmetric')\n",
        "alpha.append('asymmetric')\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 1, 0.3))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)),\n",
        "               corpus]\n",
        "\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
        "\n",
        "    # iterate through validation corpuses\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word,\n",
        "                                                  k=k, a=a, b=b)\n",
        "                    # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "\n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZjHrdSr2d6p"
      },
      "source": [
        "** **\n",
        "#### Step 7: Final Model\n",
        "** **\n",
        "\n",
        "Based on external evaluation (Code to be added from Excel based analysis), let's train the final model with parameters yielding highest coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kvkRz_Ru2d6q"
      },
      "outputs": [],
      "source": [
        "num_topics = 8\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=num_topics,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=0.01,\n",
        "                                           eta=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc-GalUB2d6q",
        "outputId": "9da2e395-076d-4f8f-8941-c470046dbb56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.009*\"model\" + 0.008*\"graph\" + 0.007*\"network\" + 0.006*\"edge\" + '\n",
            "  '0.004*\"number\" + 0.004*\"error\" + 0.003*\"state\" + 0.003*\"sequence\" + '\n",
            "  '0.003*\"agent\" + 0.003*\"time\"'),\n",
            " (1,\n",
            "  '0.007*\"model\" + 0.006*\"function\" + 0.005*\"result\" + 0.005*\"error\" + '\n",
            "  '0.004*\"learn\" + 0.004*\"probability\" + 0.004*\"prediction\" + '\n",
            "  '0.004*\"regression\" + 0.003*\"follow\" + 0.003*\"case\"'),\n",
            " (2,\n",
            "  '0.006*\"matrix\" + 0.005*\"set\" + 0.004*\"result\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"datum\" + 0.003*\"use\" + 0.003*\"give\" + 0.003*\"sample\" + '\n",
            "  '0.003*\"vector\" + 0.003*\"label\"'),\n",
            " (3,\n",
            "  '0.016*\"model\" + 0.010*\"use\" + 0.006*\"feature\" + 0.005*\"network\" + '\n",
            "  '0.005*\"time\" + 0.005*\"learn\" + 0.005*\"set\" + 0.005*\"datum\" + 0.005*\"state\" '\n",
            "  '+ 0.005*\"show\"'),\n",
            " (4,\n",
            "  '0.021*\"model\" + 0.009*\"use\" + 0.007*\"time\" + 0.006*\"datum\" + '\n",
            "  '0.006*\"function\" + 0.006*\"show\" + 0.005*\"learn\" + 0.005*\"method\" + '\n",
            "  '0.005*\"result\" + 0.005*\"input\"'),\n",
            " (5,\n",
            "  '0.007*\"rule\" + 0.005*\"learn\" + 0.005*\"weight\" + 0.005*\"set\" + 0.005*\"input\" '\n",
            "  '+ 0.003*\"output\" + 0.003*\"reinforcement\" + 0.003*\"layer\" + 0.003*\"value\" + '\n",
            "  '0.003*\"condition\"'),\n",
            " (6,\n",
            "  '0.012*\"function\" + 0.010*\"use\" + 0.009*\"problem\" + 0.008*\"set\" + '\n",
            "  '0.007*\"point\" + 0.006*\"show\" + 0.006*\"datum\" + 0.005*\"base\" + '\n",
            "  '0.005*\"number\" + 0.005*\"method\"'),\n",
            " (7,\n",
            "  '0.009*\"model\" + 0.009*\"log\" + 0.008*\"sample\" + 0.007*\"set\" + '\n",
            "  '0.006*\"problem\" + 0.006*\"use\" + 0.005*\"distribution\" + 0.005*\"label\" + '\n",
            "  '0.005*\"result\" + 0.005*\"bound\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP198UpQ2d6r"
      },
      "source": [
        "** **\n",
        "#### Step 8: Visualize Results\n",
        "** **"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "mzQN0be9QVrm",
        "outputId": "70a03d35-0f86-4fc6-e454-d0c9888df8c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./results', exist_ok=True)"
      ],
      "metadata": {
        "id": "VEmIUDqfQpuV",
        "outputId": "7518b6ad-c226-42dd-9c92-18b9e64e15c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "zWjIib6Q2d6r",
        "outputId": "cf2736d9-60eb-43a2-8f99-71c6d970a474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "4     -0.066593  0.031251       1        1  32.357526\n",
              "3     -0.058696  0.036427       2        1  20.655233\n",
              "6     -0.045991 -0.039374       3        1  15.796936\n",
              "7     -0.022208 -0.042960       4        1  14.067231\n",
              "0      0.037391  0.010433       5        1   5.299945\n",
              "1      0.043143  0.007109       6        1   5.032805\n",
              "2      0.041866 -0.032948       7        1   3.827058\n",
              "5      0.071087  0.030061       8        1   2.963266, topic_info=          Term         Freq        Total Category  logprob  loglift\n",
              "324      model  2396.000000  2396.000000  Default  30.0000  30.0000\n",
              "219      graph   225.000000   225.000000  Default  29.0000  29.0000\n",
              "208   function  1024.000000  1024.000000  Default  28.0000  28.0000\n",
              "482        set   989.000000   989.000000  Default  27.0000  27.0000\n",
              "336    network   585.000000   585.000000  Default  26.0000  26.0000\n",
              "...        ...          ...          ...      ...      ...      ...\n",
              "1039      unit     9.998627   185.902059   Topic8  -6.3267   0.5961\n",
              "216       give    12.297654   650.126367   Topic8  -6.1198  -0.4489\n",
              "324      model    14.605238  2396.020542   Topic8  -5.9478  -1.5813\n",
              "742     figure    11.136819   682.474977   Topic8  -6.2189  -0.5966\n",
              "531     system    10.044281   382.280249   Topic8  -6.3222  -0.1203\n",
              "\n",
              "[582 rows x 6 columns], token_table=      Topic      Freq      Term\n",
              "term                           \n",
              "2577      1  0.083809  acoustic\n",
              "2577      2  0.810154  acoustic\n",
              "2577      3  0.027936  acoustic\n",
              "2577      4  0.027936  acoustic\n",
              "2577      7  0.027936  acoustic\n",
              "...     ...       ...       ...\n",
              "3089      1  0.144840        zy\n",
              "3089      2  0.144840        zy\n",
              "3089      3  0.144840        zy\n",
              "3089      4  0.144840        zy\n",
              "3089      8  0.434519        zy\n",
              "\n",
              "[2480 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 4, 7, 8, 1, 2, 3, 6])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el3041382731926782725649044352\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el3041382731926782725649044352_data = {\"mdsDat\": {\"x\": [-0.06659318579036631, -0.05869588059177835, -0.045991018397509034, -0.02220774484167293, 0.03739114696045639, 0.043143459360018496, 0.04186616347945855, 0.07108705982139324], \"y\": [0.03125102018084644, 0.03642729352656975, -0.03937370326286597, -0.0429599666016483, 0.010433263640292118, 0.0071094602458546185, -0.032948080862995055, 0.03006071313394645], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [32.357525888868786, 20.65523307380195, 15.796935859748737, 14.067231071539066, 5.299945113605489, 5.032804560725248, 3.827057980303784, 2.963266451406939]}, \"tinfo\": {\"Term\": [\"model\", \"graph\", \"function\", \"set\", \"network\", \"matrix\", \"input\", \"learn\", \"edge\", \"weight\", \"result\", \"layer\", \"rule\", \"log\", \"problem\", \"sample\", \"error\", \"prediction\", \"output\", \"label\", \"algorithm\", \"category\", \"let\", \"regression\", \"bound\", \"probability\", \"mixture\", \"theorem\", \"condition\", \"object\", \"monotonic_feature\", \"fly\", \"spike\", \"inducing_input\", \"visit\", \"fitc\", \"stop_signal\", \"neuron\", \"stop_trial\", \"svg\", \"rnn\", \"hyper\", \"vfe\", \"covariate_shift\", \"wavelet_coefficient\", \"laplacian\", \"spike_train\", \"response\", \"full_gp\", \"strf\", \"differential_equation\", \"gsm\", \"poedge\", \"retain\", \"optimise\", \"opponent\", \"gaussian_processe\", \"sfa\", \"arrival\", \"elastic_net\", \"predictive\", \"stop\", \"stimulus\", \"attention\", \"noise\", \"mixture\", \"policy\", \"go\", \"model\", \"input\", \"code\", \"neural\", \"action\", \"time\", \"trial\", \"test\", \"trajectory\", \"datum\", \"class\", \"value\", \"variance\", \"method\", \"use\", \"step\", \"parameter\", \"show\", \"different\", \"learn\", \"figure\", \"distribution\", \"function\", \"state\", \"information\", \"result\", \"give\", \"image\", \"estimate\", \"also\", \"set\", \"network\", \"variable\", \"example\", \"problem\", \"point\", \"pivot_language\", \"character\", \"topic\", \"mask\", \"corrlda\", \"auxiliary_variable\", \"symcorrlda\", \"speaker\", \"lsda\", \"rcpn\", \"galileo\", \"language\", \"semantic\", \"spelling\", \"multilingual_topic\", \"event_coreference\", \"speech\", \"acoustic\", \"adaptation\", \"event_mention\", \"detector\", \"particle\", \"super_pixel\", \"japanese\", \"bilingual\", \"cma_es\", \"deep\", \"storage\", \"physics_engine\", \"tsbn\", \"video\", \"detection\", \"contour\", \"synapsis\", \"memory\", \"category\", \"prune\", \"object\", \"layer\", \"feature\", \"document\", \"word\", \"physical\", \"model\", \"event\", \"use\", \"network\", \"image\", \"train\", \"training\", \"state\", \"figure\", \"learn\", \"time\", \"output\", \"datum\", \"set\", \"show\", \"single\", \"variable\", \"sample\", \"distribution\", \"result\", \"number\", \"approach\", \"method\", \"give\", \"value\", \"ssp\", \"mesh\", \"acquisition_acquisition\", \"constraints_constraint\", \"prototype\", \"rigid_structure\", \"preference\", \"short_sighted\", \"affine_transformation\", \"short_sighte\", \"demonstrate_demonstrate\", \"sql\", \"maximum_margin\", \"fault\", \"flight\", \"spectral_clustere\", \"border_pattern\", \"modes_mode\", \"bopp_bopp\", \"potentially_potentially\", \"query\", \"rigid\", \"qbrank\", \"planner\", \"probabilistic_planne\", \"region_region\", \"similarity_search\", \"adapting_adapte\", \"engine\", \"sim\", \"way_resemblance\", \"pose\", \"similarity\", \"median\", \"optimization\", \"function\", \"search\", \"problem\", \"shape\", \"hash\", \"point\", \"cluster\", \"way\", \"part\", \"use\", \"base\", \"set\", \"prior\", \"cell\", \"maximum\", \"number\", \"show\", \"datum\", \"method\", \"figure\", \"approach\", \"learn\", \"give\", \"size\", \"sample\", \"result\", \"follow\", \"performance\", \"state\", \"define\", \"value\", \"error\", \"also\", \"model\", \"classifier_chain\", \"gumbel_process\", \"sbm\", \"lsbm\", \"gcc\", \"pile\", \"mcv\", \"misclassified_item\", \"gumbel\", \"uniform_convergence\", \"item\", \"sfds\", \"appxpca\", \"clique\", \"decomposable\", \"block_krylov\", \"contingency_table\", \"cc\", \"fused_sparsity\", \"community_detection\", \"arm\", \"cone\", \"splice\", \"lazysvd\", \"knnz\", \"multi_label\", \"heap\", \"fna\", \"gk_xk\", \"bandit\", \"globally_optimal\", \"gap\", \"dantzig_selector\", \"bound\", \"log\", \"theorem\", \"let\", \"bind\", \"sample\", \"vertex\", \"recovery\", \"label\", \"proof\", \"move\", \"loss\", \"degree\", \"problem\", \"distribution\", \"set\", \"matrix\", \"prove\", \"order\", \"table\", \"define\", \"model\", \"result\", \"first\", \"probability\", \"random\", \"observation\", \"denote\", \"use\", \"method\", \"number\", \"parameter\", \"give\", \"time\", \"large\", \"function\", \"follow\", \"value\", \"also\", \"learn\", \"datum\", \"infect\", \"team\", \"edge_exchangeable\", \"firm\", \"graph_frequency\", \"skill\", \"infection\", \"industry\", \"oblivious_equilibrium\", \"elo\", \"trueskill\", \"en\", \"edge_exchangeability\", \"edge_transmission\", \"oblivious\", \"cascade\", \"diffusion\", \"connie_netinf\", \"mpe\", \"investment\", \"step_augmente\", \"vertice\", \"erroneous\", \"profit\", \"pbm\", \"vertex_exchangeability\", \"transmission\", \"active_vertice\", \"click\", \"contagion\", \"beta\", \"player\", \"feedback\", \"edge\", \"graph\", \"game\", \"agent\", \"mental\", \"social\", \"sparsity\", \"network\", \"random\", \"model\", \"draw\", \"error\", \"sequence\", \"number\", \"infer\", \"system\", \"position\", \"state\", \"probability\", \"result\", \"time\", \"distribution\", \"step\", \"show\", \"new\", \"performance\", \"set\", \"use\", \"function\", \"consider\", \"give\", \"problem\", \"section\", \"datum\", \"figure\", \"delay_conditione\", \"microstimuli\", \"trace_conditione\", \"trace_conditioning\", \"def\", \"excess_risk\", \"long_latency\", \"clsr\", \"nonparanormal\", \"climbing_fiber\", \"hippocampal\", \"countable\", \"hippocampal_damage\", \"apgs\", \"mdlx\", \"norm_regularize\", \"ugms\", \"pollack\", \"compressed_domain\", \"apg\", \"fb\", \"node_wise\", \"metabolic_constraint\", \"corrective_movement\", \"crit\", \"proprioceptive\", \"mle_estimator\", \"fiber\", \"opinion\", \"houk\", \"regularizer\", \"fn\", \"compress\", \"measurement\", \"precise\", \"iid\", \"mutual_information\", \"pc\", \"trace\", \"movement\", \"lasso\", \"regression\", \"prediction\", \"error\", \"probability\", \"result\", \"function\", \"stimulus\", \"estimation\", \"estimator\", \"linear\", \"model\", \"learn\", \"follow\", \"assume\", \"case\", \"assumption\", \"true\", \"theorem\", \"condition\", \"distribution\", \"let\", \"measure\", \"consider\", \"show\", \"time\", \"problem\", \"class\", \"weight\", \"set\", \"nd_order\", \"selective_sample\", \"weighted_trace\", \"rbf\", \"lpp\", \"sil\", \"tissue\", \"specificity\", \"label_request\", \"diffeomorphism\", \"diffeomap\", \"empirical_marginal\", \"vb_pca\", \"locality_preserve\", \"pap_smear\", \"diffeomorphic\", \"cancer\", \"msa\", \"cervical\", \"ramanujam\", \"back_constraine\", \"mistake\", \"dimensionality_reduction\", \"lp\", \"movielen\", \"transductive\", \"kortum\", \"colposcopy\", \"pre_cancerous\", \"columnar\", \"excess\", \"sensitivity\", \"vb\", \"discard\", \"pca\", \"matrix\", \"guarantee\", \"norm\", \"algorithm\", \"map\", \"mapping\", \"eigenvector\", \"entry\", \"set\", \"result\", \"vector\", \"dimensional\", \"label\", \"datum\", \"order\", \"measure\", \"give\", \"obtain\", \"sample\", \"theorem\", \"figure\", \"use\", \"distribution\", \"show\", \"let\", \"follow\", \"cluster\", \"example\", \"bound\", \"learn\", \"base\", \"number\", \"transducer\", \"slot\", \"rulenet\", \"suffix_tree\", \"interpolation\", \"eye_position\", \"hdirl\", \"multilinear_interpolation\", \"motoneuron\", \"foveation\", \"forward_stepwise\", \"dirl\", \"eye_movement\", \"alphabet\", \"group_sparse\", \"symbolic\", \"subsymbolic\", \"selected_group\", \"pxs\", \"collicular\", \"delivery\", \"iterative_hard\", \"triangulation\", \"null_hypothesis\", \"boy\", \"scp_rule\", \"template\", \"zy\", \"iht\", \"hillcar\", \"confidence_interval\", \"rule\", \"selective\", \"reinforcement\", \"string\", \"eye\", \"symbol\", \"grid\", \"value_iteration\", \"group\", \"weight\", \"layer\", \"category\", \"condition\", \"input\", \"output\", \"learn\", \"set\", \"signal\", \"mixture\", \"value\", \"prediction\", \"selection\", \"inference\", \"example\", \"use\", \"result\", \"unit\", \"give\", \"model\", \"figure\", \"system\"], \"Freq\": [2396.0, 225.0, 1024.0, 989.0, 585.0, 460.0, 476.0, 851.0, 169.0, 322.0, 825.0, 166.0, 115.0, 477.0, 737.0, 667.0, 438.0, 264.0, 319.0, 406.0, 303.0, 138.0, 296.0, 211.0, 200.0, 447.0, 187.0, 223.0, 177.0, 244.0, 68.07712409438787, 76.35161534941786, 75.1774961531584, 48.8406301046907, 62.162572755248604, 41.34388849163249, 39.57384469138387, 135.75107881243147, 31.271930212534446, 29.472725021720084, 62.479757972915976, 42.411723589975544, 25.75214177866583, 46.55049766677664, 25.64117280596083, 42.590520165651704, 41.46075088042208, 212.5358353623343, 24.260208212358446, 21.17477740194811, 21.053436301296887, 20.10950689932583, 20.087530997671433, 52.99755718266431, 19.22447789013507, 23.73193703921248, 30.096321666854344, 18.216312741015212, 25.491303021657657, 18.15791709195265, 50.066300696290824, 52.58324789919754, 101.49090228235089, 54.814208022539695, 189.4867348189592, 136.53179648964488, 117.92791699587005, 60.09837000355456, 1273.2993178203903, 298.8222146720221, 94.98002823150141, 186.10957280329512, 140.84344528977178, 438.17530105370446, 95.69200834523845, 179.84841892953065, 100.97356129886724, 395.88923732224674, 170.62859956370616, 285.9675438499614, 84.56585726630921, 308.47228895735407, 528.1295321060757, 193.40188254706916, 237.90879020031494, 339.87639093327175, 177.0322333472021, 334.21606691934494, 282.4632006355598, 281.17660178997534, 361.5215645984205, 264.7466677066374, 161.77636158759208, 299.73618469071874, 256.40995079299654, 185.83523392910402, 186.5163393672616, 200.79737328125546, 269.224924588017, 202.65067846773178, 177.3012972940895, 172.38647264219748, 184.0699958577736, 175.46154198826125, 43.0094837219128, 52.663552681328824, 102.04693556750816, 38.718295290921695, 26.234229116008088, 26.18063044324328, 23.070158012305416, 49.05289631071835, 21.467062597560737, 20.747528438028713, 19.14406771186607, 110.0643912223276, 48.370494581962554, 19.056479194125533, 18.276750335170192, 18.25859821502294, 46.77019913175059, 29.18577065899667, 57.304986270561194, 16.658502438132594, 41.8744815674354, 41.64082747689995, 15.939886095175513, 15.887031659336746, 15.879019767306753, 15.707056791599248, 57.29699479966637, 18.212871889864495, 14.329772616978904, 14.249761474892335, 65.26765428874619, 90.5615173585558, 37.14096000368099, 26.273691825208218, 71.06930786537053, 96.54722435741324, 34.88860182452434, 154.25341332549908, 103.85286277277825, 244.5693778787629, 99.65297950612783, 140.34131078254092, 39.295559939784596, 639.6321389157226, 64.47653951242849, 401.3903178384904, 207.55646066158565, 145.98063902474843, 97.37831917835527, 154.85170720555257, 188.06921577326062, 180.7043560659152, 201.79984249504662, 205.8640477396564, 113.54196532407848, 195.5110223179103, 197.76306510397936, 181.38595002532443, 98.54314072820456, 119.05274932746322, 141.1306829553245, 142.73046560814228, 151.18961680250487, 119.74683194553378, 98.9169745665772, 114.69904499468497, 112.19702028600797, 98.2148868635217, 49.76865574804979, 56.825329633398496, 45.19856670548189, 43.004477362525265, 63.18148517827455, 29.7130267796177, 29.821810658333, 19.717194283736312, 18.202589440742074, 17.516816133089794, 17.44909865447974, 20.434519219815108, 30.517046677997698, 15.97271081252154, 21.497858728003997, 15.961578720542871, 15.929867442473011, 15.262226516595494, 14.532848900105499, 14.528340533003304, 90.62017784528987, 20.314834046975104, 15.596652606985625, 23.38142083804986, 13.121078383628767, 15.989328258006763, 13.081564229240579, 13.071941867584817, 17.454412211087483, 13.830099802931583, 25.229915556871266, 32.30347412732558, 65.44857258532697, 23.751844452369134, 111.79276777820716, 364.4220510441952, 64.54132530994886, 275.44958246139134, 67.13643720375785, 38.270324016912774, 195.818575457533, 101.10216953498005, 97.76474232994265, 90.13284383834477, 311.30400477979066, 144.7398659720206, 231.35464250453117, 89.63681239648194, 57.244160060772316, 39.97672534719339, 143.05328549202528, 181.2125072397499, 176.35964397237953, 140.52013154241058, 128.06394706960052, 93.05784742560854, 137.63334669150504, 112.91470911743573, 72.59829444758856, 111.35649389197604, 119.00374651449009, 89.90606499062223, 81.39910598000169, 92.11902422729295, 77.98371421870279, 84.00343590539147, 79.54546950147136, 77.40740677139443, 85.14745748442309, 19.815414590151903, 18.261496222866185, 18.10457916419532, 18.094041688587932, 17.426390077476032, 17.32951453182019, 16.64592921914094, 14.925862105359377, 14.274791681117586, 14.162467661543715, 48.717047610390296, 12.596655808261184, 11.770389614361289, 14.276330810858743, 11.660624629778741, 10.962971578546641, 10.2919434629323, 10.261973701687294, 10.215712499381887, 10.191063243858578, 58.81771325224256, 13.262234384686575, 9.442049464070244, 9.393209137730706, 9.391523203445963, 19.060326192881146, 8.69800617706548, 8.691022916723819, 8.690906245744443, 22.785607384658626, 12.675057161090168, 29.00042784213772, 12.0437605362137, 124.31003261122656, 228.61404888477716, 104.72082991493932, 123.697619686588, 78.17188032894023, 224.88272861766993, 25.21938564877394, 30.760475241725757, 137.3645725635829, 54.64861256430079, 47.098995746601005, 57.3802896988795, 38.43518498366928, 156.46403530247267, 144.87691029907816, 181.78488297990623, 108.11245374005821, 41.693411383310526, 79.60095302173983, 63.223734839175734, 83.70311701905106, 230.20544722469546, 127.7760909758757, 90.31066899152708, 90.14474586043507, 63.57003284969241, 54.106196046043266, 61.445496038671116, 153.57443974770476, 106.29349908571822, 95.24401249901534, 90.1442019109683, 99.18019773403908, 113.47227217321402, 71.8633769383005, 107.08070522650327, 81.36618030374991, 83.22403352620681, 78.66913667896924, 85.6854728569554, 84.03021608588888, 20.166237631862415, 19.185004157454923, 13.463155500872045, 16.198666267035588, 12.018341893866417, 14.858512813144296, 11.020853154361617, 13.299660836327078, 9.50494197544024, 8.61022004656781, 8.61064156561976, 8.641439820217984, 7.1929943957454165, 7.170134555889376, 8.544262794693037, 22.036347660444022, 11.893684038547129, 6.688250948214985, 6.633412134648351, 6.633304426050822, 5.743902719687676, 13.496290158723689, 5.69055261360148, 7.095534999032352, 6.620271685749848, 5.2614485475507005, 7.635355352421757, 4.779713985286943, 8.315818099212347, 4.760215818091069, 8.063761350318824, 29.299826562412232, 18.91249907687273, 63.048290676401734, 77.49013889319686, 26.804911185947883, 31.956509772609273, 8.585511694294722, 10.633210494064405, 17.181349440944953, 65.41410245240772, 30.596146692569782, 85.41538174721605, 19.951248942360557, 38.24441193927047, 33.303158346035865, 40.45336040708744, 17.90283402596947, 30.475417581121434, 20.30851544932013, 34.33676032277045, 29.714985567230535, 30.580676244712848, 31.207018623392752, 28.085744566406714, 24.09889394483088, 27.493344951164236, 21.572217922894176, 23.135904560218844, 27.72358517807616, 27.764728697914858, 25.815866800489335, 22.159065596590523, 22.104342642542314, 22.05438681279986, 21.043374531114534, 21.54207581742792, 21.067424405421107, 17.953935013959573, 8.958280745745189, 8.956080351966676, 8.508372090198101, 8.301503008977996, 7.4232772000511575, 6.706568034545711, 6.9786179763952685, 6.611712818220821, 6.254856493543416, 6.2495501167881, 10.306529463095952, 5.806757910667396, 5.802218907420767, 5.358057813110215, 4.814919357747992, 4.809096607176312, 4.773889753857363, 4.762333263618109, 4.453160246128918, 6.066927384701264, 4.3668204984405925, 8.031467146157361, 4.004950261407076, 3.9967468837976394, 4.903528883491011, 3.9167379675559824, 4.004776109638174, 3.557222373010577, 3.55409289336654, 9.323176800655538, 4.645977214618261, 8.688471003792749, 26.400609831769028, 12.308685453772192, 13.534615497273549, 6.626254214492786, 8.025453105071268, 22.37186795084748, 24.533919591337426, 17.301011836344085, 34.06244288917638, 36.715416756084046, 44.25936620469003, 38.175770560685166, 51.70811268629609, 52.402583500206845, 18.266399776312397, 22.314443201855905, 16.630348398536412, 23.241863844752476, 62.63339730281165, 40.280196210139465, 29.620825311717756, 23.453320581141952, 27.24011423882164, 17.857462674066305, 16.75327964432215, 18.006482407696367, 16.911295560757225, 24.15417806563798, 19.13737976592738, 17.552440550167333, 19.276265525195605, 22.03322403115795, 20.496577377345897, 18.06197324275899, 17.18208663410586, 17.192809960168177, 17.27470883774949, 14.507891006637998, 13.719949810771894, 10.129952056858889, 10.572618663762677, 9.45911166681778, 7.821668741600422, 8.214032535197065, 7.822224522692188, 5.855599435615873, 5.456300149832488, 5.45394207408081, 5.030029085135811, 5.2228834423415975, 4.8774369059764595, 4.28513780137023, 4.273572341586193, 5.485467306099188, 3.8916233475253525, 3.89126684044077, 3.8909380803698466, 3.880030561136955, 7.021375500153991, 9.726119497616715, 8.160819648730156, 3.0699020543718176, 5.821508445652676, 2.7124015426185335, 2.712297281112665, 2.7120444049200043, 2.711238270091364, 4.244373654622154, 7.800141263094373, 3.313763830867957, 4.087479006708538, 11.262155496098067, 43.073293584031724, 17.80296827483017, 18.62983264512904, 30.53177121479835, 18.03004647133728, 14.640585634139091, 7.304325671936949, 10.835895782452432, 37.222820619393914, 32.1681959086963, 21.542875576172907, 12.238532547449484, 21.455790312577687, 28.111937337265193, 18.107028458686408, 15.606460861230047, 22.577109355095356, 17.380092015213688, 21.83961391169767, 15.635406882206075, 20.81116664635092, 25.22603603049095, 19.936856564333144, 20.219541457849154, 15.918281800724717, 17.49559694734402, 14.924076031869795, 16.179207990620856, 13.801917852393602, 16.00299763751722, 14.549109615837056, 13.831090894390472, 13.118256664653503, 12.044161021400674, 11.026390793419909, 9.736779929071083, 16.75918083066382, 5.366261427291962, 5.355128310136062, 5.5622109298483675, 5.704192434589599, 5.028379573548039, 4.6795836777584725, 4.341663738148427, 4.017088500091147, 5.663657354307696, 5.69345222034318, 6.331135270032545, 3.655077459797675, 3.3266760570361336, 3.32597488647522, 3.004053940318446, 3.0024214563987703, 2.9884914694845732, 2.917799544678915, 3.9365818150338137, 2.648854898977137, 2.6486275231011867, 8.6695911739155, 2.651234289049096, 2.6505329557280706, 2.5841081127690875, 4.999605864452939, 36.39378335574075, 4.985344497951457, 19.37136603925022, 10.388473369169791, 6.328867696157278, 12.775205224243901, 8.888624617584549, 7.099202840989088, 16.916957878718783, 28.362506740747747, 18.507152815150004, 16.379332584216296, 17.72319985729811, 25.603881702442273, 19.549008935490626, 29.470410914388925, 26.701864019678318, 12.138021522577025, 13.10261737104694, 17.990280085921132, 13.861711000488501, 10.121042880786533, 12.3165146776406, 13.844882907026571, 17.013116552671388, 13.25005068848742, 9.998627094584894, 12.297654035904602, 14.60523825285351, 11.136818971879576, 10.044281305897869], \"Total\": [2396.0, 225.0, 1024.0, 989.0, 585.0, 460.0, 476.0, 851.0, 169.0, 322.0, 825.0, 166.0, 115.0, 477.0, 737.0, 667.0, 438.0, 264.0, 319.0, 406.0, 303.0, 138.0, 296.0, 211.0, 200.0, 447.0, 187.0, 223.0, 177.0, 244.0, 72.2215707481211, 81.97980608565938, 81.06081272126487, 52.71871080069622, 67.69645294247002, 45.261909372384615, 43.39778658247136, 150.68468222384953, 35.05318612281861, 33.23647397519713, 71.16321531737748, 48.427096672000545, 29.51428716965409, 53.45137380026754, 29.493246849637085, 49.094644515741976, 48.348055721683, 248.74727501565172, 28.454760978197623, 24.883425773482013, 24.85317022901143, 23.926371281685206, 23.924140504189364, 63.21459175627462, 22.998350944671287, 28.612547069246, 36.286209231599365, 22.070710562325864, 30.906778352627786, 22.01700207131963, 61.53628365677538, 65.16079603071512, 128.81097304510695, 69.23126164911159, 261.24048352706683, 187.27928440701882, 160.51187141534652, 78.15551051340229, 2396.020541529048, 476.79743580845195, 134.8596938577254, 301.2040826167163, 219.02337389802244, 896.5536681511045, 140.73187611696716, 321.60467522597594, 152.6738711592062, 920.5138687292949, 304.5889365677302, 611.1006019190164, 122.2931058086018, 703.1344434903123, 1480.8171089179352, 377.5851073662638, 514.0989512187509, 848.3481810375073, 344.546307052845, 851.1507872417153, 682.4749773082385, 688.6169543753682, 1024.0366250692448, 640.8050146796693, 308.49364507955437, 825.4126745117821, 650.1263669462642, 385.49499864943925, 401.217272451307, 496.6214241169195, 989.0504938313317, 585.2330155727001, 415.3281618160755, 419.2894799598329, 737.2047573400071, 527.7123586549404, 47.25365282631769, 58.65239625592522, 115.38748588122557, 44.45500030729522, 30.30113495302078, 30.26173688727467, 27.069468851191587, 57.81522562571254, 25.392805096796597, 24.63242919237326, 23.011212943411916, 132.3890753212857, 58.32058456743024, 22.978640302788556, 22.225547672224913, 22.21459813283385, 57.115523929414834, 35.79564911110303, 70.80226918354626, 20.598945613607498, 51.9371228664802, 51.692839002371976, 19.79307033007657, 19.802546974120947, 19.79868153583106, 19.679822986147116, 71.97328179974903, 23.03172640100873, 18.169503902234105, 18.189030549324336, 84.25650094354019, 118.05387943988326, 47.81101628153567, 33.90767217570819, 98.16413333552109, 138.21803374563976, 46.39857104768425, 244.95653783296086, 166.41168060386084, 458.7777267116844, 159.966663153306, 255.1896178866985, 55.564882840694224, 2396.020541529048, 112.6351977189442, 1480.8171089179352, 585.2330155727001, 385.49499864943925, 218.4049990821728, 442.3392898432979, 640.8050146796693, 682.4749773082385, 851.1507872417153, 896.5536681511045, 319.6740871160136, 920.5138687292949, 989.0504938313317, 848.3481810375073, 260.48286021677984, 415.3281618160755, 667.9590070065578, 688.6169543753682, 825.4126745117821, 574.0051116299472, 373.9928868951388, 703.1344434903123, 650.1263669462642, 611.1006019190164, 53.858391979106514, 62.05803271588352, 49.421499810340606, 47.20347694857442, 69.43143745599971, 33.85370545289366, 35.32261357088979, 23.633534893849713, 22.153840971790007, 21.420218336071017, 21.410942639680464, 25.07746802634628, 37.86064726965219, 19.935741593967258, 26.839803598422023, 19.932956641237364, 19.89355855413687, 19.200769417893195, 18.46561345626733, 18.466397428138972, 115.54742134423107, 25.998833482185482, 19.968208144210543, 30.097194390537233, 16.997135871986544, 20.74221136775286, 16.99356676108524, 16.9900630010553, 22.824422999044824, 18.08516262297816, 33.575146637494434, 45.2425660619538, 99.94734119060774, 33.71899232921134, 224.80367293834018, 1024.0366250692448, 117.93150858989743, 737.2047573400071, 124.45047015833109, 62.17051145408963, 527.7123586549404, 244.39997146998815, 237.36446200806515, 234.24451139857274, 1480.8171089179352, 488.44323511488113, 989.0504938313317, 241.80667268975833, 123.75312329998248, 72.9069556583969, 574.0051116299472, 848.3481810375073, 920.5138687292949, 703.1344434903123, 682.4749773082385, 373.9928868951388, 851.1507872417153, 650.1263669462642, 247.12124771078015, 667.9590070065578, 825.4126745117821, 474.94413195743715, 356.0834939105532, 640.8050146796693, 362.90344043206665, 611.1006019190164, 438.4984329932785, 496.6214241169195, 2396.020541529048, 23.731596010931497, 22.1596417427569, 22.12950392140221, 22.123889457017267, 21.322889262122477, 21.272578398378666, 20.532299918083968, 18.918508564874134, 18.136147148153842, 18.07112454109828, 62.59556957551222, 16.494688899725904, 15.644634588710908, 18.99459052174023, 15.63352384475109, 14.836740877454357, 14.11329331871831, 14.096246355716987, 14.087726928364237, 14.10318295585705, 81.83448205893276, 18.52890300334819, 13.308143211894286, 13.24492648242804, 13.244778686104064, 27.081539793406137, 12.506719417364755, 12.505292313171747, 12.505836272920597, 32.8259667500825, 18.289908728715876, 42.696372282881335, 17.455779812126046, 200.2563072859277, 477.02442058236284, 223.08774460853897, 296.0011555016876, 172.8516535121681, 667.9590070065578, 43.62196948387445, 56.65269206342923, 406.82714170009893, 131.6623758241069, 110.35099982015527, 146.93915195744793, 90.00421840536471, 737.2047573400071, 688.6169543753682, 989.0504938313317, 460.68349036266045, 102.13309155427439, 313.72808320212454, 215.2427731817584, 362.90344043206665, 2396.020541529048, 825.4126745117821, 441.8457013971433, 447.0285227596514, 233.61702518379832, 172.1151345511477, 221.35977531949226, 1480.8171089179352, 703.1344434903123, 574.0051116299472, 514.0989512187509, 650.1263669462642, 896.5536681511045, 328.2134542173109, 1024.0366250692448, 474.94413195743715, 611.1006019190164, 496.6214241169195, 851.1507872417153, 920.5138687292949, 24.453905132046586, 23.793498069463418, 17.603587515734386, 21.46779180919938, 16.14600704145373, 19.977173207110315, 15.194170259588788, 18.672090445629525, 13.795831135095009, 12.731834172576932, 12.732887084081364, 13.200941155113505, 11.277802826957661, 11.295005539834284, 13.557721215957525, 34.968636074711235, 18.90021012487572, 10.805792237809575, 10.856382118676258, 10.85655091954822, 9.81803944968248, 23.209368149040564, 9.822917997391947, 12.284910474974838, 11.64828582417883, 9.331143487297451, 13.883676371360094, 8.844327112088942, 15.4153852751984, 8.858171582832231, 15.046721766349098, 58.45540375774572, 38.087133216969235, 169.34078353833877, 225.84194671365387, 64.6730386506972, 87.3319223823457, 16.772713581369395, 24.070355996409994, 55.785004373042035, 585.2330155727001, 233.61702518379832, 2396.020541529048, 104.0271377941076, 438.4984329932785, 331.2022713634424, 574.0051116299472, 87.21444036837161, 382.2802485411916, 141.52214650641676, 640.8050146796693, 447.0285227596514, 825.4126745117821, 896.5536681511045, 688.6169543753682, 377.5851073662638, 848.3481810375073, 237.21211389615667, 356.0834939105532, 989.0504938313317, 1480.8171089179352, 1024.0366250692448, 382.57720575943443, 650.1263669462642, 737.2047573400071, 373.99230574705547, 920.5138687292949, 682.4749773082385, 22.37888670585718, 13.192666738310052, 13.19151503436021, 12.730832296653054, 13.273356372338963, 11.943648773677175, 10.896048650245943, 11.482439252650945, 10.982695294393537, 10.436603715932913, 10.441166697276222, 17.658968039591507, 9.977583721443025, 9.978925764472685, 9.507921791194851, 9.139782314063694, 9.144050026179869, 9.153238224193009, 9.162089271736406, 8.601059305946732, 11.876464099218355, 8.678733946358376, 16.037347924099397, 8.14063454749833, 8.143292993417104, 10.163455006237792, 8.219008788530058, 8.54289021163045, 7.6756459169701685, 7.68217933107876, 20.23195844968229, 10.053661508520015, 19.182603718584943, 67.78981300889012, 32.76026520874902, 40.6025575078867, 16.34030960060066, 21.34147931873727, 97.76183758181325, 115.12323220870509, 68.5073735862027, 211.79976605125572, 264.07493458305896, 438.4984329932785, 447.0285227596514, 825.4126745117821, 1024.0366250692448, 128.81097304510695, 201.64326567796388, 108.12830892163316, 235.13521891055615, 2396.020541529048, 851.1507872417153, 474.94413195743715, 271.97378152477165, 403.8074742677102, 152.8803506052522, 169.28636746387266, 223.08774460853897, 177.76979750512916, 688.6169543753682, 296.0011555016876, 207.59548155172826, 382.57720575943443, 848.3481810375073, 896.5536681511045, 737.2047573400071, 304.5889365677302, 322.75499617570085, 989.0504938313317, 18.839740249424572, 18.041052905001248, 14.48358465279616, 15.59984755067698, 14.326810461438138, 12.066415807385047, 13.223846729666876, 12.86894109203138, 10.060141434781464, 9.672319176584574, 9.674422899952793, 9.291025434430262, 9.833039439566006, 9.42749478311711, 8.466393421034265, 8.477254551399467, 11.175095461163837, 8.06658590372664, 8.066874073704346, 8.06741789168642, 8.079048128079538, 15.208542678724221, 21.616150876014668, 19.32446113767787, 7.2922332036627475, 14.19103970666111, 6.8673051031509456, 6.867553141430085, 6.867499953660518, 6.86821802212295, 11.396096016354884, 22.771226708827783, 8.618882257707936, 11.225339586527216, 38.63663168675755, 460.68349036266045, 113.76218989606518, 132.69513131381248, 303.2018012402716, 128.1681787136894, 101.58760864807758, 28.929635889946827, 65.07410762130847, 989.0504938313317, 825.4126745117821, 351.76424119473216, 94.85250269378433, 406.82714170009893, 920.5138687292949, 313.72808320212454, 207.59548155172826, 650.1263669462642, 308.45259738115936, 667.9590070065578, 223.08774460853897, 682.4749773082385, 1480.8171089179352, 688.6169543753682, 848.3481810375073, 296.0011555016876, 474.94413195743715, 244.39997146998815, 419.2894799598329, 200.2563072859277, 851.1507872417153, 488.44323511488113, 574.0051116299472, 17.592973083135565, 16.587048034556247, 15.562080723861975, 14.153459071528637, 27.91300184281883, 9.634272067151667, 9.651982771568953, 10.156987503782709, 10.441940090688487, 9.289878086919954, 8.964184006079499, 8.621653793528917, 8.256930858597162, 11.655653472570457, 11.838982688484467, 13.366237723640161, 7.935183827542022, 7.592722833247308, 7.592675320303359, 7.224557700593372, 7.22598337077342, 7.249016948175097, 7.333916225204269, 10.24472198208502, 6.895918965747835, 6.895571359018245, 22.57170041259839, 6.904193330836094, 6.904396036824087, 6.985840190611892, 13.726388326437291, 115.70131899321204, 13.749617944902617, 66.60646579540938, 32.66332181231323, 19.873830268254018, 51.964106899045305, 33.4106804944379, 25.06662482351401, 91.48512866288414, 322.75499617570085, 166.41168060386084, 138.21803374563976, 177.76979750512916, 476.79743580845195, 319.6740871160136, 851.1507872417153, 989.0504938313317, 141.00274579253406, 187.27928440701882, 611.1006019190164, 264.07493458305896, 105.54451722587487, 232.213589408049, 419.2894799598329, 1480.8171089179352, 825.4126745117821, 185.902059498554, 650.1263669462642, 2396.020541529048, 682.4749773082385, 382.2802485411916], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.7991, -6.6844, -6.6999, -7.1312, -6.89, -7.2978, -7.3415, -6.1089, -7.577, -7.6363, -6.8849, -7.2723, -7.7712, -7.1792, -7.7755, -7.2681, -7.295, -5.6606, -7.8309, -7.9669, -7.9727, -8.0185, -8.0196, -7.0495, -8.0635, -7.8529, -7.6153, -8.1174, -7.7814, -8.1206, -7.1064, -7.0573, -6.3997, -7.0158, -5.7754, -6.1032, -6.2496, -6.9237, -3.8704, -5.3199, -6.4661, -5.7934, -6.0721, -4.9371, -6.4586, -5.8276, -6.4049, -5.0386, -5.8802, -5.3638, -6.5822, -5.2881, -4.7504, -5.7549, -5.5478, -5.1911, -5.8434, -5.2079, -5.3762, -5.3807, -5.1294, -5.4409, -5.9335, -5.3168, -5.4729, -5.7949, -5.7912, -5.7174, -5.4242, -5.7082, -5.8419, -5.87, -5.8044, -5.8523, -6.8094, -6.6069, -5.9454, -6.9145, -7.3038, -7.3058, -7.4323, -6.6779, -7.5043, -7.5384, -7.6188, -5.8698, -6.6919, -7.6234, -7.6652, -7.6662, -6.7256, -7.1972, -6.5225, -7.7579, -6.8362, -6.8418, -7.802, -7.8053, -7.8058, -7.8167, -6.5226, -7.6687, -7.9085, -7.9141, -6.3923, -6.0648, -6.9561, -7.3023, -6.3072, -6.0008, -7.0187, -5.5322, -5.9279, -5.0713, -5.9691, -5.6268, -6.8997, -4.1099, -6.4045, -4.5759, -5.2354, -5.5874, -5.9922, -5.5284, -5.334, -5.374, -5.2636, -5.2436, -5.8387, -5.2952, -5.2838, -5.3702, -5.9803, -5.7913, -5.6212, -5.6099, -5.5523, -5.7855, -5.9766, -5.8285, -5.8506, -5.9837, -6.3953, -6.2627, -6.4916, -6.5414, -6.1567, -6.9111, -6.9074, -7.3212, -7.4011, -7.4395, -7.4434, -7.2855, -6.8844, -7.5318, -7.2347, -7.5325, -7.5345, -7.5773, -7.6263, -7.6266, -5.796, -7.2913, -7.5556, -7.1507, -7.7285, -7.5308, -7.7315, -7.7322, -7.4431, -7.6758, -7.0747, -6.8275, -6.1214, -7.135, -5.586, -4.4044, -6.1354, -4.6843, -6.096, -6.658, -5.0255, -5.6866, -5.7201, -5.8014, -4.5619, -5.3277, -4.8587, -5.8069, -6.2554, -6.6144, -5.3395, -5.103, -5.1302, -5.3573, -5.4502, -5.7695, -5.3781, -5.5761, -6.0177, -5.5899, -5.5235, -5.8039, -5.9033, -5.7796, -5.9462, -5.8718, -5.9264, -5.9536, -5.8583, -7.2003, -7.2819, -7.2906, -7.2911, -7.3287, -7.3343, -7.3746, -7.4836, -7.5282, -7.5361, -6.3007, -7.6533, -7.7211, -7.5281, -7.7305, -7.7922, -7.8554, -7.8583, -7.8628, -7.8652, -6.1123, -7.6018, -7.9415, -7.9467, -7.9469, -7.2391, -8.0236, -8.0244, -8.0244, -7.0606, -7.6471, -6.8194, -7.6982, -5.3639, -4.7547, -5.5354, -5.3689, -5.8278, -4.7711, -6.9591, -6.7605, -5.2641, -6.1858, -6.3345, -6.137, -6.5377, -5.1339, -5.2108, -4.9839, -5.5035, -6.4564, -5.8097, -6.04, -5.7594, -4.7477, -5.3364, -5.6835, -5.6853, -6.0346, -6.1958, -6.0686, -5.1525, -5.5205, -5.6303, -5.6853, -5.5898, -5.4552, -5.912, -5.5131, -5.7878, -5.7652, -5.8215, -5.736, -5.7555, -6.2066, -6.2564, -6.6106, -6.4256, -6.7241, -6.512, -6.8108, -6.6228, -6.9588, -7.0576, -7.0576, -7.054, -7.2375, -7.2406, -7.0653, -6.1179, -6.7346, -7.3102, -7.3184, -7.3185, -7.4624, -6.6082, -7.4718, -7.2511, -7.3204, -7.5502, -7.1778, -7.6462, -7.0924, -7.6503, -7.1232, -5.833, -6.2707, -5.0667, -4.8604, -5.922, -5.7462, -7.0605, -6.8466, -6.3667, -5.0298, -5.7897, -4.763, -6.2173, -5.5666, -5.7049, -5.5104, -6.3256, -5.7936, -6.1995, -5.6744, -5.8189, -5.7902, -5.7699, -5.8753, -6.0284, -5.8966, -6.1392, -6.0692, -5.8883, -5.8868, -5.9596, -6.1123, -6.1148, -6.1171, -6.164, -6.1406, -6.1628, -6.271, -6.9663, -6.9665, -7.0178, -7.0424, -7.1542, -7.2558, -7.216, -7.27, -7.3255, -7.3263, -6.8261, -7.3998, -7.4006, -7.4802, -7.5871, -7.5883, -7.5957, -7.5981, -7.6652, -7.356, -7.6848, -7.0755, -7.7713, -7.7734, -7.5689, -7.7936, -7.7714, -7.8899, -7.8907, -6.9263, -7.6228, -6.9969, -5.8855, -6.6485, -6.5536, -7.2678, -7.0762, -6.051, -5.9588, -6.3081, -5.6307, -5.5557, -5.3688, -5.5166, -5.2132, -5.1999, -6.2538, -6.0536, -6.3476, -6.0129, -5.0215, -5.463, -5.7704, -6.0038, -5.8542, -6.2764, -6.3403, -6.2681, -6.3309, -5.9744, -6.2072, -6.2937, -6.2, -6.0663, -6.1386, -6.265, -6.315, -6.3144, -6.3096, -6.2103, -6.2661, -6.5695, -6.5267, -6.638, -6.8281, -6.7791, -6.828, -7.1176, -7.1882, -7.1886, -7.2695, -7.2319, -7.3003, -7.4298, -7.4325, -7.1829, -7.5261, -7.5262, -7.5263, -7.5291, -6.936, -6.6102, -6.7856, -7.7633, -7.1234, -7.8871, -7.8872, -7.8873, -7.8876, -7.4394, -6.8308, -7.6869, -7.477, -6.4635, -5.1221, -6.0056, -5.9602, -5.4662, -5.9929, -6.2012, -6.8965, -6.5021, -5.268, -5.414, -5.8149, -6.3804, -5.819, -5.5488, -5.9887, -6.1373, -5.768, -6.0296, -5.8012, -6.1354, -5.8495, -5.6571, -5.8924, -5.8783, -6.1175, -6.023, -6.182, -6.1012, -6.2602, -6.1122, -6.2074, -6.258, -6.0552, -6.1406, -6.2289, -6.3533, -5.8102, -6.949, -6.9511, -6.9132, -6.888, -7.0141, -7.086, -7.1609, -7.2386, -6.8951, -6.8898, -6.7837, -7.333, -7.4272, -7.4274, -7.5292, -7.5297, -7.5344, -7.5583, -7.2589, -7.655, -7.6551, -6.4693, -7.6541, -7.6544, -7.6798, -7.0198, -5.0348, -7.0227, -5.6654, -6.2885, -6.784, -6.0817, -6.4444, -6.6692, -5.8008, -5.2841, -5.711, -5.8331, -5.7543, -5.3864, -5.6562, -5.2458, -5.3444, -6.1328, -6.0564, -5.7393, -6.0, -6.3145, -6.1182, -6.0012, -5.7952, -6.0452, -6.3267, -6.1198, -5.9478, -6.2189, -6.3222], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0692, 1.0572, 1.053, 1.0519, 1.043, 1.0378, 1.0361, 1.024, 1.0142, 1.0081, 0.9982, 0.9957, 0.992, 0.9901, 0.9884, 0.9862, 0.9746, 0.971, 0.9688, 0.9669, 0.9624, 0.9545, 0.9535, 0.952, 0.9491, 0.9413, 0.9413, 0.9364, 0.9357, 0.9356, 0.922, 0.9139, 0.8899, 0.8948, 0.8072, 0.8123, 0.82, 0.8656, 0.4961, 0.6611, 0.7778, 0.6469, 0.6868, 0.4124, 0.7426, 0.5471, 0.7149, 0.2845, 0.5488, 0.3689, 0.7594, 0.3044, 0.0973, 0.4593, 0.3578, 0.2136, 0.4624, 0.1935, 0.2461, 0.2326, 0.0871, 0.2444, 0.4828, 0.1153, 0.1979, 0.3987, 0.3623, 0.2228, -0.1729, 0.0678, 0.2771, 0.2395, -0.2592, 0.0272, 1.4831, 1.4695, 1.4543, 1.439, 1.4331, 1.4323, 1.4173, 1.4128, 1.4093, 1.4056, 1.3932, 1.3925, 1.3901, 1.39, 1.3816, 1.3811, 1.3774, 1.3731, 1.3657, 1.3649, 1.3618, 1.361, 1.3607, 1.3569, 1.3566, 1.3517, 1.3492, 1.3425, 1.3398, 1.3331, 1.3218, 1.3121, 1.3247, 1.3221, 1.2542, 1.2184, 1.2921, 1.1147, 1.1057, 0.9481, 1.1039, 0.9793, 1.2308, 0.2565, 1.0193, 0.2718, 0.5406, 0.6061, 0.7695, 0.5276, 0.3513, 0.2483, 0.1379, 0.1059, 0.5421, 0.0279, -0.0325, 0.0345, 0.6052, 0.3277, 0.0227, 0.0035, -0.1201, 0.0099, 0.2472, -0.236, -0.1797, -0.2509, 1.7664, 1.7573, 1.756, 1.7522, 1.751, 1.7149, 1.6761, 1.6642, 1.6489, 1.6442, 1.6407, 1.6406, 1.6297, 1.6237, 1.6234, 1.6232, 1.6232, 1.6158, 1.6059, 1.6055, 1.6024, 1.5987, 1.5983, 1.5929, 1.5865, 1.5851, 1.5837, 1.5832, 1.5771, 1.5771, 1.5596, 1.5085, 1.422, 1.495, 1.1468, 0.8122, 1.2426, 0.8609, 1.2282, 1.3601, 0.854, 0.9627, 0.9583, 0.8903, 0.2858, 0.6291, 0.3926, 0.853, 1.0744, 1.2445, 0.4559, 0.3017, 0.1929, 0.2352, 0.1722, 0.4543, 0.0234, 0.0948, 0.6204, 0.0539, -0.0914, 0.1809, 0.3696, -0.0943, 0.3077, -0.139, 0.1383, -0.0134, -1.4918, 1.781, 1.7678, 1.7606, 1.7602, 1.7595, 1.7563, 1.7515, 1.7243, 1.7219, 1.7176, 1.7107, 1.6917, 1.6768, 1.6758, 1.6681, 1.6587, 1.6456, 1.6439, 1.6399, 1.6364, 1.6311, 1.6269, 1.6181, 1.6177, 1.6175, 1.6101, 1.5981, 1.5975, 1.5974, 1.5962, 1.5946, 1.5745, 1.5902, 1.4845, 1.2258, 1.2051, 1.0888, 1.1678, 0.8727, 1.4134, 1.3506, 0.8756, 1.082, 1.1099, 1.021, 1.1104, 0.4113, 0.4025, 0.2674, 0.5118, 1.0654, 0.5898, 0.7362, 0.4945, -0.3813, 0.0957, 0.3736, 0.3601, 0.6598, 0.8041, 0.6797, -0.3048, 0.072, 0.1651, 0.2203, 0.0811, -0.1057, 0.4424, -0.2966, 0.1971, -0.0324, 0.1187, -0.3346, -0.4324, 2.7447, 2.7222, 2.6693, 2.6558, 2.6422, 2.6415, 2.6164, 2.5982, 2.5649, 2.5463, 2.5463, 2.5138, 2.4877, 2.483, 2.4758, 2.4757, 2.4743, 2.4577, 2.4448, 2.4448, 2.4014, 2.3953, 2.3916, 2.3886, 2.3725, 2.3645, 2.3395, 2.3221, 2.3203, 2.3164, 2.3137, 2.2468, 2.2374, 1.9495, 1.8678, 2.0567, 1.9321, 2.2678, 2.1205, 1.7598, 0.7462, 0.9047, -0.3966, 1.2861, 0.4981, 0.6404, 0.285, 1.3541, 0.4082, 0.9961, 0.011, 0.2265, -0.358, -0.4204, -0.2619, 0.1858, -0.4919, 0.5399, 0.2037, -0.637, -1.0391, -0.743, 0.0888, -0.4439, -0.5719, 0.0598, -0.8175, -0.5405, 2.7689, 2.6021, 2.602, 2.5862, 2.5199, 2.5136, 2.5039, 2.4912, 2.4817, 2.4772, 2.4759, 2.4507, 2.4479, 2.447, 2.4157, 2.3483, 2.3466, 2.3382, 2.3349, 2.3309, 2.3175, 2.3024, 2.2976, 2.2799, 2.2775, 2.2603, 2.248, 2.2316, 2.2201, 2.2184, 2.2144, 2.2173, 2.1972, 2.0462, 2.0103, 1.8906, 2.0866, 2.0112, 1.5145, 1.4432, 1.613, 1.1617, 1.0162, 0.6959, 0.5288, 0.2189, 0.0166, 1.0359, 0.7879, 1.1171, 0.675, -0.6551, -0.0615, 0.2145, 0.5385, 0.2929, 0.842, 0.6762, 0.4724, 0.6367, -0.361, 0.2505, 0.5188, 0.0011, -0.6615, -0.7891, -0.7199, 0.1141, 0.0568, -1.0583, 3.0018, 2.9893, 2.9056, 2.8741, 2.8479, 2.8295, 2.7869, 2.7652, 2.7219, 2.6906, 2.6899, 2.6495, 2.6304, 2.6041, 2.5821, 2.5781, 2.5515, 2.5342, 2.534, 2.5339, 2.5296, 2.4902, 2.4644, 2.401, 2.3979, 2.372, 2.3341, 2.3341, 2.334, 2.3336, 2.2754, 2.1917, 2.3072, 2.2528, 2.0303, 0.8933, 1.4083, 1.2998, 0.9674, 1.3018, 1.3259, 1.8867, 1.4704, -0.0167, 0.0182, 0.4702, 1.2153, 0.3207, -0.2257, 0.4108, 0.6752, -0.0972, 0.3868, -0.1574, 0.605, -0.2272, -0.8094, -0.279, -0.4736, 0.3402, -0.0382, 0.4672, 0.0082, 0.5883, -0.7107, -0.2506, -0.4626, 3.2254, 3.1988, 3.1743, 3.1448, 3.0087, 2.9337, 2.9298, 2.9167, 2.9142, 2.9051, 2.8689, 2.8329, 2.7984, 2.7972, 2.7868, 2.7716, 2.7437, 2.6937, 2.6935, 2.6414, 2.6406, 2.6328, 2.5972, 2.5624, 2.5621, 2.562, 2.562, 2.5618, 2.5615, 2.5244, 2.5089, 2.3623, 2.5044, 2.2839, 2.3733, 2.3746, 2.1158, 2.1948, 2.2573, 1.831, 1.0871, 1.3226, 1.3861, 1.2133, 0.5945, 0.7245, 0.1557, -0.0931, 1.0664, 0.8591, -0.0066, 0.5718, 1.1744, 0.5822, 0.1082, -0.9475, -0.613, 0.5961, -0.4489, -1.5813, -0.5966, -0.1203]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 8, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 8], \"Freq\": [0.08380906826660856, 0.8101543265772161, 0.02793635608886952, 0.02793635608886952, 0.02793635608886952, 0.02023410871457946, 0.02023410871457946, 0.9105348921560757, 0.02023410871457946, 0.6437669071139859, 0.013697168236467784, 0.2511147510019094, 0.027394336472935567, 0.018262890981957044, 0.013697168236467784, 0.031960059218424826, 0.11306682660268656, 0.11306682660268656, 0.11306682660268656, 0.11306682660268656, 0.5653341330134328, 0.08474304664509737, 0.805058943128425, 0.05649536443006491, 0.028247682215032453, 0.05885793360141673, 0.05885793360141673, 0.7651531368184176, 0.05885793360141673, 0.04513889944743072, 0.04513889944743072, 0.812500190053753, 0.04513889944743072, 0.5152755003260621, 0.03435170002173747, 0.02290113334782498, 0.01145056667391249, 0.3664181335651997, 0.04580226669564996, 0.01145056667391249, 0.28693760935495577, 0.12862720419360085, 0.23746560774203238, 0.2077824067742783, 0.026385067526892486, 0.0065962668817231215, 0.10224213666670838, 0.0065962668817231215, 0.08579527543035877, 0.17159055086071753, 0.08579527543035877, 0.08579527543035877, 0.5147716525821526, 0.40473485483920363, 0.17921095562531902, 0.15504768070954567, 0.15907489319550788, 0.0322176998876978, 0.034231306130678915, 0.026176881158754464, 0.010068031214905564, 0.11626474884420396, 0.11626474884420396, 0.11626474884420396, 0.11626474884420396, 0.46505899537681583, 0.10021118741660896, 0.10021118741660896, 0.10021118741660896, 0.10021118741660896, 0.6012671244996537, 0.318187869795945, 0.26471091688906345, 0.24866783101699902, 0.0989323628777308, 0.03208617174412891, 0.013369238226720377, 0.016043085872064454, 0.008021542936032227, 0.0639196776587927, 0.0639196776587927, 0.0639196776587927, 0.7670361319055123, 0.09775830186398483, 0.012219787732998104, 0.012219787732998104, 0.7209674762468882, 0.036659363198994314, 0.10997808959698294, 0.8088840485011091, 0.03235536194004436, 0.03235536194004436, 0.03235536194004436, 0.03235536194004436, 0.03235536194004436, 0.3787129752822099, 0.18016442513425518, 0.06618285004931823, 0.1948717251452148, 0.05882920004383843, 0.08456697506301775, 0.018384125013699508, 0.014707300010959608, 0.444792281877877, 0.09157488156309232, 0.0719516926567154, 0.2158550779701462, 0.03924637781275385, 0.11773913343826156, 0.013082125937584618, 0.006541062968792309, 0.7944387938321761, 0.014444341706039565, 0.02888868341207913, 0.11555473364831652, 0.014444341706039565, 0.014444341706039565, 0.033045029891212524, 0.8591707771715256, 0.033045029891212524, 0.033045029891212524, 0.1237769578973543, 0.1237769578973543, 0.1237769578973543, 0.1237769578973543, 0.4951078315894172, 0.03046368771446729, 0.03046368771446729, 0.03046368771446729, 0.7006648174327477, 0.18278212628680374, 0.3316659713014509, 0.20063743942927276, 0.2968615175229036, 0.07779819079910577, 0.04094641621005567, 0.0122839248630167, 0.03070981215754175, 0.010236604052513917, 0.26583863662221163, 0.06645965915555291, 0.06645965915555291, 0.06645965915555291, 0.5316772732444233, 0.05050841381484065, 0.8081346210374504, 0.05050841381484065, 0.05050841381484065, 0.10413553838947145, 0.06363838457134367, 0.17934453833742306, 0.45125399968770963, 0.06363838457134367, 0.0578530768830397, 0.0752089999479516, 0.00578530768830397, 0.06740024701244071, 0.06740024701244071, 0.06740024701244071, 0.7414027171368478, 0.05415471315742259, 0.05415471315742259, 0.8123206973613388, 0.05415471315742259, 0.05026752741490032, 0.05026752741490032, 0.8042804386384051, 0.05026752741490032, 0.0699104072662774, 0.07490400778529721, 0.07490400778529721, 0.6192064643584569, 0.029961603114118884, 0.054929605709217955, 0.0699104072662774, 0.004993600519019814, 0.14501330496588194, 0.14501330496588194, 0.14501330496588194, 0.14501330496588194, 0.4350399148976459, 0.08948469420017414, 0.08948469420017414, 0.08948469420017414, 0.08948469420017414, 0.4474234710008707, 0.028597054739666675, 0.20017938317766673, 0.05719410947933335, 0.028597054739666675, 0.6291352042726669, 0.05719410947933335, 0.4061341368121229, 0.18077921943466446, 0.12629781083791627, 0.1461092321458247, 0.03466998728883976, 0.06686354691419097, 0.019811421307908434, 0.019811421307908434, 0.036174729624655295, 0.7017897547183127, 0.04340967554958636, 0.028939783699724237, 0.007234945924931059, 0.02170483777479318, 0.04340967554958636, 0.11575913479889695, 0.0709408714039984, 0.0709408714039984, 0.0709408714039984, 0.7094087140399841, 0.2666599364931315, 0.1858538951315765, 0.4605944357608635, 0.0242418124084665, 0.0080806041361555, 0.0242418124084665, 0.0080806041361555, 0.0080806041361555, 0.12396375484026806, 0.12396375484026806, 0.12396375484026806, 0.12396375484026806, 0.49585501936107224, 0.051148805360137904, 0.9036288946957696, 0.017049601786712634, 0.017049601786712634, 0.017049601786712634, 0.5614123806561025, 0.15430632684699894, 0.06566226674340381, 0.09192717344076533, 0.032831133371701904, 0.05581292673189323, 0.032831133371701904, 0.00656622667434038, 0.04213791603141102, 0.04213791603141102, 0.04213791603141102, 0.8427583206282203, 0.1297405134088846, 0.0648702567044423, 0.0648702567044423, 0.1297405134088846, 0.5189620536355384, 0.09581661115228149, 0.09581661115228149, 0.09581661115228149, 0.09581661115228149, 0.574899666913689, 0.10529313583838004, 0.05264656791919002, 0.05264656791919002, 0.7370519508686604, 0.08708950929299543, 0.08708950929299543, 0.08708950929299543, 0.08708950929299543, 0.609626565050968, 0.1759411007348678, 0.09819968413108901, 0.4132570040516662, 0.23731590331679842, 0.008183307010924084, 0.004091653505462042, 0.06137480258193063, 0.004091653505462042, 0.050813465177197634, 0.8130154428351621, 0.050813465177197634, 0.050813465177197634, 0.7044358272103399, 0.03707556985317579, 0.08898136764762189, 0.03707556985317579, 0.007415113970635157, 0.09639648161825705, 0.014830227941270315, 0.007415113970635157, 0.13841677808426492, 0.13841677808426492, 0.13841677808426492, 0.13841677808426492, 0.4152503342527948, 0.14561226966956706, 0.14561226966956706, 0.14561226966956706, 0.14561226966956706, 0.4368368090087012, 0.14559817361343785, 0.14559817361343785, 0.14559817361343785, 0.14559817361343785, 0.4367945208403135, 0.07090597938990079, 0.07090597938990079, 0.07090597938990079, 0.7090597938990078, 0.15639169968847708, 0.15639169968847708, 0.10426113312565138, 0.05213056656282569, 0.05213056656282569, 0.4691750990654312, 0.10914541108924158, 0.10914541108924158, 0.10914541108924158, 0.10914541108924158, 0.5457270554462079, 0.24751111053456915, 0.10125454521868738, 0.12375555526728457, 0.27563737309531566, 0.039376767585045096, 0.09562929270653808, 0.016875757536447898, 0.10125454521868738, 0.053969735813248036, 0.053969735813248036, 0.053969735813248036, 0.7016065655722246, 0.053969735813248036, 0.053969735813248036, 0.0728523757465014, 0.0728523757465014, 0.2914095029860056, 0.1457047514930028, 0.0728523757465014, 0.364261878732507, 0.0925429601080974, 0.0925429601080974, 0.0925429601080974, 0.0925429601080974, 0.6478007207566818, 0.28752366409714514, 0.20388041635979381, 0.18035575293366377, 0.18035575293366377, 0.05750473281942903, 0.04966317834405234, 0.028752366409714514, 0.010455405967168914, 0.02118488011146816, 0.02118488011146816, 0.9109498447931309, 0.02118488011146816, 0.11289011402060346, 0.11289011402060346, 0.11289011402060346, 0.11289011402060346, 0.5644505701030174, 0.07085518435826108, 0.07085518435826108, 0.07085518435826108, 0.7085518435826107, 0.12549409041357618, 0.773880224217053, 0.041831363471192054, 0.020915681735596027, 0.12284054690887783, 0.12284054690887783, 0.12284054690887783, 0.12284054690887783, 0.4913621876355113, 0.033002064165266784, 0.8580536682969364, 0.033002064165266784, 0.033002064165266784, 0.11325690128188627, 0.11325690128188627, 0.056628450640943136, 0.056628450640943136, 0.056628450640943136, 0.5662845064094314, 0.056628450640943136, 0.8793038730047524, 0.018708593042654306, 0.07483437217061722, 0.018708593042654306, 0.018708593042654306, 0.12280044458775861, 0.12280044458775861, 0.12280044458775861, 0.12280044458775861, 0.49120177835103446, 0.057287615377992325, 0.057287615377992325, 0.057287615377992325, 0.6874513845359078, 0.057287615377992325, 0.4301944961966193, 0.21292454862256915, 0.19119755386516415, 0.09125337798110107, 0.02389969423314552, 0.01412254659231326, 0.030417792660367024, 0.006518098427221505, 0.06396510536783087, 0.06396510536783087, 0.06396510536783087, 0.7675812644139706, 0.06396510536783087, 0.1389404477598084, 0.791960552230908, 0.027788089551961683, 0.013894044775980842, 0.013894044775980842, 0.013894044775980842, 0.075338894846819, 0.075338894846819, 0.075338894846819, 0.075338894846819, 0.602711158774552, 0.075338894846819, 0.30862203969919577, 0.1405332502201695, 0.21493320621908277, 0.23146652977439683, 0.02479998533297109, 0.04408886281417083, 0.019288877481199736, 0.016533323555314058, 0.27776475861835687, 0.14443767448154557, 0.07777413241313992, 0.4222024330999024, 0.011110590344734274, 0.02222118068946855, 0.011110590344734274, 0.02222118068946855, 0.04468497531373051, 0.04468497531373051, 0.04468497531373051, 0.04468497531373051, 0.8043295556471491, 0.13838946876692948, 0.13838946876692948, 0.13838946876692948, 0.13838946876692948, 0.41516840630078844, 0.046705089861233874, 0.046705089861233874, 0.7939865276409758, 0.046705089861233874, 0.34785000973580055, 0.18070130375885743, 0.09938571706737158, 0.2755694882322576, 0.04065779334574292, 0.013552597781914307, 0.027105195563828614, 0.018070130375885743, 0.03388283399900403, 0.7708344734773418, 0.08470708499751009, 0.025412125499253025, 0.03388283399900403, 0.016941416999502017, 0.05082425099850605, 0.03850810152001669, 0.8086701319203505, 0.03850810152001669, 0.07701620304003338, 0.10336533872267249, 0.10336533872267249, 0.10336533872267249, 0.10336533872267249, 0.5168266936133625, 0.11796271940836259, 0.11796271940836259, 0.11796271940836259, 0.11796271940836259, 0.47185087763345035, 0.10338782061916131, 0.10338782061916131, 0.10338782061916131, 0.10338782061916131, 0.5169391030958066, 0.5137190455297856, 0.19445862175421266, 0.10448522959927845, 0.09868049462154076, 0.0319260423775573, 0.02612130739981961, 0.017414204933213075, 0.014511837444344228, 0.8449626267592384, 0.04023631555996374, 0.04023631555996374, 0.04023631555996374, 0.15872839403258893, 0.10581892935505928, 0.05290946467752964, 0.05290946467752964, 0.6349135761303557, 0.34790858504319133, 0.2951951630669502, 0.08434147516198577, 0.07379879076673755, 0.010542684395248221, 0.031628053185744666, 0.12651221274297866, 0.031628053185744666, 0.138785115685365, 0.27757023137073, 0.046261705228455005, 0.046261705228455005, 0.46261705228455, 0.11598702800506344, 0.11598702800506344, 0.11598702800506344, 0.11598702800506344, 0.46394811202025377, 0.17816832930385673, 0.17816832930385673, 0.08908416465192837, 0.08908416465192837, 0.08908416465192837, 0.35633665860771346, 0.4080643065997263, 0.2076626186610707, 0.06244400421276951, 0.2105669909500367, 0.04066121204552433, 0.03485246746759228, 0.029043722889660235, 0.007260930722415059, 0.10627214236323629, 0.6251302491955075, 0.21254428472647258, 0.025005209967820304, 0.006251302491955076, 0.025005209967820304, 0.24032190570772458, 0.1826446483378707, 0.04806438114154492, 0.26916053439265153, 0.19225752456617967, 0.019225752456617968, 0.038451504913235936, 0.009612876228308984, 0.2243995758493511, 0.06495777195639112, 0.08857877994053334, 0.23030482784538667, 0.37203087575024, 0.0059052519960355555, 0.011810503992071111, 0.08866975379367964, 0.08866975379367964, 0.08866975379367964, 0.08866975379367964, 0.6206882765557574, 0.056806602580649145, 0.056806602580649145, 0.056806602580649145, 0.056806602580649145, 0.7384858335484389, 0.08853470646590507, 0.08853470646590507, 0.08853470646590507, 0.08853470646590507, 0.6197429452613356, 0.13826651725644487, 0.311099663827001, 0.06913325862822244, 0.1728331465705561, 0.24196640519877854, 0.8175499980284616, 0.04541944433491454, 0.04541944433491454, 0.04541944433491454, 0.07854327871736641, 0.07854327871736641, 0.07854327871736641, 0.07854327871736641, 0.7068895084562976, 0.1076307461493158, 0.1076307461493158, 0.1076307461493158, 0.1076307461493158, 0.538153730746579, 0.07575217465556544, 0.07575217465556544, 0.07575217465556544, 0.07575217465556544, 0.681769571900089, 0.07575217465556544, 0.04381271763329346, 0.08762543526658692, 0.7448161997659889, 0.04381271763329346, 0.04381271763329346, 0.18440514113282452, 0.015367095094402042, 0.04610128528320613, 0.4302786626432572, 0.030734190188804085, 0.1075696656608143, 0.16903804603842248, 0.015367095094402042, 0.10180274336663575, 0.10180274336663575, 0.10180274336663575, 0.10180274336663575, 0.6108164601998145, 0.3466374987076178, 0.1277085521554381, 0.18244078879348302, 0.1322695718752752, 0.08665937467690445, 0.10034243383641567, 0.018244078879348303, 0.006841529579755614, 0.46608162918184165, 0.19690079521585824, 0.14954490775887969, 0.10468143543121577, 0.03489381181040526, 0.03489381181040526, 0.004984830258629323, 0.009969660517258646, 0.4165772643959899, 0.09918506295142618, 0.10414431609899748, 0.2281256447882802, 0.024796265737856545, 0.10910356924656879, 0.009918506295142617, 0.009918506295142617, 0.2774481567240891, 0.20346198159766535, 0.01849654378160594, 0.24970334105168018, 0.01849654378160594, 0.15722062214365048, 0.05548963134481782, 0.02774481567240891, 0.15980794959773545, 0.5682060430141704, 0.11541685248725338, 0.03551287768838565, 0.017756438844192826, 0.03551287768838565, 0.06214753595467489, 0.04501544407962842, 0.8102779934333115, 0.04501544407962842, 0.04501544407962842, 0.04854617409831929, 0.8252849596714279, 0.04854617409831929, 0.04854617409831929, 0.4102177808431475, 0.21941881300912539, 0.13355927748381546, 0.1192493548962638, 0.02384987097925276, 0.021464883881327484, 0.03815979356680442, 0.033389819370953866, 0.17549869684580924, 0.17549869684580924, 0.08774934842290462, 0.08774934842290462, 0.08774934842290462, 0.3509973936916185, 0.08372650761498598, 0.08372650761498598, 0.08372650761498598, 0.08372650761498598, 0.5860855533049019, 0.4025394144972149, 0.050317426812151865, 0.050317426812151865, 0.050317426812151865, 0.050317426812151865, 0.10063485362430373, 0.050317426812151865, 0.3019045608729112, 0.12111037589212637, 0.12111037589212637, 0.12111037589212637, 0.12111037589212637, 0.48444150356850546, 0.1037961138142994, 0.1037961138142994, 0.1037961138142994, 0.1037961138142994, 0.518980569071497, 0.050161163821596154, 0.050161163821596154, 0.8025786211455385, 0.050161163821596154, 0.08420014506386751, 0.08420014506386751, 0.08420014506386751, 0.08420014506386751, 0.505200870383205, 0.08420014506386751, 0.30951807756185795, 0.5340276690327831, 0.05231291451749712, 0.05885202883218425, 0.015257933400936658, 0.023976752487186177, 0.004359409543124759, 0.0021797047715623797, 0.07876675786833409, 0.1312779297805568, 0.07876675786833409, 0.05251117191222272, 0.4988561331661159, 0.1312779297805568, 0.02625558595611136, 0.11705640307054178, 0.11705640307054178, 0.11705640307054178, 0.11705640307054178, 0.46822561228216714, 0.11705640307054178, 0.4132019625279759, 0.2652111887147647, 0.18755266384248553, 0.03809663484300487, 0.03077035891165778, 0.017583062235233017, 0.03077035891165778, 0.016117807048963602, 0.04658140943827674, 0.09316281887655348, 0.04658140943827674, 0.04658140943827674, 0.7453025510124278, 0.3394850272972913, 0.19916454934774422, 0.16974251364864565, 0.20369101637837478, 0.029422035699098577, 0.027158802183783304, 0.020369101637837478, 0.0090529340612611, 0.9058389398175741, 0.02209363267847742, 0.02209363267847742, 0.02209363267847742, 0.07451619355804767, 0.037258096779023836, 0.7824200323595006, 0.037258096779023836, 0.9270575722097809, 0.02439625190025739, 0.012198125950128695, 0.012198125950128695, 0.012198125950128695, 0.012198125950128695, 0.0994662491026325, 0.0994662491026325, 0.0994662491026325, 0.198932498205265, 0.49733124551316255, 0.07996614353002418, 0.07996614353002418, 0.07996614353002418, 0.7196952917702176, 0.33688172825838525, 0.15159677771627336, 0.1894959721453417, 0.17054637493080751, 0.03789919442906834, 0.06316532404844723, 0.03579368362745343, 0.014738575611304355, 0.11155505055694988, 0.11155505055694988, 0.11155505055694988, 0.11155505055694988, 0.5577752527847494, 0.10764403909756243, 0.10764403909756243, 0.10764403909756243, 0.10764403909756243, 0.5382201954878122, 0.8434440907231336, 0.03514350378013057, 0.03514350378013057, 0.03514350378013057, 0.35350298137580943, 0.09277011942182845, 0.3554560365215321, 0.10448845029616467, 0.025389716894395155, 0.05077943378879031, 0.010741803301474872, 0.0068356930100294646, 0.07098377226396967, 0.07098377226396967, 0.07098377226396967, 0.7098377226396967, 0.04345707470784581, 0.8256844194490703, 0.04345707470784581, 0.04345707470784581, 0.37109745422084434, 0.17008633318455366, 0.015462393925868515, 0.015462393925868515, 0.4174846359984499, 0.015462393925868515, 0.046842387141211035, 0.1405271614236331, 0.023421193570605518, 0.6792146135475601, 0.023421193570605518, 0.023421193570605518, 0.023421193570605518, 0.023421193570605518, 0.8267603763325847, 0.027558679211086158, 0.08267603763325847, 0.027558679211086158, 0.04689795963891153, 0.04689795963891153, 0.04689795963891153, 0.797265313861496, 0.3937696008277104, 0.1722742003621233, 0.17381236286535653, 0.1522780878200911, 0.03383957507113136, 0.018457950038798923, 0.035377737574364604, 0.018457950038798923, 0.07996266528495509, 0.07996266528495509, 0.07996266528495509, 0.7196639875645958, 0.054674958461108154, 0.10934991692221631, 0.054674958461108154, 0.710774459994406, 0.054674958461108154, 0.7677001865365726, 0.012795003108942877, 0.1151550279804859, 0.03838500932682863, 0.012795003108942877, 0.012795003108942877, 0.012795003108942877, 0.012795003108942877, 0.23024952076741997, 0.03542300319498769, 0.15054776357869767, 0.19039864217305882, 0.3409464057517565, 0.013283626198120383, 0.026567252396240765, 0.004427875399373461, 0.061934817533063805, 0.061934817533063805, 0.061934817533063805, 0.061934817533063805, 0.7432178103967656, 0.29930548710807514, 0.11972219484323006, 0.2095138409756526, 0.05986109742161503, 0.029930548710807515, 0.2693749383972676, 0.3169914107774044, 0.22954550435605148, 0.0655844298160147, 0.10930738302669117, 0.04372295321067647, 0.03279221490800735, 0.03279221490800735, 0.185822551145375, 0.08446671697329866, 0.08446671697329866, 0.08446671697329866, 0.08446671697329866, 0.16893343394659732, 0.506800301839792, 0.8358977533425345, 0.041794887667126725, 0.041794887667126725, 0.041794887667126725, 0.08790266791748777, 0.12306373508448287, 0.23733720337721698, 0.3252398712947047, 0.03516106716699511, 0.017580533583497555, 0.15822480225147798, 0.008790266791748777, 0.05513850278292401, 0.05513850278292401, 0.05513850278292401, 0.7719390389609362, 0.045127083353089854, 0.045127083353089854, 0.045127083353089854, 0.8122875003556174, 0.016084796097237498, 0.32169592194474994, 0.6112222516950249, 0.032169592194474995, 0.016084796097237498, 0.10360565530075512, 0.10360565530075512, 0.10360565530075512, 0.10360565530075512, 0.5180282765037756, 0.07995701883353727, 0.07995701883353727, 0.07995701883353727, 0.7196131695018354, 0.1431467042924739, 0.1431467042924739, 0.1431467042924739, 0.1431467042924739, 0.4294401128774217, 0.0957747375358799, 0.0957747375358799, 0.0957747375358799, 0.0957747375358799, 0.5746484252152794, 0.10022466640403928, 0.10022466640403928, 0.10022466640403928, 0.10022466640403928, 0.6013479984242357, 0.13017139497830185, 0.13017139497830185, 0.13017139497830185, 0.13017139497830185, 0.5206855799132074, 0.8672830478454732, 0.04129919275454634, 0.02064959637727317, 0.04129919275454634, 0.02064959637727317, 0.14483526070442276, 0.14483526070442276, 0.14483526070442276, 0.14483526070442276, 0.4345057821132683, 0.22166091380455102, 0.04925798084545578, 0.02462899042272789, 0.22166091380455102, 0.07388697126818368, 0.34480586591819046, 0.04925798084545578, 0.02462899042272789, 0.48249653212529575, 0.37873383704458696, 0.05966354967140754, 0.04928728016333666, 0.002594067377017719, 0.002594067377017719, 0.02334660639315947, 0.002594067377017719, 0.9294612720186793, 0.01896859738813631, 0.01896859738813631, 0.01896859738813631, 0.1071117348013987, 0.05355586740069935, 0.05355586740069935, 0.05355586740069935, 0.6962262762090915, 0.0408932640656036, 0.0408932640656036, 0.0408932640656036, 0.0408932640656036, 0.8178652813120719, 0.06581471596771904, 0.06581471596771904, 0.06581471596771904, 0.06581471596771904, 0.7239618756449095, 0.4127756808155296, 0.19492184927400008, 0.1031939202038824, 0.057329955668823554, 0.2063878404077648, 0.022931982267529422, 0.011465991133764711, 0.2971402327309632, 0.30144661291546987, 0.18517434793378865, 0.11196588479717452, 0.0344510414760537, 0.004306380184506712, 0.004306380184506712, 0.051676562214080554, 0.5251323733369724, 0.25608307094827665, 0.05834804148188582, 0.0713142729223049, 0.02917402074094291, 0.042140252181361984, 0.012966231440419071, 0.0064831157202095355, 0.6271006879326422, 0.13213158307610853, 0.08808772205073903, 0.07969841518876389, 0.0020973267154937862, 0.012583960292962718, 0.0041946534309875725, 0.054530494602838445, 0.07165119721849406, 0.17912799304623514, 0.07165119721849406, 0.03582559860924703, 0.6090351763571995, 0.0921102850629483, 0.0921102850629483, 0.0921102850629483, 0.0921102850629483, 0.6447719954406381, 0.03195114308509164, 0.0798778577127291, 0.04792671462763746, 0.7828030055847451, 0.03195114308509164, 0.13794973955078763, 0.13794973955078763, 0.13794973955078763, 0.13794973955078763, 0.4138492186523629, 0.05049855462061798, 0.8079768739298877, 0.05049855462061798, 0.05049855462061798, 0.07550145032239484, 0.07550145032239484, 0.07550145032239484, 0.6795130529015535, 0.1456175289985539, 0.1456175289985539, 0.1456175289985539, 0.1456175289985539, 0.4368525869956617, 0.29250752420968834, 0.21630808512985356, 0.08848967118948554, 0.33675235980443113, 0.007374139265790463, 0.002458046421930154, 0.05161897486053324, 0.004916092843860308, 0.09940218102129723, 0.09940218102129723, 0.09940218102129723, 0.09940218102129723, 0.5964130861277834, 0.06798144014646627, 0.8308842684568101, 0.060427946796858915, 0.015106986699214729, 0.007553493349607364, 0.007553493349607364, 0.007553493349607364, 0.007553493349607364, 0.8758592800526795, 0.020368820466341385, 0.04073764093268277, 0.020368820466341385, 0.020368820466341385, 0.020368820466341385, 0.2528842097528564, 0.2589778051685879, 0.14015269456182405, 0.2193694349663333, 0.03960837020225462, 0.04265516791012036, 0.03351477478652314, 0.012187190831462962, 0.4962969417769003, 0.014596968875791184, 0.014596968875791184, 0.18976059538528542, 0.014596968875791184, 0.24814847088845016, 0.014596968875791184, 0.014596968875791184, 0.22834935541849447, 0.6249561306190374, 0.018027580690933773, 0.0060091935636445905, 0.0060091935636445905, 0.0060091935636445905, 0.11417467770924723, 0.07550060782343289, 0.07550060782343289, 0.07550060782343289, 0.679505470410896, 0.39240990551436633, 0.23732575123922753, 0.1621334340149178, 0.10103967627016618, 0.007049279739779035, 0.04699519826519357, 0.018798079306077426, 0.034071518742265335, 0.17229662469918713, 0.05743220823306238, 0.1520264335581063, 0.4189172835823373, 0.06418893861342266, 0.06418893861342266, 0.05405384304288224, 0.016891825950900698, 0.4252872047978458, 0.19563211420700907, 0.11057467324743991, 0.11057467324743991, 0.0042528720479784584, 0.09781605710350454, 0.04678159252776304, 0.0042528720479784584, 0.10607271846926015, 0.10607271846926015, 0.10607271846926015, 0.10607271846926015, 0.5303635923463007, 0.1928622435884607, 0.13206871028340245, 0.138357696487374, 0.48005928023649463, 0.010481643673285908, 0.027252273550543365, 0.01257797240794309, 0.004192657469314363, 0.091776389046999, 0.091776389046999, 0.091776389046999, 0.091776389046999, 0.642434723328993, 0.21097168172698644, 0.18374952924608495, 0.1088886099236059, 0.387915672852846, 0.006805538120225369, 0.027222152480901474, 0.05444430496180295, 0.020416614360676107, 0.20699154152355637, 0.05174788538088909, 0.05174788538088909, 0.05174788538088909, 0.20699154152355637, 0.41398308304711273, 0.0697992063684787, 0.0697992063684787, 0.0697992063684787, 0.0697992063684787, 0.0697992063684787, 0.6281928573163083, 0.04520000888373719, 0.04520000888373719, 0.04520000888373719, 0.8136001599072694, 0.039381234022315796, 0.8270059144686317, 0.039381234022315796, 0.039381234022315796, 0.2574742056194587, 0.343298940825945, 0.062417989241080905, 0.09362698386162137, 0.023406745965405342, 0.015604497310270226, 0.14044047579243205, 0.062417989241080905, 0.28546788713633914, 0.32484276812066176, 0.0984372024608066, 0.05906232147648396, 0.00984372024608066, 0.00984372024608066, 0.1476558036912099, 0.07874976196864528, 0.04498931472668988, 0.8772916371704527, 0.02249465736334494, 0.02249465736334494, 0.02249465736334494, 0.35816347547012867, 0.1866791447904913, 0.09333957239524565, 0.23443427485317514, 0.01085343865060996, 0.017365501840975937, 0.09333957239524565, 0.006512063190365976, 0.15087723661841174, 0.10972889935884489, 0.5486444967942244, 0.1234450117787005, 0.027432224839711222, 0.013716112419855611, 0.013716112419855611, 0.013716112419855611, 0.026412649336863454, 0.026412649336863454, 0.818792129442767, 0.10565059734745381, 0.048703749895998884, 0.048703749895998884, 0.048703749895998884, 0.827963748231981, 0.10517545494811342, 0.10517545494811342, 0.10517545494811342, 0.10517545494811342, 0.5258772747405671, 0.35164541855314896, 0.12042651320313322, 0.12042651320313322, 0.1348776947875092, 0.09634121056250657, 0.08670708950625591, 0.07707296845000526, 0.009634121056250658, 0.33928401597720476, 0.1770177474663677, 0.029502957911061285, 0.029502957911061285, 0.014751478955530643, 0.38353845284379673, 0.14828438380314274, 0.08897063028188563, 0.7117650422550851, 0.02965687676062855, 0.02965687676062855, 0.09168318095610704, 0.7232784275426223, 0.08149616084987293, 0.020374040212468233, 0.010187020106234117, 0.07130914074363881, 0.010187020106234117, 0.05962064487351461, 0.298103224367573, 0.05962064487351461, 0.05962064487351461, 0.5365858038616315, 0.016113949415351894, 0.016113949415351894, 0.918495116675058, 0.016113949415351894, 0.016113949415351894, 0.016113949415351894, 0.31177224711116214, 0.06235444942223242, 0.06235444942223242, 0.06235444942223242, 0.4988355953778594, 0.4380385612616396, 0.16355335891262518, 0.20053064005808824, 0.15075353082381102, 0.01848864057273154, 0.0071110156048967465, 0.011377624967834794, 0.009955421846855444, 0.07579968628299463, 0.07579968628299463, 0.07579968628299463, 0.07579968628299463, 0.6821971765469517, 0.05285828936096438, 0.05285828936096438, 0.05285828936096438, 0.7928743404144657, 0.13150503912500913, 0.1972575586875137, 0.06575251956250457, 0.06575251956250457, 0.06575251956250457, 0.460267636937532, 0.7315277844732385, 0.1334904716192041, 0.04271695091814531, 0.016018856594304492, 0.010679237729536328, 0.005339618864768164, 0.005339618864768164, 0.06941504524198613, 0.12166917273473882, 0.12166917273473882, 0.12166917273473882, 0.12166917273473882, 0.48667669093895527, 0.5312976153316367, 0.2671095630889611, 0.035475488847752645, 0.0959924992350954, 0.035475488847752645, 0.02629359761656961, 0.0020867934616325087, 0.0062603803848975265, 0.052081246237356514, 0.052081246237356514, 0.7812186935603477, 0.052081246237356514, 0.941546954678621, 0.01384627874527384, 0.01384627874527384, 0.01384627874527384, 0.09576764387795537, 0.09576764387795537, 0.09576764387795537, 0.09576764387795537, 0.09576764387795537, 0.5746058632677322, 0.21748783462872145, 0.19030185530013127, 0.07249594487624049, 0.42591367614791287, 0.036247972438120245, 0.027185979328590182, 0.009061993109530061, 0.027185979328590182, 0.5819850495383656, 0.11292247229848885, 0.00868634402296068, 0.00868634402296068, 0.0434317201148034, 0.217158600574017, 0.026059032068882042, 0.13713220245037136, 0.13713220245037136, 0.13713220245037136, 0.13713220245037136, 0.41139660735111405, 0.09211171724323315, 0.09211171724323315, 0.09211171724323315, 0.09211171724323315, 0.6447820207026321, 0.12396818331011825, 0.12396818331011825, 0.12396818331011825, 0.12396818331011825, 0.495872733240473, 0.07385104448481042, 0.14770208896962084, 0.03692552224240521, 0.7015849226056989, 0.09845438912153585, 0.09845438912153585, 0.09845438912153585, 0.09845438912153585, 0.5907263347292152, 0.044993266971310314, 0.8098788054835857, 0.044993266971310314, 0.044993266971310314, 0.3059917542698336, 0.06119835085396673, 0.06119835085396673, 0.12239670170793346, 0.4283884559777671, 0.05307928807726228, 0.05307928807726228, 0.05307928807726228, 0.05307928807726228, 0.7961893211589341, 0.34687038256265723, 0.35541398804449603, 0.12473664003484718, 0.015378489867309926, 0.11106687126390502, 0.02563081644551654, 0.015378489867309926, 0.0051261632891033086, 0.617521510280078, 0.15936038974969752, 0.09960024359356096, 0.03652008931763902, 0.016600040598926825, 0.04648011367699511, 0.01328003247914146, 0.009960024359356095, 0.9025469476583245, 0.0331818730756737, 0.01327274923026948, 0.00663637461513474, 0.00663637461513474, 0.0331818730756737, 0.00663637461513474, 0.2445069058546918, 0.2529381784703708, 0.2571538147782103, 0.11382218031166685, 0.09274399877246929, 0.01264690892351854, 0.01264690892351854, 0.01264690892351854, 0.11522417972261993, 0.11522417972261993, 0.11522417972261993, 0.11522417972261993, 0.46089671889047973, 0.7234713297428801, 0.07272992203764403, 0.03445101570204191, 0.126320390907487, 0.011483671900680637, 0.015311562534240849, 0.011483671900680637, 0.003827890633560212, 0.0910523303428514, 0.0910523303428514, 0.0910523303428514, 0.0910523303428514, 0.6373663123999599, 0.5350610779538788, 0.037680357602385826, 0.030144286081908663, 0.18840178801192914, 0.007536071520477166, 0.060288572163817326, 0.14318535888906614, 0.007536071520477166, 0.10941179621545974, 0.10941179621545974, 0.10941179621545974, 0.10941179621545974, 0.5470589810772987, 0.29283371527759466, 0.09761123842586489, 0.09761123842586489, 0.09761123842586489, 0.39044495370345955, 0.24912670131793185, 0.20905737173532743, 0.24912670131793185, 0.16550375262380088, 0.06968579057844247, 0.020905737173532744, 0.024390026702454867, 0.012195013351227434, 0.11430599178003395, 0.6286829547901868, 0.23677669725864176, 0.004082356849286927, 0.004082356849286927, 0.004082356849286927, 0.004082356849286927, 0.004082356849286927, 0.0737587079768976, 0.0737587079768976, 0.0737587079768976, 0.0737587079768976, 0.6638283717920783, 0.07248566543092246, 0.07248566543092246, 0.07248566543092246, 0.07248566543092246, 0.7248566543092245, 0.36022398705196595, 0.22659250798430114, 0.04067045015102841, 0.31374347259364777, 0.01743019292186932, 0.01743019292186932, 0.011620128614579546, 0.005810064307289773, 0.3955226865839707, 0.1329215586060885, 0.19127736482339566, 0.184793386354806, 0.025935913874358736, 0.009725967702884525, 0.05511381698301231, 0.006483978468589684, 0.1302821952467986, 0.1302821952467986, 0.1302821952467986, 0.1302821952467986, 0.5211287809871944, 0.8387928534260495, 0.03494970222608539, 0.03494970222608539, 0.03494970222608539, 0.03494970222608539, 0.8261461895989676, 0.04348137839994566, 0.04348137839994566, 0.04348137839994566, 0.13344977690034668, 0.1556914063837378, 0.49821250042796095, 0.17348470997045068, 0.013344977690034667, 0.022241629483391114, 0.0044483258966782225, 0.31874736548711624, 0.20081084025688323, 0.08287431502665021, 0.254997892389693, 0.044624631168196274, 0.031874736548711625, 0.05737452578768092, 0.009562420964613487, 0.42230510836183877, 0.35661320261666385, 0.08446102167236774, 0.0563073477815785, 0.003128185987865472, 0.012512743951461888, 0.006256371975730944, 0.06256371975730944, 0.1181140481276901, 0.1181140481276901, 0.1181140481276901, 0.1181140481276901, 0.4724561925107604, 0.4629458967690641, 0.18673447936903426, 0.07975118389719171, 0.17506357440846962, 0.0369578657084547, 0.029177262401411604, 0.01556120661408619, 0.011670904960564641, 0.2518735642843305, 0.2305283469720991, 0.38421391162016516, 0.09391895617381815, 0.008538086924892559, 0.004269043462446279, 0.008538086924892559, 0.017076173849785117, 0.038690078521480084, 0.8124916489510818, 0.11607023556444025, 0.019345039260740042, 0.08584954173465237, 0.08584954173465237, 0.08584954173465237, 0.17169908346930474, 0.6009467921425666, 0.18742843175299956, 0.04685710793824989, 0.04685710793824989, 0.14057132381474965, 0.04685710793824989, 0.3748568635059991, 0.09371421587649978, 0.12941086688241812, 0.23293956038835265, 0.051764346752967254, 0.2847039071413199, 0.025882173376483627, 0.2847039071413199, 0.29768299236759, 0.21624141898400406, 0.2274747394507056, 0.13199151548374274, 0.06459159268353368, 0.028083301166753775, 0.016849980700052267, 0.014041650583376888, 0.19796676313594053, 0.7018821602092437, 0.05399093540071105, 0.017996978466903686, 0.017996978466903686, 0.0550372759421924, 0.7705218631906936, 0.0550372759421924, 0.0550372759421924, 0.04700887599390477, 0.04700887599390477, 0.04700887599390477, 0.799150891896381, 0.04700887599390477, 0.02116238513190783, 0.9099825606720368, 0.02116238513190783, 0.02116238513190783, 0.13290275326319545, 0.03322568831579886, 0.7641908312633738, 0.03322568831579886, 0.4105693991861248, 0.03421411659884373, 0.017107058299421864, 0.017107058299421864, 0.4961046906832341, 0.8359756956158068, 0.041798784780790334, 0.041798784780790334, 0.041798784780790334, 0.33162005234451725, 0.13833293612085576, 0.3714144586258593, 0.08906367120109891, 0.02084468900451251, 0.015159773821463645, 0.024634632459878424, 0.009474858638414779, 0.7351481168309276, 0.031150343933513885, 0.19313213238778607, 0.012460137573405554, 0.006230068786702777, 0.01869020636010833, 0.006230068786702777, 0.10925095310607023, 0.10925095310607023, 0.10925095310607023, 0.10925095310607023, 0.5462547655303511, 0.11051539369259364, 0.06630923621555618, 0.7072985196325993, 0.022103078738518727, 0.044206157477037454, 0.022103078738518727, 0.3391695305993937, 0.2967733392744695, 0.05652825509989895, 0.10599047831231054, 0.14132063774974737, 0.014132063774974737, 0.007066031887487369, 0.04239619132492421, 0.05415241407488646, 0.05415241407488646, 0.8122862111232969, 0.05415241407488646, 0.14561339741501994, 0.14561339741501994, 0.14561339741501994, 0.14561339741501994, 0.4368401922450598, 0.15262391705744466, 0.24419826729191144, 0.09157435023446679, 0.09157435023446679, 0.36629740093786717, 0.03052478341148893, 0.03052478341148893, 0.4316956479795844, 0.15525896111546456, 0.03029443143716382, 0.15525896111546456, 0.03029443143716382, 0.14011174539688265, 0.007573607859290955, 0.05301525501503668, 0.812528755861824, 0.04875172535170945, 0.01625057511723648, 0.01625057511723648, 0.01625057511723648, 0.08125287558618241, 0.01625057511723648, 0.028310475893667274, 0.028310475893667274, 0.8493142768100183, 0.05662095178733455, 0.028310475893667274, 0.3763337007525611, 0.16542140692420268, 0.372198165579456, 0.033084281384840535, 0.02894874621173547, 0.008271070346210134, 0.008271070346210134, 0.008271070346210134, 0.058833441559299884, 0.058833441559299884, 0.7648347402708985, 0.058833441559299884, 0.32436409002465477, 0.17001152304740524, 0.13198262973416985, 0.20132943518771673, 0.06710981172923891, 0.08500576152370262, 0.004473987448615928, 0.015658956070155747, 0.249591444124576, 0.09224031630690853, 0.37303069094705654, 0.21161013740996662, 0.029842455275764526, 0.02441655431653461, 0.013564752398074784, 0.0040694257194224355, 0.16280134918965264, 0.08140067459482632, 0.08140067459482632, 0.08140067459482632, 0.5698047221637842, 0.16709405296916935, 0.030380736903485334, 0.205069974098526, 0.41773513242292337, 0.015190368451742667, 0.10633257916219867, 0.053166289581099334, 0.007595184225871333, 0.09839173778860169, 0.09839173778860169, 0.09839173778860169, 0.09839173778860169, 0.4919586889430085, 0.09839173778860169, 0.01440269763439253, 0.02880539526878506, 0.9073699509667295, 0.01440269763439253, 0.02880539526878506, 0.21540520966516533, 0.12728489662032497, 0.10770260483258266, 0.41122812754258836, 0.009791145893871152, 0.058746875363226904, 0.04895572946935575, 0.019582291787742304, 0.021552387873589696, 0.7543335755756394, 0.08620955149435879, 0.10776193936794848, 0.021552387873589696, 0.13170588202631664, 0.13170588202631664, 0.13170588202631664, 0.13170588202631664, 0.3951176460789499, 0.05007960618088478, 0.05007960618088478, 0.8012736988941564, 0.05007960618088478, 0.008654455360114595, 0.07789009824103134, 0.787555437770428, 0.04327227680057297, 0.03461782144045838, 0.04327227680057297, 0.008654455360114595, 0.12395539854586102, 0.12395539854586102, 0.12395539854586102, 0.12395539854586102, 0.49582159418344407, 0.22686702717108148, 0.14553733818522208, 0.13269580834534955, 0.2739526365839475, 0.13269580834534955, 0.05136611935949015, 0.029963569626369254, 0.008561019893248359, 0.06410319054410268, 0.06410319054410268, 0.06410319054410268, 0.06410319054410268, 0.7051350959851295, 0.04059688925482112, 0.8525346743512435, 0.04059688925482112, 0.04059688925482112, 0.1235598829471809, 0.1235598829471809, 0.017651411849597273, 0.5471937673375155, 0.017651411849597273, 0.1235598829471809, 0.017651411849597273, 0.017651411849597273, 0.0482108673116051, 0.0482108673116051, 0.7713738769856816, 0.0964217346232102, 0.5288013395297892, 0.02832864318909585, 0.08026448903577157, 0.16052897807154315, 0.004721440531515976, 0.16052897807154315, 0.004721440531515976, 0.02832864318909585, 0.09885350471503201, 0.19770700943006403, 0.04942675235751601, 0.14828025707254802, 0.4448407712176441, 0.43539316571873604, 0.03002711487715421, 0.07506778719288552, 0.06005422975430842, 0.015013557438577105, 0.09008134463146263, 0.285257591332965, 0.8562907874532397, 0.05628202358847585, 0.0040201445420339895, 0.012060433626101968, 0.020100722710169945, 0.04020144542033989, 0.008040289084067979, 0.36345455947528915, 0.18293879493589554, 0.1441703085918647, 0.15507394537612337, 0.037556971145779876, 0.06299879030905012, 0.03876848634403084, 0.01574969757726253, 0.838414020046871, 0.06327652981485819, 0.06327652981485819, 0.015819132453714548, 0.03846326415703245, 0.0769265283140649, 0.769265283140649, 0.03846326415703245, 0.029538863962512697, 0.029538863962512697, 0.8861659188753809, 0.029538863962512697, 0.029538863962512697, 0.8712366315025132, 0.05620881493564601, 0.028104407467823004, 0.014052203733911502, 0.014052203733911502, 0.014052203733911502, 0.21607359550902525, 0.1815018202275812, 0.1555729887664982, 0.01728588764072202, 0.04321471910180505, 0.04321471910180505, 0.025928831461083032, 0.3111459775329964, 0.06425875933586818, 0.06425875933586818, 0.06425875933586818, 0.06425875933586818, 0.7068463526945501, 0.22606177686966578, 0.21109079826902566, 0.1661778624671053, 0.33684701851440263, 0.011976782880512094, 0.011976782880512094, 0.03293615292140826, 0.0029941957201280235, 0.045188541214105815, 0.045188541214105815, 0.045188541214105815, 0.8133937418539046, 0.14502061510713954, 0.14502061510713954, 0.14502061510713954, 0.14502061510713954, 0.4350618453214186, 0.06783598459525954, 0.1187129730417042, 0.5511673748364837, 0.22894644800900094, 0.008479498074407442, 0.008479498074407442, 0.025438494223222325, 0.3877085110357023, 0.15775725621452713, 0.14973570081378848, 0.18182192241674314, 0.05615088780517068, 0.026738518002462228, 0.03476007340320089, 0.008021555400738668, 0.13170505785107306, 0.13170505785107306, 0.13170505785107306, 0.13170505785107306, 0.3951151735532192, 0.3126642753917477, 0.265290900332392, 0.028424025035613427, 0.14212012517806713, 0.03789870004748457, 0.08527207510684029, 0.028424025035613427, 0.09474675011871142, 0.14545858714143095, 0.14545858714143095, 0.07272929357071548, 0.07272929357071548, 0.14545858714143095, 0.36364646785357735, 0.055429137382707036, 0.055429137382707036, 0.055429137382707036, 0.055429137382707036, 0.7760079233578985, 0.06858641815181399, 0.8230370178217679, 0.05143981361386049, 0.017146604537953497, 0.017146604537953497, 0.3513205547639038, 0.04391506934548797, 0.08783013869097595, 0.04391506934548797, 0.08783013869097595, 0.3513205547639038, 0.42572171808953174, 0.23248632831839675, 0.042270241512435774, 0.1298300275024813, 0.09963699785074147, 0.051328150407957726, 0.009057908895521952, 0.009057908895521952, 0.2719780250631714, 0.20019200357809644, 0.23355733750777918, 0.18401487197582603, 0.028309980303973234, 0.01718820232741232, 0.03740961683025035, 0.027298909578831333, 0.8155605117093755, 0.04530891731718752, 0.04530891731718752, 0.04530891731718752, 0.06062557506111056, 0.06062557506111056, 0.06062557506111056, 0.7881324757944373, 0.1446358537424539, 0.2812363822769937, 0.5383667889302451, 0.008035325207914105, 0.008035325207914105, 0.01607065041582821, 0.008035325207914105, 0.04668486493977652, 0.04668486493977652, 0.8403275689159774, 0.04668486493977652, 0.042312756195444785, 0.042312756195444785, 0.8462551239088957, 0.042312756195444785, 0.40077884010335124, 0.21335579429031346, 0.21335579429031346, 0.07897700672624862, 0.03182655494938377, 0.02593274847727567, 0.023575225888432427, 0.010608851649794592, 0.3829712655344563, 0.2553141770229709, 0.014184120945720604, 0.13474914898434573, 0.028368241891441208, 0.07801266520146331, 0.014184120945720604, 0.08510472567432362, 0.08287465109464956, 0.08287465109464956, 0.08287465109464956, 0.08287465109464956, 0.6629972087571965, 0.0552939456972008, 0.0552939456972008, 0.7741152397608112, 0.0552939456972008, 0.0552939456972008, 0.16008429848560646, 0.11005795520885445, 0.6503424625977763, 0.040021074621401614, 0.010005268655350404, 0.010005268655350404, 0.020010537310700807, 0.05884579818110758, 0.05884579818110758, 0.7649953763543985, 0.05884579818110758, 0.3455121765981068, 0.38006339425791746, 0.08445853205731499, 0.07294145950404476, 0.06142438695077454, 0.03455121765981068, 0.0038390241844234087, 0.015356096737693635, 0.22660940942455882, 0.2185162162308246, 0.2954015515712999, 0.15377067068095063, 0.02023298298433561, 0.01618638638746849, 0.028326176178069852, 0.04046596596867122, 0.10011426437891403, 0.05005713218945702, 0.05005713218945702, 0.05005713218945702, 0.7508569828418552, 0.060288002899411205, 0.060288002899411205, 0.060288002899411205, 0.060288002899411205, 0.7234560347929345, 0.24926926718054682, 0.04154487786342447, 0.04154487786342447, 0.1661795114536979, 0.45699365649766915, 0.14340771484936873, 0.05377789306851327, 0.01792596435617109, 0.2509635009863953, 0.3047413940549086, 0.08962982178085546, 0.08962982178085546, 0.03585192871234218, 0.08648240919043161, 0.8475276100662299, 0.017296481838086325, 0.017296481838086325, 0.017296481838086325, 0.07770647117339075, 0.07770647117339075, 0.07770647117339075, 0.1554129423467815, 0.621651769387126, 0.050168172138156206, 0.050168172138156206, 0.8026907542104993, 0.050168172138156206, 0.07003349921018541, 0.8228936157196787, 0.017508374802546352, 0.017508374802546352, 0.017508374802546352, 0.017508374802546352, 0.035016749605092705, 0.04351867590175237, 0.826854842133295, 0.04351867590175237, 0.04351867590175237, 0.9252312860209589, 0.024672834293892238, 0.024672834293892238, 0.012336417146946119, 0.8480175549564538, 0.0413667099978758, 0.0413667099978758, 0.0206833549989379, 0.07514196263730015, 0.07514196263730015, 0.07514196263730015, 0.6762776637357013, 0.039876434054243605, 0.039876434054243605, 0.7975286810848721, 0.039876434054243605, 0.039876434054243605, 0.018567208623457114, 0.018567208623457114, 0.9283604311728557, 0.018567208623457114, 0.4135423318003688, 0.29338097501309185, 0.14356941330427897, 0.07178470665213948, 0.05305826143853788, 0.014044833910201204, 0.0031210742022669343, 0.0062421484045338685, 0.5111430409589401, 0.16155298185748887, 0.11917842923913113, 0.06621023846618396, 0.0635618289275366, 0.021187276309178868, 0.037077733541063015, 0.018538866770531508, 0.10185332877556735, 0.10185332877556735, 0.10185332877556735, 0.10185332877556735, 0.6111199726534041, 0.784094690167676, 0.04657988258421838, 0.007763313764036397, 0.007763313764036397, 0.007763313764036397, 0.13973964775265513, 0.8133725066068432, 0.04603995320416094, 0.06138660427221459, 0.030693302136107294, 0.015346651068053647, 0.015346651068053647, 0.015346651068053647, 0.015346651068053647, 0.9217059935530503, 0.023042649838826257, 0.023042649838826257, 0.023042649838826257, 0.8843703933611872, 0.028528077205199585, 0.028528077205199585, 0.028528077205199585, 0.04341836919164699, 0.7815306454496458, 0.04341836919164699, 0.04341836919164699, 0.04341836919164699, 0.04341836919164699, 0.8439352439317043, 0.0401873925681764, 0.0401873925681764, 0.0401873925681764, 0.061230759427721514, 0.030615379713860757, 0.2143076579970253, 0.09184613914158227, 0.24492303771088605, 0.3061537971386076, 0.12602102506171642, 0.12602102506171642, 0.12602102506171642, 0.12602102506171642, 0.5040841002468657, 0.07065410617617984, 0.07065410617617984, 0.07065410617617984, 0.07065410617617984, 0.7065410617617983, 0.05052273261922631, 0.808363721907621, 0.05052273261922631, 0.05052273261922631, 0.8725353965538397, 0.030087427467373782, 0.030087427467373782, 0.030087427467373782, 0.07697620990140579, 0.5965656267358949, 0.019244052475351447, 0.019244052475351447, 0.019244052475351447, 0.2501726821795688, 0.22444610533105047, 0.149630736887367, 0.0748153684436835, 0.0748153684436835, 0.44889221066210094, 0.03694198824133856, 0.8496657295507869, 0.03694198824133856, 0.03694198824133856, 0.14745925270511645, 0.7667881140666055, 0.02949185054102329, 0.02949185054102329, 0.02949185054102329, 0.41330934727320895, 0.21711820141567306, 0.1831117361337002, 0.03923822917150718, 0.07847645834301437, 0.03400646528197289, 0.007847645834301436, 0.026158819447671455, 0.25087950318499447, 0.26017133663629055, 0.12079383486684919, 0.29269275371582687, 0.027875500353888277, 0.004645916725648046, 0.03252141707953632, 0.009291833451296092, 0.04202828844588431, 0.04202828844588431, 0.04202828844588431, 0.04202828844588431, 0.7985374804718018, 0.04202828844588431, 0.26581958338642053, 0.22151631948868378, 0.08860652779547351, 0.044303263897736755, 0.3987293750796308, 0.5596933560543632, 0.2052208972199332, 0.1274857088790494, 0.034203482869988866, 0.012437630134541406, 0.006218815067270703, 0.031094075336353513, 0.02798466780271816, 0.08965082342418587, 0.0672381175681394, 0.19274927036199963, 0.4706668229769758, 0.022412705856046468, 0.08068574108176728, 0.0717206587393487, 0.004482541171209294, 0.488537402231876, 0.22976873255654442, 0.08253828256885576, 0.1260381882470365, 0.03457684810316931, 0.022307643937528585, 0.005576910984382146, 0.008923057575011434, 0.07562096116529862, 0.07562096116529862, 0.07562096116529862, 0.07562096116529862, 0.604967689322389, 0.051998706395042844, 0.8839780087157283, 0.008666451065840474, 0.034665804263361896, 0.008666451065840474, 0.008666451065840474, 0.008666451065840474, 0.5830495969585148, 0.04091576119007121, 0.04091576119007121, 0.010228940297517803, 0.020457880595035605, 0.22503668654539166, 0.07160258208262463, 0.07580630408222858, 0.07580630408222858, 0.07580630408222858, 0.07580630408222858, 0.6822567367400572, 0.078549459823055, 0.078549459823055, 0.078549459823055, 0.078549459823055, 0.706945138407495, 0.3754492815851174, 0.4441290282165413, 0.09157299550856522, 0.05952244708056739, 0.004578649775428261, 0.009157299550856523, 0.004578649775428261, 0.004578649775428261, 0.3843203710441906, 0.35040975006970315, 0.16503168874250537, 0.040692745169384884, 0.0022607080649658267, 0.018085664519726614, 0.027128496779589922, 0.013564248389794961, 0.6615408336288179, 0.08514882017004587, 0.20304718655934015, 0.006549909243849682, 0.006549909243849682, 0.02619963697539873, 0.013099818487699364, 0.05684087591531583, 0.05684087591531583, 0.05684087591531583, 0.05684087591531583, 0.7389313868991058, 0.14093400070336096, 0.21140100105504142, 0.07046700035168048, 0.07046700035168048, 0.42280200211008284, 0.14405406367190285, 0.07202703183595142, 0.07202703183595142, 0.07202703183595142, 0.5762162546876114, 0.07202703183595142, 0.6821482285947148, 0.04973997500169795, 0.00710571071452828, 0.01421142142905656, 0.10658566071792419, 0.04973997500169795, 0.042634264287169675, 0.042634264287169675, 0.13635279832667402, 0.13635279832667402, 0.13635279832667402, 0.13635279832667402, 0.409058394980022, 0.4548505656631361, 0.06497865223759088, 0.12405015427176438, 0.16540020569568586, 0.02362860081366941, 0.10042155345809499, 0.04725720162733882, 0.02362860081366941, 0.07853678379432097, 0.07853678379432097, 0.07853678379432097, 0.07853678379432097, 0.7068310541488887, 0.05497819123939768, 0.7696946773515675, 0.05497819123939768, 0.05497819123939768, 0.10936073152891229, 0.10936073152891229, 0.10936073152891229, 0.10936073152891229, 0.5468036576445614, 0.05533689935707923, 0.05533689935707923, 0.05533689935707923, 0.7747165909991092, 0.521780125844999, 0.1559961200979894, 0.19902953253881406, 0.03765423588572158, 0.010758353110206165, 0.01613752966530925, 0.005379176555103083, 0.05379176555103083, 0.35655989981492103, 0.2707964390639836, 0.21001918341371295, 0.10399663744601864, 0.018908479535639754, 0.010804845448937, 0.016882571013964065, 0.011480148289495564, 0.4680080482687873, 0.16036639416203202, 0.13745690928174173, 0.13582051750457813, 0.027818660211781063, 0.024545876657453877, 0.01636391777163592, 0.029455051988944655, 0.11968105084438087, 0.039893683614793625, 0.47872420337752347, 0.039893683614793625, 0.2792557853035554, 0.42616903035432235, 0.2865204215376518, 0.11557126246896877, 0.1131635278341986, 0.028892815617242194, 0.019261877078161464, 0.004815469269540366, 0.004815469269540366, 0.6950514457702268, 0.14718736498663626, 0.05723953082813633, 0.03270830333036361, 0.04906245499554542, 0.008177075832590903, 0.008177075832590903, 0.11602432544030775, 0.2320486508806155, 0.11602432544030775, 0.11602432544030775, 0.11602432544030775, 0.34807297632092327, 0.10169795475203915, 0.10169795475203915, 0.10169795475203915, 0.10169795475203915, 0.10169795475203915, 0.5084897737601957, 0.4008366499138956, 0.24732474143623348, 0.09949845919848473, 0.1279265903980518, 0.00852843935987012, 0.0426421967993506, 0.06254188863904754, 0.00852843935987012, 0.1146211429506485, 0.0229242285901297, 0.0687726857703891, 0.5731057147532425, 0.1833938287210376, 0.0229242285901297, 0.0229242285901297, 0.1071680015810824, 0.1071680015810824, 0.1071680015810824, 0.1071680015810824, 0.535840007905412, 0.04308605014916523, 0.04308605014916523, 0.17234420059666092, 0.1292581504474957, 0.560118651939148, 0.04308605014916523, 0.04308605014916523, 0.8809292886034057, 0.0338818957155156, 0.0338818957155156, 0.0338818957155156, 0.17802780594996956, 0.7714538257832014, 0.023737040793329275, 0.011868520396664637, 0.011868520396664637, 0.011868520396664637, 0.9158530071389266, 0.014771822695789139, 0.029543645391578277, 0.014771822695789139, 0.014771822695789139, 0.014771822695789139, 0.8815577387106137, 0.033906066873485144, 0.033906066873485144, 0.033906066873485144, 0.1853689454089592, 0.2612016958035334, 0.4128671965926819, 0.08425861154952691, 0.016851722309905382, 0.008425861154952691, 0.025277583464858075, 0.008425861154952691, 0.02978393544477534, 0.1489196772238767, 0.7445983861193834, 0.02978393544477534, 0.30363588840202155, 0.2757509598753053, 0.16111292037658287, 0.04337655548600308, 0.04027823009414572, 0.05267153166157517, 0.03717990470228835, 0.08675311097200616, 0.06904368110327873, 0.06904368110327873, 0.06904368110327873, 0.06904368110327873, 0.6904368110327872, 0.31741103212107613, 0.5486116604561809, 0.09796636793860375, 0.0078373094350883, 0.0078373094350883, 0.00391865471754415, 0.0078373094350883, 0.0078373094350883, 0.14483951304400983, 0.14483951304400983, 0.14483951304400983, 0.14483951304400983, 0.4345185391320294], \"Term\": [\"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acquisition_acquisition\", \"acquisition_acquisition\", \"acquisition_acquisition\", \"acquisition_acquisition\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"active_vertice\", \"active_vertice\", \"active_vertice\", \"active_vertice\", \"active_vertice\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptation\", \"adapting_adapte\", \"adapting_adapte\", \"adapting_adapte\", \"adapting_adapte\", \"affine_transformation\", \"affine_transformation\", \"affine_transformation\", \"affine_transformation\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"alphabet\", \"alphabet\", \"alphabet\", \"alphabet\", \"alphabet\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"apg\", \"apg\", \"apg\", \"apg\", \"apg\", \"apgs\", \"apgs\", \"apgs\", \"apgs\", \"apgs\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"appxpca\", \"appxpca\", \"appxpca\", \"appxpca\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arrival\", \"arrival\", \"arrival\", \"arrival\", \"arrival\", \"arrival\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"attention\", \"attention\", \"attention\", \"attention\", \"attention\", \"attention\", \"auxiliary_variable\", \"auxiliary_variable\", \"auxiliary_variable\", \"auxiliary_variable\", \"back_constraine\", \"back_constraine\", \"back_constraine\", \"back_constraine\", \"back_constraine\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"beta\", \"beta\", \"beta\", \"beta\", \"beta\", \"bilingual\", \"bilingual\", \"bilingual\", \"bilingual\", \"bind\", \"bind\", \"bind\", \"bind\", \"bind\", \"bind\", \"bind\", \"bind\", \"block_krylov\", \"block_krylov\", \"block_krylov\", \"block_krylov\", \"bopp_bopp\", \"bopp_bopp\", \"bopp_bopp\", \"bopp_bopp\", \"border_pattern\", \"border_pattern\", \"border_pattern\", \"border_pattern\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"cancer\", \"cancer\", \"cancer\", \"cancer\", \"cancer\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"category\", \"cc\", \"cc\", \"cc\", \"cc\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cervical\", \"cervical\", \"cervical\", \"cervical\", \"cervical\", \"character\", \"character\", \"character\", \"character\", \"character\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classifier_chain\", \"classifier_chain\", \"classifier_chain\", \"classifier_chain\", \"click\", \"click\", \"click\", \"click\", \"click\", \"climbing_fiber\", \"climbing_fiber\", \"climbing_fiber\", \"climbing_fiber\", \"climbing_fiber\", \"clique\", \"clique\", \"clique\", \"clique\", \"clsr\", \"clsr\", \"clsr\", \"clsr\", \"clsr\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cma_es\", \"cma_es\", \"cma_es\", \"cma_es\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"collicular\", \"collicular\", \"collicular\", \"collicular\", \"collicular\", \"colposcopy\", \"colposcopy\", \"colposcopy\", \"colposcopy\", \"colposcopy\", \"columnar\", \"columnar\", \"columnar\", \"columnar\", \"columnar\", \"community_detection\", \"community_detection\", \"community_detection\", \"community_detection\", \"compress\", \"compress\", \"compress\", \"compress\", \"compress\", \"compress\", \"compressed_domain\", \"compressed_domain\", \"compressed_domain\", \"compressed_domain\", \"compressed_domain\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"cone\", \"cone\", \"cone\", \"cone\", \"cone\", \"cone\", \"confidence_interval\", \"confidence_interval\", \"confidence_interval\", \"confidence_interval\", \"confidence_interval\", \"confidence_interval\", \"connie_netinf\", \"connie_netinf\", \"connie_netinf\", \"connie_netinf\", \"connie_netinf\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"constraints_constraint\", \"constraints_constraint\", \"constraints_constraint\", \"constraints_constraint\", \"contagion\", \"contagion\", \"contagion\", \"contagion\", \"contagion\", \"contingency_table\", \"contingency_table\", \"contingency_table\", \"contingency_table\", \"contour\", \"contour\", \"contour\", \"contour\", \"corrective_movement\", \"corrective_movement\", \"corrective_movement\", \"corrective_movement\", \"corrective_movement\", \"corrlda\", \"corrlda\", \"corrlda\", \"corrlda\", \"countable\", \"countable\", \"countable\", \"countable\", \"countable\", \"countable\", \"countable\", \"covariate_shift\", \"covariate_shift\", \"covariate_shift\", \"covariate_shift\", \"covariate_shift\", \"crit\", \"crit\", \"crit\", \"crit\", \"crit\", \"dantzig_selector\", \"dantzig_selector\", \"dantzig_selector\", \"dantzig_selector\", \"dantzig_selector\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"decomposable\", \"decomposable\", \"decomposable\", \"decomposable\", \"decomposable\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"def\", \"def\", \"def\", \"def\", \"def\", \"def\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"delay_conditione\", \"delay_conditione\", \"delay_conditione\", \"delay_conditione\", \"delay_conditione\", \"delivery\", \"delivery\", \"delivery\", \"delivery\", \"delivery\", \"demonstrate_demonstrate\", \"demonstrate_demonstrate\", \"demonstrate_demonstrate\", \"demonstrate_demonstrate\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detector\", \"detector\", \"detector\", \"detector\", \"diffeomap\", \"diffeomap\", \"diffeomap\", \"diffeomap\", \"diffeomap\", \"diffeomorphic\", \"diffeomorphic\", \"diffeomorphic\", \"diffeomorphic\", \"diffeomorphic\", \"diffeomorphism\", \"diffeomorphism\", \"diffeomorphism\", \"diffeomorphism\", \"diffeomorphism\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"differential_equation\", \"differential_equation\", \"differential_equation\", \"differential_equation\", \"diffusion\", \"diffusion\", \"diffusion\", \"diffusion\", \"diffusion\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensionality_reduction\", \"dimensionality_reduction\", \"dimensionality_reduction\", \"dimensionality_reduction\", \"dimensionality_reduction\", \"dirl\", \"dirl\", \"dirl\", \"dirl\", \"dirl\", \"discard\", \"discard\", \"discard\", \"discard\", \"discard\", \"discard\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"draw\", \"draw\", \"draw\", \"draw\", \"draw\", \"draw\", \"draw\", \"draw\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge_exchangeability\", \"edge_exchangeability\", \"edge_exchangeability\", \"edge_exchangeability\", \"edge_exchangeability\", \"edge_exchangeable\", \"edge_exchangeable\", \"edge_exchangeable\", \"edge_exchangeable\", \"edge_exchangeable\", \"edge_transmission\", \"edge_transmission\", \"edge_transmission\", \"edge_transmission\", \"edge_transmission\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"elastic_net\", \"elastic_net\", \"elastic_net\", \"elastic_net\", \"elo\", \"elo\", \"elo\", \"elo\", \"elo\", \"empirical_marginal\", \"empirical_marginal\", \"empirical_marginal\", \"empirical_marginal\", \"empirical_marginal\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"engine\", \"engine\", \"engine\", \"engine\", \"engine\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"entry\", \"erroneous\", \"erroneous\", \"erroneous\", \"erroneous\", \"erroneous\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event_coreference\", \"event_coreference\", \"event_coreference\", \"event_coreference\", \"event_mention\", \"event_mention\", \"event_mention\", \"event_mention\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"excess\", \"excess\", \"excess\", \"excess\", \"excess\", \"excess\", \"excess_risk\", \"excess_risk\", \"excess_risk\", \"excess_risk\", \"excess_risk\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_movement\", \"eye_position\", \"eye_position\", \"eye_position\", \"eye_position\", \"eye_position\", \"fault\", \"fault\", \"fault\", \"fault\", \"fb\", \"fb\", \"fb\", \"fb\", \"fb\", \"fb\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"fiber\", \"fiber\", \"fiber\", \"fiber\", \"fiber\", \"fiber\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fitc\", \"fitc\", \"fitc\", \"fitc\", \"flight\", \"flight\", \"flight\", \"flight\", \"fly\", \"fly\", \"fly\", \"fly\", \"fly\", \"fly\", \"fn\", \"fn\", \"fn\", \"fn\", \"fn\", \"fna\", \"fna\", \"fna\", \"fna\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"forward_stepwise\", \"forward_stepwise\", \"forward_stepwise\", \"forward_stepwise\", \"forward_stepwise\", \"foveation\", \"foveation\", \"foveation\", \"foveation\", \"foveation\", \"full_gp\", \"full_gp\", \"full_gp\", \"full_gp\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"fused_sparsity\", \"fused_sparsity\", \"fused_sparsity\", \"fused_sparsity\", \"galileo\", \"galileo\", \"galileo\", \"galileo\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gap\", \"gaussian_processe\", \"gaussian_processe\", \"gaussian_processe\", \"gaussian_processe\", \"gcc\", \"gcc\", \"gcc\", \"gcc\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"gk_xk\", \"gk_xk\", \"gk_xk\", \"gk_xk\", \"globally_optimal\", \"globally_optimal\", \"globally_optimal\", \"globally_optimal\", \"globally_optimal\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph_frequency\", \"graph_frequency\", \"graph_frequency\", \"graph_frequency\", \"graph_frequency\", \"grid\", \"grid\", \"grid\", \"grid\", \"grid\", \"grid\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group_sparse\", \"group_sparse\", \"group_sparse\", \"group_sparse\", \"group_sparse\", \"group_sparse\", \"gsm\", \"gsm\", \"gsm\", \"gsm\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"gumbel\", \"gumbel\", \"gumbel\", \"gumbel\", \"gumbel_process\", \"gumbel_process\", \"gumbel_process\", \"gumbel_process\", \"hash\", \"hash\", \"hash\", \"hash\", \"hash\", \"hdirl\", \"hdirl\", \"hdirl\", \"hdirl\", \"hdirl\", \"heap\", \"heap\", \"heap\", \"heap\", \"hillcar\", \"hillcar\", \"hillcar\", \"hillcar\", \"hillcar\", \"hippocampal\", \"hippocampal\", \"hippocampal\", \"hippocampal\", \"hippocampal\", \"hippocampal_damage\", \"hippocampal_damage\", \"hippocampal_damage\", \"hippocampal_damage\", \"hippocampal_damage\", \"houk\", \"houk\", \"houk\", \"houk\", \"houk\", \"hyper\", \"hyper\", \"hyper\", \"hyper\", \"hyper\", \"iht\", \"iht\", \"iht\", \"iht\", \"iht\", \"iid\", \"iid\", \"iid\", \"iid\", \"iid\", \"iid\", \"iid\", \"iid\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"inducing_input\", \"inducing_input\", \"inducing_input\", \"inducing_input\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"infect\", \"infect\", \"infect\", \"infect\", \"infect\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infer\", \"infer\", \"infer\", \"infer\", \"infer\", \"infer\", \"infer\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"interpolation\", \"interpolation\", \"interpolation\", \"interpolation\", \"interpolation\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"item\", \"item\", \"item\", \"item\", \"item\", \"iterative_hard\", \"iterative_hard\", \"iterative_hard\", \"iterative_hard\", \"iterative_hard\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"knnz\", \"knnz\", \"knnz\", \"knnz\", \"kortum\", \"kortum\", \"kortum\", \"kortum\", \"kortum\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label_request\", \"label_request\", \"label_request\", \"label_request\", \"label_request\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"lazysvd\", \"lazysvd\", \"lazysvd\", \"lazysvd\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"locality_preserve\", \"locality_preserve\", \"locality_preserve\", \"locality_preserve\", \"locality_preserve\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"long_latency\", \"long_latency\", \"long_latency\", \"long_latency\", \"long_latency\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lpp\", \"lpp\", \"lpp\", \"lpp\", \"lpp\", \"lpp\", \"lsbm\", \"lsbm\", \"lsbm\", \"lsbm\", \"lsda\", \"lsda\", \"lsda\", \"lsda\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mapping\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum\", \"maximum_margin\", \"maximum_margin\", \"maximum_margin\", \"maximum_margin\", \"mcv\", \"mcv\", \"mcv\", \"mcv\", \"mdlx\", \"mdlx\", \"mdlx\", \"mdlx\", \"mdlx\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"median\", \"median\", \"median\", \"median\", \"median\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"mental\", \"mental\", \"mental\", \"mental\", \"mental\", \"mesh\", \"mesh\", \"mesh\", \"mesh\", \"mesh\", \"mesh\", \"metabolic_constraint\", \"metabolic_constraint\", \"metabolic_constraint\", \"metabolic_constraint\", \"metabolic_constraint\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"microstimuli\", \"microstimuli\", \"microstimuli\", \"microstimuli\", \"microstimuli\", \"misclassified_item\", \"misclassified_item\", \"misclassified_item\", \"misclassified_item\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mistake\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mle_estimator\", \"mle_estimator\", \"mle_estimator\", \"mle_estimator\", \"mle_estimator\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modes_mode\", \"modes_mode\", \"modes_mode\", \"modes_mode\", \"monotonic_feature\", \"monotonic_feature\", \"monotonic_feature\", \"monotonic_feature\", \"motoneuron\", \"motoneuron\", \"motoneuron\", \"motoneuron\", \"motoneuron\", \"motoneuron\", \"move\", \"move\", \"move\", \"move\", \"move\", \"move\", \"move\", \"move\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movielen\", \"movielen\", \"movielen\", \"movielen\", \"movielen\", \"mpe\", \"mpe\", \"mpe\", \"mpe\", \"mpe\", \"msa\", \"msa\", \"msa\", \"msa\", \"msa\", \"multi_label\", \"multi_label\", \"multi_label\", \"multi_label\", \"multilinear_interpolation\", \"multilinear_interpolation\", \"multilinear_interpolation\", \"multilinear_interpolation\", \"multilinear_interpolation\", \"multilingual_topic\", \"multilingual_topic\", \"multilingual_topic\", \"multilingual_topic\", \"mutual_information\", \"mutual_information\", \"mutual_information\", \"mutual_information\", \"mutual_information\", \"nd_order\", \"nd_order\", \"nd_order\", \"nd_order\", \"nd_order\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"node_wise\", \"node_wise\", \"node_wise\", \"node_wise\", \"node_wise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"nonparanormal\", \"nonparanormal\", \"nonparanormal\", \"nonparanormal\", \"nonparanormal\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm_regularize\", \"norm_regularize\", \"norm_regularize\", \"norm_regularize\", \"norm_regularize\", \"null_hypothesis\", \"null_hypothesis\", \"null_hypothesis\", \"null_hypothesis\", \"null_hypothesis\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"oblivious\", \"oblivious\", \"oblivious\", \"oblivious\", \"oblivious\", \"oblivious_equilibrium\", \"oblivious_equilibrium\", \"oblivious_equilibrium\", \"oblivious_equilibrium\", \"oblivious_equilibrium\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"obtain\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opponent\", \"opponent\", \"opponent\", \"opponent\", \"opponent\", \"optimise\", \"optimise\", \"optimise\", \"optimise\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"pap_smear\", \"pap_smear\", \"pap_smear\", \"pap_smear\", \"pap_smear\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"particle\", \"particle\", \"particle\", \"particle\", \"pbm\", \"pbm\", \"pbm\", \"pbm\", \"pbm\", \"pc\", \"pc\", \"pc\", \"pc\", \"pc\", \"pc\", \"pc\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"physical\", \"physical\", \"physical\", \"physical\", \"physical\", \"physics_engine\", \"physics_engine\", \"physics_engine\", \"physics_engine\", \"pile\", \"pile\", \"pile\", \"pile\", \"pile\", \"pivot_language\", \"pivot_language\", \"pivot_language\", \"pivot_language\", \"planner\", \"planner\", \"planner\", \"planner\", \"player\", \"player\", \"player\", \"player\", \"player\", \"poedge\", \"poedge\", \"poedge\", \"poedge\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"pollack\", \"pollack\", \"pollack\", \"pollack\", \"pollack\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"potentially_potentially\", \"potentially_potentially\", \"potentially_potentially\", \"potentially_potentially\", \"pre_cancerous\", \"pre_cancerous\", \"pre_cancerous\", \"pre_cancerous\", \"pre_cancerous\", \"precise\", \"precise\", \"precise\", \"precise\", \"precise\", \"precise\", \"precise\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"predictive\", \"predictive\", \"predictive\", \"predictive\", \"predictive\", \"predictive\", \"predictive\", \"preference\", \"preference\", \"preference\", \"preference\", \"preference\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probabilistic_planne\", \"probabilistic_planne\", \"probabilistic_planne\", \"probabilistic_planne\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"proprioceptive\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove\", \"prune\", \"prune\", \"prune\", \"prune\", \"prune\", \"pxs\", \"pxs\", \"pxs\", \"pxs\", \"pxs\", \"qbrank\", \"qbrank\", \"qbrank\", \"qbrank\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"ramanujam\", \"ramanujam\", \"ramanujam\", \"ramanujam\", \"ramanujam\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rcpn\", \"rcpn\", \"rcpn\", \"rcpn\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"region_region\", \"region_region\", \"region_region\", \"region_region\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retain\", \"retain\", \"retain\", \"retain\", \"rigid\", \"rigid\", \"rigid\", \"rigid\", \"rigid_structure\", \"rigid_structure\", \"rigid_structure\", \"rigid_structure\", \"rigid_structure\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rnn\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rulenet\", \"rulenet\", \"rulenet\", \"rulenet\", \"rulenet\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sbm\", \"sbm\", \"sbm\", \"sbm\", \"scp_rule\", \"scp_rule\", \"scp_rule\", \"scp_rule\", \"scp_rule\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"selected_group\", \"selected_group\", \"selected_group\", \"selected_group\", \"selected_group\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selective\", \"selective\", \"selective\", \"selective\", \"selective\", \"selective\", \"selective_sample\", \"selective_sample\", \"selective_sample\", \"selective_sample\", \"selective_sample\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"sensitivity\", \"sensitivity\", \"sensitivity\", \"sensitivity\", \"sensitivity\", \"sensitivity\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sfa\", \"sfa\", \"sfa\", \"sfa\", \"sfds\", \"sfds\", \"sfds\", \"sfds\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"short_sighte\", \"short_sighte\", \"short_sighte\", \"short_sighte\", \"short_sighted\", \"short_sighted\", \"short_sighted\", \"short_sighted\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"sil\", \"sil\", \"sil\", \"sil\", \"sil\", \"sim\", \"sim\", \"sim\", \"sim\", \"sim\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity_search\", \"similarity_search\", \"similarity_search\", \"similarity_search\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"slot\", \"slot\", \"slot\", \"slot\", \"slot\", \"social\", \"social\", \"social\", \"social\", \"social\", \"sparsity\", \"sparsity\", \"sparsity\", \"sparsity\", \"sparsity\", \"sparsity\", \"sparsity\", \"sparsity\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"specificity\", \"specificity\", \"specificity\", \"specificity\", \"specificity\", \"spectral_clustere\", \"spectral_clustere\", \"spectral_clustere\", \"spectral_clustere\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"spelling\", \"spelling\", \"spelling\", \"spelling\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike_train\", \"spike_train\", \"spike_train\", \"spike_train\", \"splice\", \"splice\", \"splice\", \"splice\", \"sql\", \"sql\", \"sql\", \"sql\", \"sql\", \"ssp\", \"ssp\", \"ssp\", \"ssp\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step_augmente\", \"step_augmente\", \"step_augmente\", \"step_augmente\", \"step_augmente\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop_signal\", \"stop_signal\", \"stop_signal\", \"stop_signal\", \"stop_trial\", \"stop_trial\", \"stop_trial\", \"stop_trial\", \"storage\", \"storage\", \"storage\", \"storage\", \"storage\", \"storage\", \"strf\", \"strf\", \"strf\", \"strf\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"subsymbolic\", \"subsymbolic\", \"subsymbolic\", \"subsymbolic\", \"subsymbolic\", \"suffix_tree\", \"suffix_tree\", \"suffix_tree\", \"suffix_tree\", \"suffix_tree\", \"super_pixel\", \"super_pixel\", \"super_pixel\", \"super_pixel\", \"svg\", \"svg\", \"svg\", \"svg\", \"symbol\", \"symbol\", \"symbol\", \"symbol\", \"symbol\", \"symbol\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symbolic\", \"symcorrlda\", \"symcorrlda\", \"symcorrlda\", \"symcorrlda\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"template\", \"template\", \"template\", \"template\", \"template\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tissue\", \"tissue\", \"tissue\", \"tissue\", \"tissue\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace\", \"trace_conditione\", \"trace_conditione\", \"trace_conditione\", \"trace_conditione\", \"trace_conditione\", \"trace_conditioning\", \"trace_conditioning\", \"trace_conditioning\", \"trace_conditioning\", \"trace_conditioning\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"transducer\", \"transducer\", \"transducer\", \"transducer\", \"transducer\", \"transductive\", \"transductive\", \"transductive\", \"transductive\", \"transductive\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"triangulation\", \"triangulation\", \"triangulation\", \"triangulation\", \"triangulation\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"trueskill\", \"trueskill\", \"trueskill\", \"trueskill\", \"trueskill\", \"tsbn\", \"tsbn\", \"tsbn\", \"tsbn\", \"ugms\", \"ugms\", \"ugms\", \"ugms\", \"ugms\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"uniform_convergence\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value_iteration\", \"value_iteration\", \"value_iteration\", \"value_iteration\", \"value_iteration\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variance\", \"variance\", \"variance\", \"variance\", \"variance\", \"variance\", \"variance\", \"vb\", \"vb\", \"vb\", \"vb\", \"vb\", \"vb\", \"vb_pca\", \"vb_pca\", \"vb_pca\", \"vb_pca\", \"vb_pca\", \"vb_pca\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex_exchangeability\", \"vertex_exchangeability\", \"vertex_exchangeability\", \"vertex_exchangeability\", \"vertex_exchangeability\", \"vertice\", \"vertice\", \"vertice\", \"vertice\", \"vertice\", \"vertice\", \"vertice\", \"vfe\", \"vfe\", \"vfe\", \"vfe\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"wavelet_coefficient\", \"wavelet_coefficient\", \"wavelet_coefficient\", \"wavelet_coefficient\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way_resemblance\", \"way_resemblance\", \"way_resemblance\", \"way_resemblance\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weighted_trace\", \"weighted_trace\", \"weighted_trace\", \"weighted_trace\", \"weighted_trace\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"zy\", \"zy\", \"zy\", \"zy\", \"zy\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 4, 7, 8, 1, 2, 3, 6]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el3041382731926782725649044352\", ldavis_el3041382731926782725649044352_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el3041382731926782725649044352\", ldavis_el3041382731926782725649044352_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el3041382731926782725649044352\", ldavis_el3041382731926782725649044352_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9N0uJj22d6r"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "\n",
        "We started with understanding why evaluating the topic model is essential. Next, we reviewed existing methods and scratched the surface of topic coherence, along with the available coherence measures. Then we built a default LDA model using Gensim implementation to establish the baseline coherence score and reviewed practical ways to optimize the LDA hyperparameters.\n",
        "\n",
        "Hopefully, this article has managed to shed light on the underlying topic evaluation strategies, and intuitions behind it.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. http://qpleple.com/perplexity-to-evaluate-topic-models/\n",
        "2. https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020\n",
        "3. https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf\n",
        "4. https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb\n",
        "5. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "6. http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
        "7. http://palmetto.aksw.org/palmetto-webapp/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}